{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Efficientnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0EtwleX8X77dhFjAbbQW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khangdltUIT/Deep-Learning/blob/main/efficientnet/Efficientnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX41yrtfhrmZ"
      },
      "source": [
        "# **Efficientnet: Rethinking model scaling for convolutional Neural Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAr6BB37hwaw"
      },
      "source": [
        "## *Introduction*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ILwU_ghyWK"
      },
      "source": [
        "Efficientnet was proposed at paper: *Efficientnet: Rethinking model scaling for convolutional Neural Network* by researchers at Google Brain. In that paper, reseachers proposed a compound scaling method which scale up not only depth dimension but also width and resoluton dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X65T6-Ewi-jb"
      },
      "source": [
        "* Depth: simply means how deep the networks is which is equivalent to the number of layers in it\n",
        "* Width: simply means how wide the network is \n",
        "* Resolution: is simply the image resolution that is being passed to a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TidFcbi7iyiI"
      },
      "source": [
        "![model_scaling.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gygViyYQjgKK"
      },
      "source": [
        "## **Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eixUgTnejmAc"
      },
      "source": [
        "In this section, I will use Pytorch frameword for implementaion Efficientnet.  \n",
        "Regarding the dataset, I use Dog/Cat Classification publshing on kaggle.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6tXpMBnkNLj"
      },
      "source": [
        "### Step 01: Set up environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MskEYA3Bkxw8"
      },
      "source": [
        "##### Check GPU Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKdedSgZhl_-",
        "outputId": "1de1cabf-4a4a-481d-f906-57f43764aa8b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 23 14:45:05 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMpRGieSk2DV"
      },
      "source": [
        "#### Import Libraries used in this project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp0aAP5CkRG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701137e6-f35a-4853-b104-41c96ce203b3"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "from PIL import ImageFile\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random \n",
        "from shutil import copyfile\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import re\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensor\n",
        "import re\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensor\n",
        "\n",
        "print('Importing... done!')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing... done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvAwsJvVlKRB"
      },
      "source": [
        "### Step 02: CORONA CHEST DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJqVHVR_oUxG"
      },
      "source": [
        "* Information:  \n",
        "The utility of this dataset has been confirmed by a senior radiologist in Tongji Hospital, Wuhan, China, who has performed diagnosis and treatment of a large number of COVID-19 patients during the outbreak of this disease between January and April."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX4lFZb__nZT",
        "outputId": "8f76f7b5-4f5c-4d64-f339-31bf700323cd"
      },
      "source": [
        "!git clone https://github.com/UCSD-AI4H/COVID-CT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'COVID-CT'...\n",
            "remote: Enumerating objects: 5459, done.\u001b[K\n",
            "remote: Total 5459 (delta 0), reused 0 (delta 0), pack-reused 5459\u001b[K\n",
            "Receiving objects: 100% (5459/5459), 1.09 GiB | 18.85 MiB/s, done.\n",
            "Resolving deltas: 100% (360/360), done.\n",
            "Checking out files: 100% (1048/1048), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQZhq9XVAAdn"
      },
      "source": [
        "!unzip -q '/content/COVID-CT/Images-processed/CT_COVID.zip'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-DdWu0KAKU-"
      },
      "source": [
        "!unzip -q '/content/COVID-CT/Images-processed/CT_NonCOVID.zip'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoIBcbz6Bc8o"
      },
      "source": [
        "!mkdir Image"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83b8Pvs8BerK"
      },
      "source": [
        "!mv '/content/CT_COVID' '/content/Image'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZOUySfvCrud"
      },
      "source": [
        "!mv '/content/CT_NonCOVID' '/content/Image'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOOaf537AOmD"
      },
      "source": [
        "## Define Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfhQ66ZrAR2Q"
      },
      "source": [
        "def read_txt(txt_path):\n",
        "    with open(txt_path) as f:\n",
        "        lines = f.readlines()\n",
        "    txt_data = [line.strip() for line in lines]\n",
        "    return txt_data"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9kwAbFJAYSE"
      },
      "source": [
        "class CovidCTDataset():\n",
        "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            txt_path (string): Path to the txt file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        File structure:\n",
        "        - root_dir\n",
        "            - CT_COVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "            - CT_NonCOVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
        "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
        "        self.num_cls = len(self.classes)\n",
        "        self.img_list = []\n",
        "        for c in range(self.num_cls):\n",
        "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
        "            self.img_list += cls_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.img_list[idx][0]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        sample = {'img': image,\n",
        "                  'label': int(self.img_list[idx][1])}\n",
        "        return sample"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZxX3wFDAjSs"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "train_transformer = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_transformer = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7woXK5uxAn6M",
        "outputId": "72a61b3c-c8a4-4f0a-b9cc-10d1ce450c64"
      },
      "source": [
        "\n",
        "trainset = CovidCTDataset(root_dir='/content/Image',\n",
        "                            txt_COVID='/content/COVID-CT/Data-split/COVID/trainCT_COVID.txt',\n",
        "                            txt_NonCOVID='/content/COVID-CT/Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
        "                            transform= train_transformer)\n",
        "valset = CovidCTDataset(root_dir='/content/Image',\n",
        "                            txt_COVID='/content/COVID-CT/Data-split/COVID/valCT_COVID.txt',\n",
        "                            txt_NonCOVID='/content/COVID-CT/Data-split/NonCOVID/valCT_NonCOVID.txt',\n",
        "                            transform= val_transformer)\n",
        "testset = CovidCTDataset(root_dir='/content/Image',\n",
        "                            txt_COVID='/content/COVID-CT/Data-split/COVID/testCT_COVID.txt',\n",
        "                            txt_NonCOVID='/content/COVID-CT/Data-split/NonCOVID/testCT_NonCOVID.txt',\n",
        "                            transform= val_transformer)\n",
        "print(trainset.__len__())\n",
        "print(valset.__len__())\n",
        "print(testset.__len__())\n",
        "\n",
        "batchsize = 8\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
        "val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "425\n",
            "118\n",
            "203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOmp8AXeFeLi"
      },
      "source": [
        "alpha = None\n",
        "device = 'cuda'\n",
        "def train(optimizer, epoch):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    \n",
        "    for batch_index, batch_samples in enumerate(train_loader):\n",
        "        \n",
        "        # move data to device\n",
        "        data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
        "#        data = data[:, 0, :, :]\n",
        "#        data = data[:, None, :, :]\n",
        "#         data, targets_a, targets_b, lam = mixup_data(data, target.long(), alpha, use_cuda=True)\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        \n",
        "        criteria = nn.CrossEntropyLoss()\n",
        "        loss = criteria(output, target.long())\n",
        "#         loss = mixup_criterion(criteria, output, targets_a, targets_b, lam)\n",
        "        train_loss += criteria(output, target.long())\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        train_correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
        "    \n",
        "        # Display progress and write to tensorboard\n",
        "        if batch_index % bs == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
        "                epoch, batch_index, len(train_loader),\n",
        "                100.0 * batch_index / len(train_loader), loss.item()/ bs))\n",
        "    \n",
        "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
        "        100.0 * train_correct / len(train_loader.dataset)))\n",
        "    f = open('model_result/{}.txt'.format(modelname), 'a+')\n",
        "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
        "        100.0 * train_correct / len(train_loader.dataset)))\n",
        "    f.write('\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "# In[155]:\n",
        "\n",
        "\n",
        "def val(epoch):\n",
        "    \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    results = []\n",
        "    \n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    FP = 0\n",
        "    \n",
        "    \n",
        "    criteria = nn.CrossEntropyLoss()\n",
        "    # Don't update model\n",
        "    with torch.no_grad():\n",
        "        tpr_list = []\n",
        "        fpr_list = []\n",
        "        \n",
        "        predlist=[]\n",
        "        scorelist=[]\n",
        "        targetlist=[]\n",
        "        # Predict\n",
        "        for batch_index, batch_samples in enumerate(val_loader):\n",
        "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
        "#            data = data[:, 0, :, :]\n",
        "#            data = data[:, None, :, :]\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += criteria(output, target.long())\n",
        "            score = F.softmax(output, dim=1)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "#             print('target',target.long()[:, 2].view_as(pred))\n",
        "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
        "            \n",
        "#             print(output[:,1].cpu().numpy())\n",
        "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
        "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
        "            targetcpu=target.long().cpu().numpy()\n",
        "            predlist=np.append(predlist, pred.cpu().numpy())\n",
        "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
        "            targetlist=np.append(targetlist,targetcpu)\n",
        "           \n",
        "          \n",
        "    return targetlist, scorelist, predlist\n",
        "\n",
        "\n",
        "# In[152]:\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    results = []\n",
        "    \n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    FP = 0\n",
        "    \n",
        "    \n",
        "    criteria = nn.CrossEntropyLoss()\n",
        "    # Don't update model\n",
        "    with torch.no_grad():\n",
        "        tpr_list = []\n",
        "        fpr_list = []\n",
        "        \n",
        "        predlist=[]\n",
        "        scorelist=[]\n",
        "        targetlist=[]\n",
        "        # Predict\n",
        "        for batch_index, batch_samples in enumerate(test_loader):\n",
        "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
        "#            data = data[:, 0, :, :]\n",
        "#            data = data[:, None, :, :]\n",
        "#             print(target)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += criteria(output, target.long())\n",
        "            score = F.softmax(output, dim=1)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "#             print('target',target.long()[:, 2].view_as(pred))\n",
        "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
        "#             TP += ((pred == 1) & (target.long()[:, 2].view_as(pred).data == 1)).cpu().sum()\n",
        "#             TN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
        "# #             # FN    predict 0 label 1\n",
        "#             FN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 1)).cpu().sum()\n",
        "# #             # FP    predict 1 label 0\n",
        "#             FP += ((pred == 1) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
        "#             print(TP,TN,FN,FP)\n",
        "            \n",
        "            \n",
        "#             print(output[:,1].cpu().numpy())\n",
        "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
        "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
        "            targetcpu=target.long().cpu().numpy()\n",
        "            predlist=np.append(predlist, pred.cpu().numpy())\n",
        "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
        "            targetlist=np.append(targetlist,targetcpu)\n",
        "           \n",
        "    return targetlist, scorelist, predlist\n",
        "    "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHHNAOqCFhrd",
        "outputId": "ce485c08-2f7d-4f7b-b83f-a1e272921828"
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16031 sha256=a21125982129f4340f578db5c728490fc33d0a70e21eeca51c455fe47f0e94bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgtaowIfG5_Q",
        "outputId": "48d27c84-002d-4e43-a348-93c27a42e43a"
      },
      "source": [
        "!mkdir model_result\n",
        "!mkdir model_backup\n",
        "!mkdir train_10epochs_model_backup"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory model_result: File exists\n",
            "mkdir: cannot create directory model_backup: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SckJlKcVEfau",
        "outputId": "5682b4cb-7b9c-40b7-dd4b-dc1dd5ead4f2"
      },
      "source": [
        "### efficientNet\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=2)\n",
        "model = model.cuda()\n",
        "modelname = 'efficientNet-b4'\n",
        "\n",
        "\n",
        "# model = EfficientNet.from_name('efficientnet-b1').cuda()\n",
        "# modelname = 'efficientNet_random'\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# train\n",
        "bs = 10\n",
        "votenum = 10\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "r_list = []\n",
        "p_list = []\n",
        "acc_list = []\n",
        "AUC_list = []\n",
        "# TP = 0\n",
        "# TN = 0\n",
        "# FN = 0\n",
        "# FP = 0\n",
        "vote_pred = np.zeros(valset.__len__())\n",
        "vote_score = np.zeros(valset.__len__())\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "#scheduler = StepLR(optimizer, step_size=1)\n",
        "\n",
        "total_epoch = 3000\n",
        "for epoch in range(1, total_epoch+1):\n",
        "    train(optimizer, epoch)\n",
        "    \n",
        "    targetlist, scorelist, predlist = val(epoch)\n",
        "    print('target',targetlist)\n",
        "    print('score',scorelist)\n",
        "    print('predict',predlist)\n",
        "    vote_pred = vote_pred + predlist \n",
        "    vote_score = vote_score + scorelist \n",
        "\n",
        "    if epoch % votenum == 0:\n",
        "        \n",
        "        # major vote\n",
        "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
        "        vote_pred[vote_pred > (votenum/2)] = 1\n",
        "        vote_score = vote_score/votenum\n",
        "        \n",
        "        print('vote_pred', vote_pred)\n",
        "        print('targetlist', targetlist)\n",
        "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
        "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
        "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
        "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
        "        \n",
        "        \n",
        "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
        "        print('TP+FP',TP+FP)\n",
        "        p = TP / (TP + FP)\n",
        "        print('precision',p)\n",
        "        p = TP / (TP + FP)\n",
        "        r = TP / (TP + FN)\n",
        "        print('recall',r)\n",
        "        F1 = 2 * r * p / (r + p)\n",
        "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "        print('F1',F1)\n",
        "        print('acc',acc)\n",
        "        AUC = roc_auc_score(targetlist, vote_score)\n",
        "        print('AUCp', roc_auc_score(targetlist, vote_pred))\n",
        "        print('AUC', AUC)\n",
        "        \n",
        "        \n",
        "        \n",
        "#         if epoch == total_epoch:\n",
        "        torch.save(model.state_dict(), \"train_10epochs_model_backup/{}.pth\".format(modelname))  \n",
        "        \n",
        "        vote_pred = np.zeros(valset.__len__())\n",
        "        vote_score = np.zeros(valset.__len__())\n",
        "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
        "        epoch, r, p, F1, acc, AUC))\n",
        "\n",
        "        f = open('/content/model_result/{}.txt'.format(modelname), 'a+')\n",
        "        f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
        "        epoch, r, p, F1, acc, AUC))\n",
        "        f.close()\n",
        "\n",
        "\n",
        "# In[145]:\n",
        "\n",
        "\n",
        "# test\n",
        "bs = 10\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "r_list = []\n",
        "p_list = []\n",
        "acc_list = []\n",
        "AUC_list = []\n",
        "# TP = 0\n",
        "# TN = 0\n",
        "# FN = 0\n",
        "# FP = 0\n",
        "vote_pred = np.zeros(testset.__len__())\n",
        "vote_score = np.zeros(testset.__len__())\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "\n",
        "total_epoch = 10\n",
        "for epoch in range(1, total_epoch+1):\n",
        "    \n",
        "    targetlist, scorelist, predlist = test(epoch)\n",
        "#     print('target',targetlist)\n",
        "#     print('score',scorelist)\n",
        "#     print('predict',predlist)\n",
        "    vote_pred = vote_pred + predlist \n",
        "    vote_score = vote_score + scorelist \n",
        "    \n",
        "    TP = ((predlist == 1) & (targetlist == 1)).sum()\n",
        "    TN = ((predlist == 0) & (targetlist == 0)).sum()\n",
        "    FN = ((predlist == 0) & (targetlist == 1)).sum()\n",
        "    FP = ((predlist == 1) & (targetlist == 0)).sum()\n",
        "\n",
        "    print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
        "    print('TP+FP',TP+FP)\n",
        "    p = TP / (TP + FP)\n",
        "    print('precision',p)\n",
        "    p = TP / (TP + FP)\n",
        "    r = TP / (TP + FN)\n",
        "    print('recall',r)\n",
        "    F1 = 2 * r * p / (r + p)\n",
        "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "    print('F1',F1)\n",
        "    print('acc',acc)\n",
        "    AUC = roc_auc_score(targetlist, vote_score)\n",
        "    print('AUC', AUC)\n",
        "\n",
        "    if epoch % votenum == 0:\n",
        "        \n",
        "        # major vote\n",
        "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
        "        vote_pred[vote_pred > (votenum/2)] = 1\n",
        "        \n",
        "#         print('vote_pred', vote_pred)\n",
        "#         print('targetlist', targetlist)\n",
        "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
        "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
        "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
        "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
        "        \n",
        "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
        "        print('TP+FP',TP+FP)\n",
        "        p = TP / (TP + FP)\n",
        "        print('precision',p)\n",
        "        p = TP / (TP + FP)\n",
        "        r = TP / (TP + FN)\n",
        "        print('recall',r)\n",
        "        F1 = 2 * r * p / (r + p)\n",
        "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "        print('F1',F1)\n",
        "        print('acc',acc)\n",
        "        AUC = roc_auc_score(targetlist, vote_score)\n",
        "        print('AUC', AUC)\n",
        "        \n",
        "        \n",
        "#         f = open('model_result/{modelname}.txt', 'a+')\n",
        "#         f.write('precision, recall, F1, acc= \\n')\n",
        "#         f.writelines(str(p))\n",
        "#         f.writelines('\\n')\n",
        "#         f.writelines(str(r))\n",
        "#         f.writelines('\\n')\n",
        "#         f.writelines(str(F1))\n",
        "#         f.writelines('\\n')\n",
        "#         f.writelines(str(acc))\n",
        "#         f.writelines('\\n')\n",
        "#         f.close()\n",
        "        \n",
        "        \n",
        "        vote_pred = np.zeros((1,testset.__len__()))\n",
        "        vote_score = np.zeros(testset.__len__())\n",
        "        print('vote_pred',vote_pred)\n",
        "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
        "        epoch, r, p, F1, acc, AUC))\n",
        "\n",
        "        f = open(f'model_result/test_{modelname}.txt', 'a+')\n",
        "        f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
        "        epoch, r, p, F1, acc, AUC))\n",
        "        f.close()\n",
        "\n",
        "    "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch: 13 [30/54 (56%)]\tTrain Loss: 0.009013\n",
            "Train Epoch: 13 [40/54 (74%)]\tTrain Loss: 0.008398\n",
            "Train Epoch: 13 [50/54 (93%)]\tTrain Loss: 0.006525\n",
            "\n",
            "Train set: Average loss: 0.0133, Accuracy: 413/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [7.64484107e-02 1.43414184e-01 6.23716824e-02 1.28180817e-01\n",
            " 1.44993945e-03 2.95025945e-01 1.86523721e-01 2.09198833e-01\n",
            " 1.05718285e-01 1.37665058e-02 1.66175783e-01 8.25861841e-03\n",
            " 1.45994499e-01 2.70142090e-02 1.58307016e-01 1.03974862e-05\n",
            " 5.57662588e-06 5.05048483e-05 4.68406618e-01 3.97734204e-03\n",
            " 9.05646145e-01 3.76930743e-01 4.39845733e-02 9.99898434e-01\n",
            " 3.70120376e-01 3.36937219e-01 9.73814130e-01 1.28146727e-02\n",
            " 5.98875806e-02 1.80402081e-02 2.05881298e-01 9.05583978e-01\n",
            " 9.07090247e-01 5.95338224e-03 4.17172350e-03 1.04114246e-02\n",
            " 2.04486921e-02 1.18881829e-01 9.53941569e-02 8.38241875e-02\n",
            " 1.03174381e-01 5.00193417e-01 1.19183391e-01 8.67593661e-03\n",
            " 2.26016954e-01 6.60517439e-02 3.02686095e-01 4.53634471e-01\n",
            " 2.83745080e-01 1.24776401e-01 2.51949221e-01 1.17167896e-04\n",
            " 1.18186887e-04 1.41763128e-03 3.44367713e-01 7.25218561e-04\n",
            " 3.96351159e-01 7.37423950e-04 1.04266694e-02 1.48020452e-02\n",
            " 9.97497737e-01 9.99981165e-01 9.99861598e-01 9.99999285e-01\n",
            " 8.07406723e-01 9.99881983e-01 6.98971331e-01 9.99998569e-01\n",
            " 9.91155267e-01 9.98557985e-01 8.98208767e-02 4.26043481e-01\n",
            " 9.99477684e-01 8.78748417e-01 7.15139389e-01 9.62111533e-01\n",
            " 9.93342817e-01 9.99686599e-01 9.90215659e-01 9.99999404e-01\n",
            " 9.99999881e-01 9.93423223e-01 9.99979258e-01 6.61180139e-01\n",
            " 1.98325038e-01 9.99092579e-01 9.92720604e-01 9.88044918e-01\n",
            " 8.54626819e-02 9.95664775e-01 9.03877616e-01 8.58085096e-01\n",
            " 1.33724764e-01 9.02522743e-01 9.26151216e-01 3.74878079e-01\n",
            " 5.51844276e-02 8.27114284e-01 8.01261008e-01 9.02609289e-01\n",
            " 1.90584973e-01 5.43869548e-02 9.41346139e-02 9.59155243e-03\n",
            " 3.57660353e-02 1.55088887e-01 3.03384185e-01 4.86751385e-02\n",
            " 1.65756315e-01 1.17785083e-02 4.28992100e-02 3.20870847e-01\n",
            " 3.51590693e-01 9.23738837e-01 8.95027459e-01 5.92270851e-01\n",
            " 9.99972105e-01 9.99923468e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 14 [0/54 (0%)]\tTrain Loss: 0.000812\n",
            "Train Epoch: 14 [10/54 (19%)]\tTrain Loss: 0.003751\n",
            "Train Epoch: 14 [20/54 (37%)]\tTrain Loss: 0.014426\n",
            "Train Epoch: 14 [30/54 (56%)]\tTrain Loss: 0.014387\n",
            "Train Epoch: 14 [40/54 (74%)]\tTrain Loss: 0.004443\n",
            "Train Epoch: 14 [50/54 (93%)]\tTrain Loss: 0.000940\n",
            "\n",
            "Train set: Average loss: 0.0171, Accuracy: 411/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [8.83474387e-03 7.40377456e-02 4.96520996e-02 4.22252804e-01\n",
            " 1.01730622e-01 3.22666407e-01 1.72014907e-01 2.32569203e-01\n",
            " 1.11876018e-01 1.31895900e-01 5.86327732e-01 8.05402249e-02\n",
            " 4.60832208e-01 1.40162343e-02 5.65814450e-02 4.88945516e-03\n",
            " 4.56758789e-05 2.15246971e-03 5.22451103e-01 1.98340118e-02\n",
            " 7.93164968e-01 6.31278694e-01 6.12611137e-02 9.98368442e-01\n",
            " 7.20841229e-01 5.64950168e-01 9.60829258e-01 2.20331457e-02\n",
            " 5.74714951e-02 1.26558961e-02 8.26037005e-02 4.83443707e-01\n",
            " 1.57216340e-01 4.60888967e-02 8.68673716e-03 4.06537093e-02\n",
            " 2.12214515e-02 4.63007540e-01 3.35744172e-01 2.90531904e-01\n",
            " 5.69938719e-01 6.84772789e-01 1.64968506e-01 8.27531796e-03\n",
            " 5.06387837e-02 1.60546780e-01 3.79617602e-01 3.80160362e-01\n",
            " 3.46371502e-01 1.96357638e-01 2.95272917e-01 9.59227677e-04\n",
            " 1.62211843e-02 5.32976538e-03 7.84518600e-01 4.85480018e-02\n",
            " 7.58430183e-01 4.68335534e-03 8.14433470e-02 1.39960973e-02\n",
            " 9.86702144e-01 9.99687791e-01 9.98963594e-01 9.99918342e-01\n",
            " 1.02543987e-01 8.72336805e-01 4.84338015e-01 9.98440921e-01\n",
            " 8.90605628e-01 9.93116379e-01 1.29461512e-01 2.20775589e-01\n",
            " 9.23778296e-01 7.63165474e-01 4.22700822e-01 9.06131089e-01\n",
            " 6.97570682e-01 9.94409859e-01 9.55965877e-01 9.89484966e-01\n",
            " 9.97857034e-01 6.36621952e-01 9.74368572e-01 9.94267046e-01\n",
            " 8.52578282e-01 9.79390860e-01 9.45430398e-01 9.60639954e-01\n",
            " 1.63522676e-01 9.98275399e-01 9.81140018e-01 8.50270987e-01\n",
            " 1.20854467e-01 9.64004278e-01 9.67167914e-01 9.07138884e-01\n",
            " 9.62756515e-01 9.58570302e-01 9.08975720e-01 9.54965055e-01\n",
            " 6.12955809e-01 5.16160190e-01 3.58535081e-01 5.00029363e-02\n",
            " 1.36119738e-01 9.38420117e-01 7.58266091e-01 2.25864425e-01\n",
            " 9.73289967e-01 8.64827335e-02 1.05485529e-01 9.64660764e-01\n",
            " 6.01882577e-01 9.79587317e-01 7.86623478e-01 3.95231128e-01\n",
            " 9.99789536e-01 9.99618411e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 15 [0/54 (0%)]\tTrain Loss: 0.004185\n",
            "Train Epoch: 15 [10/54 (19%)]\tTrain Loss: 0.001929\n",
            "Train Epoch: 15 [20/54 (37%)]\tTrain Loss: 0.001719\n",
            "Train Epoch: 15 [30/54 (56%)]\tTrain Loss: 0.001347\n",
            "Train Epoch: 15 [40/54 (74%)]\tTrain Loss: 0.013705\n",
            "Train Epoch: 15 [50/54 (93%)]\tTrain Loss: 0.001997\n",
            "\n",
            "Train set: Average loss: 0.0090, Accuracy: 413/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [9.71847854e-04 6.02036864e-02 3.45179848e-02 3.78255069e-01\n",
            " 1.16968127e-02 1.41979948e-01 1.53218880e-01 3.34894896e-01\n",
            " 5.36500625e-02 5.22700325e-02 6.61592066e-01 2.35229526e-02\n",
            " 4.77712065e-01 6.59789238e-03 7.98875466e-02 6.82250684e-05\n",
            " 4.74776152e-06 2.09085550e-03 2.93874323e-01 8.60625412e-03\n",
            " 8.49200249e-01 9.32870626e-01 3.42474312e-01 9.99981880e-01\n",
            " 9.07338440e-01 9.83210444e-01 9.95814741e-01 2.99159996e-02\n",
            " 4.34493840e-01 1.33740101e-02 2.97497958e-01 9.99336541e-01\n",
            " 8.85265172e-01 7.91111588e-03 4.37088404e-03 9.36400332e-03\n",
            " 6.68893009e-03 4.74957317e-01 8.76818359e-01 4.65719074e-01\n",
            " 3.68843436e-01 8.29194784e-01 3.00246090e-01 6.10533468e-02\n",
            " 8.71462464e-01 4.82215971e-01 9.54956889e-01 8.75657797e-01\n",
            " 6.16902590e-01 2.36466333e-01 5.38880289e-01 1.73892586e-05\n",
            " 3.61756218e-04 5.39621862e-04 4.73691940e-01 1.38149294e-03\n",
            " 7.87141800e-01 2.84610724e-04 1.50165670e-02 9.53269098e-03\n",
            " 9.97514009e-01 9.99989629e-01 9.99931574e-01 9.99999881e-01\n",
            " 9.36878622e-01 9.98323858e-01 8.68564129e-01 1.00000000e+00\n",
            " 9.92492020e-01 9.95490015e-01 3.99510562e-01 8.58464360e-01\n",
            " 9.99969602e-01 9.12783384e-01 8.04506600e-01 9.99969721e-01\n",
            " 8.24443400e-01 9.97848868e-01 9.99683619e-01 9.99999404e-01\n",
            " 1.00000000e+00 9.99840140e-01 9.99981165e-01 9.99893665e-01\n",
            " 6.67570293e-01 9.99999881e-01 9.99980569e-01 9.99997020e-01\n",
            " 2.23847985e-01 9.99992609e-01 9.93434310e-01 9.09669757e-01\n",
            " 1.21536382e-01 9.94228601e-01 9.97636557e-01 9.66272771e-01\n",
            " 7.00399041e-01 9.74879920e-01 9.72377896e-01 9.97687101e-01\n",
            " 6.70102596e-01 6.39174819e-01 9.47197556e-01 2.19337549e-02\n",
            " 1.01054952e-01 9.47213054e-01 7.74531186e-01 5.98878898e-02\n",
            " 9.70501244e-01 3.74007039e-02 4.33222391e-02 8.63252699e-01\n",
            " 6.22391880e-01 9.98435438e-01 9.98501658e-01 8.69646192e-01\n",
            " 9.99999166e-01 9.99998689e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 16 [0/54 (0%)]\tTrain Loss: 0.000401\n",
            "Train Epoch: 16 [10/54 (19%)]\tTrain Loss: 0.001467\n",
            "Train Epoch: 16 [20/54 (37%)]\tTrain Loss: 0.002017\n",
            "Train Epoch: 16 [30/54 (56%)]\tTrain Loss: 0.002760\n",
            "Train Epoch: 16 [40/54 (74%)]\tTrain Loss: 0.000480\n",
            "Train Epoch: 16 [50/54 (93%)]\tTrain Loss: 0.037280\n",
            "\n",
            "Train set: Average loss: 0.0097, Accuracy: 419/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.91846353e-04 1.15518726e-01 4.38399874e-02 3.69921386e-01\n",
            " 2.18499941e-03 1.17767490e-01 3.18146288e-01 2.89877176e-01\n",
            " 4.28959578e-02 2.90070213e-02 7.54812300e-01 1.98490266e-02\n",
            " 4.74722475e-01 1.13690710e-02 1.05247080e-01 4.50480275e-06\n",
            " 1.41442467e-06 1.17451849e-03 8.05419404e-03 1.91506022e-03\n",
            " 2.07288787e-01 6.92635655e-01 5.78730144e-02 9.99998093e-01\n",
            " 7.64158368e-01 8.50821912e-01 9.60970759e-01 1.96668953e-02\n",
            " 2.00266764e-01 6.47491263e-03 1.85627997e-01 9.47635174e-01\n",
            " 2.78623730e-01 1.15469575e-03 2.28278898e-03 4.20478871e-03\n",
            " 3.51892458e-03 1.71187535e-01 5.60680449e-01 6.20031282e-02\n",
            " 2.46313941e-02 1.92678034e-01 9.65884030e-02 3.33899469e-03\n",
            " 1.12832151e-01 7.24586993e-02 3.59627664e-01 3.37626815e-01\n",
            " 4.17678893e-01 9.26423073e-02 2.20430166e-01 1.65828851e-06\n",
            " 1.27340682e-04 3.23665648e-04 9.91076082e-02 1.82056843e-04\n",
            " 5.82757294e-01 9.53635172e-05 8.17035325e-03 7.35144038e-03\n",
            " 9.96872246e-01 9.99999523e-01 9.99975920e-01 1.00000000e+00\n",
            " 7.82075703e-01 9.99955058e-01 8.95949543e-01 1.00000000e+00\n",
            " 9.90731657e-01 9.79545236e-01 2.17408955e-01 5.22807777e-01\n",
            " 9.99987364e-01 7.95865774e-01 8.73084307e-01 9.99913096e-01\n",
            " 9.95012462e-01 9.99999762e-01 9.99978900e-01 9.99994397e-01\n",
            " 9.99999762e-01 9.98494148e-01 9.99997854e-01 9.99266684e-01\n",
            " 3.08565885e-01 9.99994874e-01 9.99984145e-01 9.99993682e-01\n",
            " 3.31668735e-01 9.99985695e-01 9.80606019e-01 6.25170171e-01\n",
            " 6.20557033e-02 9.86985147e-01 9.95712280e-01 7.81965196e-01\n",
            " 2.98752755e-01 9.84236479e-01 8.46295357e-01 9.91429985e-01\n",
            " 6.64821863e-01 1.73883945e-01 7.71990418e-01 9.56024881e-03\n",
            " 1.41215576e-02 4.89992440e-01 3.12743634e-01 1.45575618e-02\n",
            " 6.33054972e-01 7.77979335e-03 2.06387602e-02 7.39644170e-01\n",
            " 2.19427422e-01 9.98840034e-01 9.99688268e-01 3.15355211e-01\n",
            " 1.00000000e+00 9.99999881e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 17 [0/54 (0%)]\tTrain Loss: 0.000337\n",
            "Train Epoch: 17 [10/54 (19%)]\tTrain Loss: 0.000259\n",
            "Train Epoch: 17 [20/54 (37%)]\tTrain Loss: 0.009984\n",
            "Train Epoch: 17 [30/54 (56%)]\tTrain Loss: 0.015266\n",
            "Train Epoch: 17 [40/54 (74%)]\tTrain Loss: 0.001331\n",
            "Train Epoch: 17 [50/54 (93%)]\tTrain Loss: 0.015544\n",
            "\n",
            "Train set: Average loss: 0.0097, Accuracy: 412/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.68264217e-04 7.79822022e-02 4.12307642e-02 1.49032950e-01\n",
            " 5.22779301e-03 7.06243515e-02 2.63577670e-01 1.62415057e-01\n",
            " 7.55309733e-03 7.52698109e-02 5.14244616e-01 1.02626063e-01\n",
            " 2.40320772e-01 6.13108336e-04 2.44585387e-02 7.82102294e-09\n",
            " 2.95420945e-11 5.95508880e-07 3.98435397e-04 1.85763420e-04\n",
            " 4.51760113e-01 4.66691583e-01 9.05162841e-02 9.99998689e-01\n",
            " 5.13842583e-01 7.35099196e-01 9.94325340e-01 1.88325141e-02\n",
            " 1.23547949e-01 2.68036569e-03 1.21859312e-01 8.47253382e-01\n",
            " 3.29857498e-01 1.69833031e-04 1.89499988e-05 9.90589833e-05\n",
            " 1.48964275e-04 2.59670824e-01 6.74306303e-02 4.31597456e-02\n",
            " 4.97371517e-02 3.50582242e-01 7.48866871e-02 3.40214488e-03\n",
            " 7.80408755e-02 3.25415164e-01 6.86540723e-01 5.94953716e-01\n",
            " 3.70063215e-01 2.70234078e-01 1.93372414e-01 4.11966816e-09\n",
            " 2.27879946e-06 3.52135999e-06 1.49165332e-01 6.99777884e-05\n",
            " 1.94696158e-01 7.06122762e-07 1.81644002e-03 1.20117259e-03\n",
            " 9.98321593e-01 9.99999881e-01 9.99840617e-01 9.99999881e-01\n",
            " 2.80218452e-01 9.11668181e-01 7.33920813e-01 1.00000000e+00\n",
            " 9.99095082e-01 7.20696151e-01 2.20535412e-01 4.71827686e-01\n",
            " 9.42382872e-01 3.75974655e-01 3.88529360e-01 9.99978662e-01\n",
            " 9.81517255e-01 9.99998569e-01 9.99974132e-01 9.99999523e-01\n",
            " 1.00000000e+00 9.99927998e-01 1.00000000e+00 9.64672089e-01\n",
            " 2.83831626e-01 9.99898195e-01 9.99985814e-01 9.99978065e-01\n",
            " 1.72734279e-02 9.99863505e-01 9.97299016e-01 9.45519686e-01\n",
            " 1.55604303e-01 9.97953773e-01 9.99829888e-01 9.76143360e-01\n",
            " 1.43058196e-01 9.36815321e-01 3.65955442e-01 8.77950132e-01\n",
            " 6.11830711e-01 3.23275365e-02 1.19339712e-02 2.84231845e-02\n",
            " 3.54543217e-02 6.48654819e-01 5.86446226e-01 7.40591576e-03\n",
            " 8.74494240e-02 2.84288940e-03 1.53329615e-02 3.37934047e-01\n",
            " 3.74771953e-01 9.86641228e-01 8.44469786e-01 1.06100231e-01\n",
            " 9.99999523e-01 9.99980688e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 18 [0/54 (0%)]\tTrain Loss: 0.015329\n",
            "Train Epoch: 18 [10/54 (19%)]\tTrain Loss: 0.001345\n",
            "Train Epoch: 18 [20/54 (37%)]\tTrain Loss: 0.000940\n",
            "Train Epoch: 18 [30/54 (56%)]\tTrain Loss: 0.002117\n",
            "Train Epoch: 18 [40/54 (74%)]\tTrain Loss: 0.000332\n",
            "Train Epoch: 18 [50/54 (93%)]\tTrain Loss: 0.000559\n",
            "\n",
            "Train set: Average loss: 0.0107, Accuracy: 420/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.96395631e-04 5.95911071e-02 3.38750780e-02 8.86417776e-02\n",
            " 1.97026832e-03 1.47252241e-02 2.25837603e-01 1.52000383e-01\n",
            " 3.12491873e-04 5.86765166e-03 4.80202645e-01 8.34596157e-03\n",
            " 2.78407246e-01 1.03231997e-03 9.22925677e-03 1.42819729e-07\n",
            " 4.77471451e-10 2.52353534e-08 3.25800269e-04 6.73847552e-03\n",
            " 1.38956130e-01 3.02842915e-01 1.59378648e-01 9.99998331e-01\n",
            " 2.72381693e-01 7.35727727e-01 9.98577356e-01 2.88947020e-02\n",
            " 3.72986607e-02 3.38989589e-03 1.46304116e-01 9.83456075e-01\n",
            " 9.19188797e-01 3.83550389e-04 5.16479740e-05 4.51885979e-04\n",
            " 4.54499153e-04 1.85072273e-01 4.25419301e-01 6.48682833e-01\n",
            " 1.07008867e-01 6.23673320e-01 8.99551287e-02 5.50125446e-03\n",
            " 2.51818269e-01 2.97931820e-01 7.31173873e-01 8.05212140e-01\n",
            " 8.00926745e-01 2.14389086e-01 8.17019165e-01 1.04854493e-07\n",
            " 2.20670722e-06 2.51364270e-07 3.45131665e-01 2.59130798e-06\n",
            " 2.71674156e-01 6.02763578e-07 5.87187475e-04 1.34658359e-03\n",
            " 9.98112440e-01 9.99999642e-01 9.99989390e-01 1.00000000e+00\n",
            " 5.58283925e-01 9.92855012e-01 9.18829381e-01 1.00000000e+00\n",
            " 9.99442518e-01 9.49122310e-01 4.63714510e-01 6.82948828e-01\n",
            " 9.97012258e-01 5.89980721e-01 4.96056020e-01 1.00000000e+00\n",
            " 9.99100208e-01 1.00000000e+00 9.99995589e-01 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.58423674e-01\n",
            " 7.88046122e-02 9.99994159e-01 9.99999762e-01 9.99999762e-01\n",
            " 7.35716969e-02 9.99606192e-01 9.90389943e-01 7.79550791e-01\n",
            " 5.21630310e-02 9.52017725e-01 9.89222050e-01 7.55902588e-01\n",
            " 3.76543868e-03 9.88643765e-01 8.35289657e-01 9.93903935e-01\n",
            " 6.72873616e-01 7.61944130e-02 1.31409932e-02 4.51386161e-03\n",
            " 2.58684587e-02 4.34265107e-01 7.01593876e-01 1.24431113e-02\n",
            " 1.01430990e-01 6.84753526e-03 1.47711607e-02 3.66225421e-01\n",
            " 1.54594481e-01 9.13305998e-01 9.55959320e-01 2.25245580e-01\n",
            " 9.99999881e-01 9.99832749e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 19 [0/54 (0%)]\tTrain Loss: 0.000524\n",
            "Train Epoch: 19 [10/54 (19%)]\tTrain Loss: 0.000564\n",
            "Train Epoch: 19 [20/54 (37%)]\tTrain Loss: 0.003612\n",
            "Train Epoch: 19 [30/54 (56%)]\tTrain Loss: 0.000247\n",
            "Train Epoch: 19 [40/54 (74%)]\tTrain Loss: 0.000797\n",
            "Train Epoch: 19 [50/54 (93%)]\tTrain Loss: 0.000126\n",
            "\n",
            "Train set: Average loss: 0.0102, Accuracy: 415/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.78858429e-04 1.17826514e-01 6.58829659e-02 4.99192983e-01\n",
            " 8.65246132e-02 1.39771864e-01 3.02508086e-01 3.29466015e-01\n",
            " 8.83872062e-03 1.58420373e-02 3.57479721e-01 2.42058206e-02\n",
            " 3.02445054e-01 4.45075566e-03 1.83651764e-02 3.41914820e-05\n",
            " 6.47248380e-05 2.11109378e-04 1.55139063e-02 3.29530984e-02\n",
            " 4.60311413e-01 6.79383755e-01 8.90446752e-02 9.99674439e-01\n",
            " 7.29991019e-01 7.27834821e-01 9.72584665e-01 9.19136629e-02\n",
            " 7.86573142e-02 2.15927847e-02 1.38764307e-01 6.81632042e-01\n",
            " 4.60251123e-01 4.44833748e-03 1.02263514e-03 6.51280675e-03\n",
            " 5.10724587e-03 3.16523433e-01 3.44907105e-01 2.72321820e-01\n",
            " 1.36013448e-01 2.64556855e-01 2.37299934e-01 1.94257963e-02\n",
            " 2.39662915e-01 6.03616536e-02 1.75786838e-01 1.86169654e-01\n",
            " 4.49881852e-01 6.87561110e-02 4.61905926e-01 3.13898596e-07\n",
            " 2.44508905e-04 1.16461672e-06 1.51577443e-01 5.87327922e-05\n",
            " 3.52792889e-01 1.63669768e-06 4.27639904e-03 1.17583554e-02\n",
            " 8.97654831e-01 9.96928036e-01 9.89046872e-01 9.99740422e-01\n",
            " 6.37162030e-01 9.75353777e-01 9.55313623e-01 9.99855161e-01\n",
            " 9.77894962e-01 8.56797516e-01 4.94604968e-02 1.14382051e-01\n",
            " 9.59125459e-01 3.56433660e-01 5.32158077e-01 9.96125877e-01\n",
            " 9.60702002e-01 9.99993920e-01 9.96757686e-01 9.99982476e-01\n",
            " 1.00000000e+00 9.98170972e-01 1.00000000e+00 8.50440919e-01\n",
            " 9.20655876e-02 9.69806850e-01 9.96548235e-01 9.95986521e-01\n",
            " 8.15805614e-01 9.94049788e-01 8.51995349e-01 8.97813499e-01\n",
            " 2.85651892e-01 9.66416001e-01 9.63718414e-01 6.29859626e-01\n",
            " 6.56256303e-02 9.61877942e-01 6.71339691e-01 6.49560750e-01\n",
            " 6.25179827e-01 1.97953671e-01 1.14452675e-01 5.49059687e-03\n",
            " 1.62340254e-02 1.82023197e-01 6.76334858e-01 2.14353465e-02\n",
            " 2.72580415e-01 8.86334293e-03 2.28172075e-02 3.23062897e-01\n",
            " 1.23136200e-01 9.38537300e-01 3.52324098e-01 1.25286967e-01\n",
            " 9.99994874e-01 9.98076081e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 20 [0/54 (0%)]\tTrain Loss: 0.001142\n",
            "Train Epoch: 20 [10/54 (19%)]\tTrain Loss: 0.002170\n",
            "Train Epoch: 20 [20/54 (37%)]\tTrain Loss: 0.000329\n",
            "Train Epoch: 20 [30/54 (56%)]\tTrain Loss: 0.003441\n",
            "Train Epoch: 20 [40/54 (74%)]\tTrain Loss: 0.005769\n",
            "Train Epoch: 20 [50/54 (93%)]\tTrain Loss: 0.002896\n",
            "\n",
            "Train set: Average loss: 0.0099, Accuracy: 415/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.11605948e-04 4.27094921e-02 3.08542587e-02 1.74690381e-01\n",
            " 2.20965222e-01 7.68183637e-03 1.41452268e-01 2.00890049e-01\n",
            " 8.78447681e-05 1.27221802e-02 4.82191116e-01 2.65755989e-02\n",
            " 4.34934795e-01 3.27768008e-04 4.92103444e-03 3.47967034e-05\n",
            " 1.46242189e-06 3.79804522e-04 1.83398463e-02 5.81070036e-02\n",
            " 9.06293571e-01 7.05502748e-01 9.09876674e-02 9.99936342e-01\n",
            " 6.75180316e-01 8.30094576e-01 9.89915013e-01 7.36340806e-02\n",
            " 1.41343683e-01 3.20674255e-02 4.03337300e-01 9.99967813e-01\n",
            " 8.81610632e-01 5.63047593e-03 3.84607888e-03 2.55085155e-03\n",
            " 3.49066849e-03 4.57943767e-01 7.25901902e-01 7.72984207e-01\n",
            " 6.46485984e-01 5.44461191e-01 3.78014356e-01 6.95935264e-02\n",
            " 4.43431109e-01 4.59476531e-01 6.44272685e-01 8.30709457e-01\n",
            " 8.25816393e-01 4.93944883e-01 8.19572985e-01 4.67356109e-08\n",
            " 2.81711414e-06 2.49810927e-09 2.66829044e-01 1.18635762e-05\n",
            " 1.52319387e-01 3.42632461e-07 1.00599973e-04 2.18978547e-03\n",
            " 9.99591887e-01 9.99968767e-01 9.99984741e-01 1.00000000e+00\n",
            " 2.33228400e-01 9.97927904e-01 9.42363799e-01 1.00000000e+00\n",
            " 9.90314662e-01 9.97540712e-01 4.99150425e-01 7.14472532e-01\n",
            " 9.97069418e-01 6.95985079e-01 5.40245712e-01 9.99996901e-01\n",
            " 9.94734764e-01 9.99999881e-01 9.99992847e-01 9.99999881e-01\n",
            " 1.00000000e+00 9.99869585e-01 1.00000000e+00 9.97600615e-01\n",
            " 5.19839287e-01 9.99993205e-01 9.99991894e-01 9.99999642e-01\n",
            " 4.66446757e-01 9.99830246e-01 8.99580896e-01 9.06902790e-01\n",
            " 2.98561096e-01 8.22804451e-01 9.04264510e-01 4.51231539e-01\n",
            " 9.61738825e-03 9.72633958e-01 8.20125759e-01 9.82605934e-01\n",
            " 6.98172569e-01 8.65250945e-01 2.55064189e-01 9.52182236e-05\n",
            " 3.84914596e-03 1.01728573e-01 1.79736301e-01 2.13280413e-02\n",
            " 1.21346131e-01 5.64166298e-03 1.53463660e-02 2.96753049e-01\n",
            " 9.89174247e-02 9.30357575e-01 9.03086901e-01 2.64916152e-01\n",
            " 9.99998331e-01 9.99765694e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 40 TN= 53 FN= 18 FP= 7\n",
            "TP+FP 47\n",
            "precision 0.851063829787234\n",
            "recall 0.6896551724137931\n",
            "F1 0.761904761904762\n",
            "acc 0.788135593220339\n",
            "AUCp 0.7864942528735632\n",
            "AUC 0.8517241379310345\n",
            "\n",
            " The epoch is 20, average recall: 0.6897, average precision: 0.8511,average F1: 0.7619, average accuracy: 0.7881, average AUC: 0.8517\n",
            "Train Epoch: 21 [0/54 (0%)]\tTrain Loss: 0.004387\n",
            "Train Epoch: 21 [10/54 (19%)]\tTrain Loss: 0.002890\n",
            "Train Epoch: 21 [20/54 (37%)]\tTrain Loss: 0.006743\n",
            "Train Epoch: 21 [30/54 (56%)]\tTrain Loss: 0.000332\n",
            "Train Epoch: 21 [40/54 (74%)]\tTrain Loss: 0.001491\n",
            "Train Epoch: 21 [50/54 (93%)]\tTrain Loss: 0.007692\n",
            "\n",
            "Train set: Average loss: 0.0046, Accuracy: 421/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.44479701e-05 1.41681433e-02 1.40767172e-02 1.14286719e-02\n",
            " 5.12920087e-03 1.46801898e-03 1.26012012e-01 6.87962696e-02\n",
            " 9.74512568e-06 7.73333851e-03 3.61746073e-01 1.73570774e-02\n",
            " 3.23782384e-01 1.28893025e-05 6.27889938e-04 6.77914780e-10\n",
            " 7.15374635e-11 1.85510984e-09 2.07003253e-03 2.77490150e-02\n",
            " 8.96289110e-01 8.52891281e-02 1.16900176e-01 9.99881506e-01\n",
            " 1.46788821e-01 7.07114220e-01 9.88805294e-01 4.06094193e-02\n",
            " 1.35828942e-01 5.30888559e-03 7.38906741e-01 9.99974489e-01\n",
            " 9.10760641e-01 2.02264593e-04 3.99633253e-04 1.38799238e-04\n",
            " 7.43859797e-04 6.13109022e-02 3.48371148e-01 2.69823968e-01\n",
            " 1.54219523e-01 3.10593337e-01 1.16302207e-01 2.52649933e-02\n",
            " 4.42309290e-01 1.59417287e-01 5.33527732e-01 7.05865681e-01\n",
            " 5.93555212e-01 2.55966872e-01 5.08980572e-01 4.62682337e-10\n",
            " 8.32580736e-06 2.83897295e-10 3.37012708e-01 7.59784916e-06\n",
            " 5.64024508e-01 3.10915311e-06 3.27510788e-05 2.38732595e-04\n",
            " 9.99037027e-01 9.99966145e-01 9.99987721e-01 1.00000000e+00\n",
            " 2.43155390e-01 9.99881148e-01 9.29817915e-01 1.00000000e+00\n",
            " 9.96335626e-01 9.97023046e-01 2.72632748e-01 6.35990977e-01\n",
            " 9.99902129e-01 7.99061716e-01 6.86757267e-01 9.99999881e-01\n",
            " 9.95772183e-01 9.99999881e-01 9.99993920e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.99999523e-01 1.00000000e+00 9.67974365e-01\n",
            " 3.28763686e-02 9.99973178e-01 1.00000000e+00 1.00000000e+00\n",
            " 2.70050406e-01 9.95893836e-01 9.24246609e-01 7.55003929e-01\n",
            " 1.25137404e-01 9.41892862e-01 9.67651188e-01 6.13093257e-01\n",
            " 1.03521599e-02 8.44196856e-01 6.69920683e-01 9.47795033e-01\n",
            " 5.13023317e-01 2.13670164e-01 1.16588445e-02 1.12122889e-05\n",
            " 8.10029102e-04 3.07513513e-02 9.89491418e-02 1.49903661e-02\n",
            " 3.30493525e-02 6.15710439e-03 8.94819945e-03 1.54015779e-01\n",
            " 7.66821131e-02 9.48374569e-01 8.77272546e-01 1.25444785e-01\n",
            " 9.99999642e-01 9.99338448e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 22 [0/54 (0%)]\tTrain Loss: 0.000511\n",
            "Train Epoch: 22 [10/54 (19%)]\tTrain Loss: 0.003555\n",
            "Train Epoch: 22 [20/54 (37%)]\tTrain Loss: 0.000124\n",
            "Train Epoch: 22 [30/54 (56%)]\tTrain Loss: 0.000134\n",
            "Train Epoch: 22 [40/54 (74%)]\tTrain Loss: 0.000108\n",
            "Train Epoch: 22 [50/54 (93%)]\tTrain Loss: 0.005688\n",
            "\n",
            "Train set: Average loss: 0.0067, Accuracy: 421/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.62685564e-05 1.52805895e-02 1.24204103e-02 7.63510391e-02\n",
            " 1.87274832e-02 1.15099885e-02 1.57764778e-01 1.88620880e-01\n",
            " 9.62220874e-05 7.26155378e-03 3.52423608e-01 1.51034798e-02\n",
            " 2.74138153e-01 2.29895759e-05 1.32941373e-03 1.26422595e-09\n",
            " 5.07441103e-11 2.41849634e-08 1.66829047e-03 3.34428926e-03\n",
            " 8.49650681e-01 9.36486721e-02 4.22584899e-02 9.98987377e-01\n",
            " 1.41502574e-01 4.12036389e-01 9.70197737e-01 2.37790998e-02\n",
            " 6.58212453e-02 1.01131015e-03 1.25784218e-01 9.99910355e-01\n",
            " 9.05429125e-01 1.68104627e-04 1.97129571e-04 6.88554894e-04\n",
            " 2.00211513e-03 2.26897612e-01 3.51701945e-01 3.23941469e-01\n",
            " 2.03484774e-01 4.84298974e-01 1.55790418e-01 2.13899985e-02\n",
            " 1.90207928e-01 6.57911822e-02 5.94869912e-01 7.75966108e-01\n",
            " 6.32373691e-01 1.73002928e-01 4.57922488e-01 6.96181601e-09\n",
            " 5.87411387e-06 1.41632772e-09 2.89241105e-01 8.36088020e-06\n",
            " 5.73888719e-01 1.16014371e-06 2.78204912e-04 1.51976346e-04\n",
            " 9.95707214e-01 9.99707401e-01 9.99427855e-01 9.99981999e-01\n",
            " 1.64868414e-01 9.99984860e-01 9.95946705e-01 9.99991775e-01\n",
            " 9.88310695e-01 9.98626828e-01 1.31107166e-01 4.51645702e-01\n",
            " 9.98864532e-01 7.17896104e-01 6.53002381e-01 9.99408722e-01\n",
            " 9.61238384e-01 9.99863029e-01 9.99782264e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.98163640e-01 9.99999642e-01 9.27063584e-01\n",
            " 3.60022560e-02 9.98644531e-01 9.98946965e-01 9.99968052e-01\n",
            " 3.15633267e-01 9.95631456e-01 9.04214799e-01 9.43566084e-01\n",
            " 4.23556685e-01 9.12419736e-01 8.57139647e-01 4.95363116e-01\n",
            " 3.90878394e-02 8.70045245e-01 7.64453232e-01 9.16852891e-01\n",
            " 5.26753366e-01 4.30732399e-01 1.78504940e-02 5.81768290e-06\n",
            " 8.69355805e-04 1.05307056e-02 7.40068108e-02 2.96034180e-02\n",
            " 1.07965261e-01 7.66461249e-03 1.93615425e-02 3.75243455e-01\n",
            " 1.21538416e-01 8.94901335e-01 9.29221869e-01 9.74488631e-02\n",
            " 9.99998450e-01 9.99648213e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
            " 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 23 [0/54 (0%)]\tTrain Loss: 0.002438\n",
            "Train Epoch: 23 [10/54 (19%)]\tTrain Loss: 0.001548\n",
            "Train Epoch: 23 [20/54 (37%)]\tTrain Loss: 0.000101\n",
            "Train Epoch: 23 [30/54 (56%)]\tTrain Loss: 0.001103\n",
            "Train Epoch: 23 [40/54 (74%)]\tTrain Loss: 0.005606\n",
            "Train Epoch: 23 [50/54 (93%)]\tTrain Loss: 0.000121\n",
            "\n",
            "Train set: Average loss: 0.0077, Accuracy: 421/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.83744074e-05 6.18752651e-02 2.62155868e-02 2.20414221e-01\n",
            " 2.32028868e-03 9.91486758e-03 2.36034825e-01 1.80948138e-01\n",
            " 1.71747830e-04 2.71022730e-02 8.30291212e-01 3.18002962e-02\n",
            " 6.32689834e-01 4.84579010e-04 2.49406695e-02 6.39885286e-07\n",
            " 1.41428229e-08 1.12946118e-05 2.86748749e-03 2.72684433e-02\n",
            " 6.55929506e-01 2.05254316e-01 7.84229860e-02 9.97875810e-01\n",
            " 1.82564333e-01 6.63996816e-01 8.24086487e-01 4.13328484e-02\n",
            " 1.15906090e-01 5.61719062e-03 8.10467526e-02 9.99502420e-01\n",
            " 9.95192885e-01 5.23767492e-04 3.67990986e-04 2.87575298e-03\n",
            " 5.88904368e-03 1.92545176e-01 1.07709125e-01 4.13520902e-01\n",
            " 1.33196071e-01 3.18670034e-01 4.66365129e-01 6.13973364e-02\n",
            " 6.87208116e-01 3.90021987e-02 6.74111009e-01 8.93220425e-01\n",
            " 6.39582038e-01 8.79443809e-02 4.58647072e-01 2.16206217e-06\n",
            " 1.09780420e-04 9.18122339e-07 1.42770529e-01 3.24792985e-04\n",
            " 7.42314041e-01 2.28040910e-04 5.99694089e-04 3.83946579e-04\n",
            " 9.96382236e-01 9.99629378e-01 9.99897480e-01 9.99998689e-01\n",
            " 5.50203919e-01 9.99989867e-01 9.80604649e-01 9.99996901e-01\n",
            " 9.86439347e-01 9.48987365e-01 8.60505849e-02 3.80981266e-01\n",
            " 9.96783733e-01 3.78081471e-01 7.37202287e-01 9.99935389e-01\n",
            " 9.92336452e-01 9.99998808e-01 9.99694109e-01 1.00000000e+00\n",
            " 9.99999404e-01 9.98416901e-01 9.99979377e-01 3.82315576e-01\n",
            " 5.31504378e-02 9.98778760e-01 9.99930978e-01 9.99978065e-01\n",
            " 4.06707019e-01 9.85192776e-01 4.76736397e-01 9.81498063e-01\n",
            " 8.54254127e-01 9.12518859e-01 8.32584620e-01 5.38123727e-01\n",
            " 2.29533464e-01 7.95432806e-01 7.44761407e-01 9.55007017e-01\n",
            " 2.91455060e-01 6.89224184e-01 1.11123566e-02 7.37053459e-04\n",
            " 5.89522487e-03 1.08688492e-02 4.80983406e-02 9.12519991e-02\n",
            " 8.79600644e-02 1.94180962e-02 2.72907261e-02 2.97633082e-01\n",
            " 1.40086934e-01 9.42851245e-01 8.24835896e-01 2.16095924e-01\n",
            " 9.99999881e-01 9.99955177e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 24 [0/54 (0%)]\tTrain Loss: 0.000773\n",
            "Train Epoch: 24 [10/54 (19%)]\tTrain Loss: 0.004872\n",
            "Train Epoch: 24 [20/54 (37%)]\tTrain Loss: 0.003794\n",
            "Train Epoch: 24 [30/54 (56%)]\tTrain Loss: 0.000215\n",
            "Train Epoch: 24 [40/54 (74%)]\tTrain Loss: 0.001465\n",
            "Train Epoch: 24 [50/54 (93%)]\tTrain Loss: 0.002420\n",
            "\n",
            "Train set: Average loss: 0.0108, Accuracy: 414/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.24322027e-04 6.28974959e-02 3.29814926e-02 1.73937473e-02\n",
            " 2.48097908e-03 2.12607556e-03 3.26210409e-01 4.11374606e-02\n",
            " 1.51797765e-04 2.72642002e-02 5.42389214e-01 3.47332992e-02\n",
            " 2.18314365e-01 4.07544576e-04 3.83124058e-03 2.42875558e-06\n",
            " 2.83032546e-06 2.86299382e-05 6.13275915e-02 2.64925696e-02\n",
            " 9.28775370e-01 1.19062461e-01 1.68000475e-01 9.99552906e-01\n",
            " 2.08815485e-01 5.81039488e-01 9.66193616e-01 2.14627720e-02\n",
            " 5.58610708e-02 3.72951012e-03 3.92122678e-02 9.96412098e-01\n",
            " 7.43639290e-01 1.83509302e-03 6.45900320e-04 3.96280084e-03\n",
            " 6.94044493e-03 2.92765666e-02 4.88236658e-02 1.35334715e-01\n",
            " 5.27363606e-02 1.44767568e-01 3.11784372e-02 2.47413013e-03\n",
            " 9.21168029e-02 1.10921510e-01 3.33218694e-01 6.75405741e-01\n",
            " 5.78409195e-01 2.19926581e-01 5.27773976e-01 3.69524751e-05\n",
            " 4.08517401e-04 9.58543842e-06 5.11526167e-01 1.09731068e-03\n",
            " 7.97669947e-01 6.94807153e-04 1.16073142e-03 2.12127110e-04\n",
            " 9.99770463e-01 9.99920726e-01 9.99917150e-01 9.99996424e-01\n",
            " 1.22486062e-01 9.92802918e-01 8.18345189e-01 9.88989055e-01\n",
            " 9.57905650e-01 9.90260899e-01 2.58834720e-01 6.33251011e-01\n",
            " 9.92849946e-01 7.46070981e-01 2.88799733e-01 9.98005927e-01\n",
            " 9.65798914e-01 9.99992847e-01 9.97751415e-01 9.99993443e-01\n",
            " 1.00000000e+00 9.99895334e-01 9.99998689e-01 2.80189484e-01\n",
            " 1.55126020e-01 9.94022906e-01 9.99187291e-01 9.99168038e-01\n",
            " 3.05187255e-01 9.98584509e-01 9.93913352e-01 9.14873600e-01\n",
            " 1.75438538e-01 9.50289130e-01 9.58637595e-01 6.50841534e-01\n",
            " 2.42268845e-01 8.32757831e-01 9.25637782e-01 9.57287371e-01\n",
            " 4.60880935e-01 6.16304696e-01 1.86420158e-02 8.88035051e-04\n",
            " 9.02668387e-03 2.06985787e-01 1.91268131e-01 4.89258617e-02\n",
            " 7.47603476e-02 2.39031985e-02 2.99669243e-02 2.38596529e-01\n",
            " 2.17822358e-01 9.72346902e-01 9.30741072e-01 1.84885949e-01\n",
            " 9.99999404e-01 9.99987841e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 25 [0/54 (0%)]\tTrain Loss: 0.000377\n",
            "Train Epoch: 25 [10/54 (19%)]\tTrain Loss: 0.009265\n",
            "Train Epoch: 25 [20/54 (37%)]\tTrain Loss: 0.000165\n",
            "Train Epoch: 25 [30/54 (56%)]\tTrain Loss: 0.001383\n",
            "Train Epoch: 25 [40/54 (74%)]\tTrain Loss: 0.007477\n",
            "Train Epoch: 25 [50/54 (93%)]\tTrain Loss: 0.001331\n",
            "\n",
            "Train set: Average loss: 0.0056, Accuracy: 418/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [8.19734007e-04 5.28003536e-02 4.77270894e-02 1.95318963e-02\n",
            " 2.23279814e-03 2.68254261e-02 2.68344551e-01 6.60656095e-02\n",
            " 5.80292428e-04 2.45850589e-02 9.71667290e-01 3.41485403e-02\n",
            " 5.27676940e-01 1.07799331e-03 7.77141983e-03 1.34873744e-06\n",
            " 5.93764469e-07 6.90088200e-04 3.36899702e-03 5.33230137e-04\n",
            " 4.26574737e-01 4.38015401e-01 1.45456836e-01 9.99986053e-01\n",
            " 4.22282994e-01 8.45761120e-01 9.81989026e-01 2.66106781e-02\n",
            " 4.57756221e-02 1.67294929e-03 1.02497093e-01 9.99991059e-01\n",
            " 9.75870848e-01 6.67777029e-04 1.81127369e-04 1.84143789e-03\n",
            " 6.47515664e-03 2.49016732e-01 2.61470348e-01 6.94737375e-01\n",
            " 5.80909610e-01 6.37407005e-01 3.45930994e-01 8.78288131e-03\n",
            " 8.49657416e-01 1.84804544e-01 9.48675156e-01 9.72225785e-01\n",
            " 9.90000963e-01 4.66684163e-01 9.53798175e-01 1.02863544e-06\n",
            " 4.78716320e-05 8.25049142e-07 5.11010699e-02 1.88899867e-05\n",
            " 8.91365528e-01 5.81896484e-06 5.60178771e-04 2.37493485e-04\n",
            " 9.99989152e-01 9.99998093e-01 9.99892592e-01 9.99999046e-01\n",
            " 4.73344445e-01 9.99990702e-01 9.80561316e-01 1.00000000e+00\n",
            " 9.93059039e-01 9.98100102e-01 1.58710945e-02 2.33000278e-01\n",
            " 9.99816835e-01 9.74928439e-01 5.38331032e-01 9.99972463e-01\n",
            " 9.99980927e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99261081e-01 1.00000000e+00 9.77487028e-01\n",
            " 7.09254682e-01 1.00000000e+00 9.99932885e-01 9.99999762e-01\n",
            " 3.51307452e-01 9.99972701e-01 9.83165026e-01 9.98990238e-01\n",
            " 5.67539334e-01 9.83395576e-01 9.64139342e-01 6.97881460e-01\n",
            " 2.06343248e-01 9.01877224e-01 7.99758434e-01 9.87583101e-01\n",
            " 6.96833014e-01 9.95405197e-01 1.12575933e-01 1.45137485e-03\n",
            " 3.80732189e-03 6.71322420e-02 1.47515312e-01 1.41817629e-01\n",
            " 1.67322844e-01 1.15818707e-02 3.24246734e-02 3.26167494e-01\n",
            " 4.12994802e-01 9.95356143e-01 9.78081644e-01 3.94522220e-01\n",
            " 1.00000000e+00 9.99999881e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 26 [0/54 (0%)]\tTrain Loss: 0.000505\n",
            "Train Epoch: 26 [10/54 (19%)]\tTrain Loss: 0.000511\n",
            "Train Epoch: 26 [20/54 (37%)]\tTrain Loss: 0.000095\n",
            "Train Epoch: 26 [30/54 (56%)]\tTrain Loss: 0.001749\n",
            "Train Epoch: 26 [40/54 (74%)]\tTrain Loss: 0.001512\n",
            "Train Epoch: 26 [50/54 (93%)]\tTrain Loss: 0.000287\n",
            "\n",
            "Train set: Average loss: 0.0039, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.44789087e-04 5.65770790e-02 3.22839022e-02 1.22895809e-02\n",
            " 3.02852038e-03 4.92526963e-03 3.33573312e-01 2.33456921e-02\n",
            " 1.67748047e-04 1.22577185e-02 9.86260951e-01 2.36577373e-02\n",
            " 5.78707099e-01 3.06850486e-03 2.61974819e-02 1.28813890e-05\n",
            " 9.46856289e-06 2.30087753e-04 3.72299249e-03 1.80620723e-03\n",
            " 5.74413240e-01 2.30256855e-01 2.25221962e-01 9.99919653e-01\n",
            " 1.79796159e-01 8.19263279e-01 9.76613820e-01 2.69954447e-02\n",
            " 1.31634936e-01 5.59413573e-03 8.34970832e-01 9.99986053e-01\n",
            " 9.94113743e-01 2.48464267e-03 7.80856994e-04 5.90433413e-03\n",
            " 8.57544225e-03 7.95012861e-02 2.54807472e-01 3.36094767e-01\n",
            " 1.77908018e-01 2.11811036e-01 1.51072621e-01 1.44065060e-02\n",
            " 8.57122302e-01 3.24191719e-01 8.31150293e-01 9.02883172e-01\n",
            " 8.46430421e-01 3.48292857e-01 8.21557820e-01 6.27388488e-07\n",
            " 1.24183871e-05 1.18262733e-06 2.88662277e-02 2.86973045e-06\n",
            " 6.77339017e-01 2.33863229e-05 1.84050950e-04 1.44640842e-04\n",
            " 9.99214649e-01 9.99953508e-01 9.99777853e-01 9.99991298e-01\n",
            " 6.70486689e-01 9.99959707e-01 9.30561185e-01 9.99971747e-01\n",
            " 9.71132457e-01 9.98996198e-01 1.57280684e-01 4.80475694e-01\n",
            " 9.99087572e-01 9.56043184e-01 8.80746365e-01 9.99312282e-01\n",
            " 9.99763072e-01 1.00000000e+00 9.99998569e-01 9.99863625e-01\n",
            " 1.00000000e+00 9.91692424e-01 9.99996066e-01 9.40826416e-01\n",
            " 4.16179210e-01 9.99491215e-01 9.95297372e-01 9.99931693e-01\n",
            " 7.98889920e-02 9.96233165e-01 9.87895012e-01 9.43925798e-01\n",
            " 1.09580070e-01 9.60784197e-01 9.28219557e-01 5.53055465e-01\n",
            " 1.63153797e-01 7.44227886e-01 7.37200677e-01 9.41816390e-01\n",
            " 3.43343049e-01 9.26883399e-01 1.44061208e-01 2.02067400e-04\n",
            " 9.52413131e-04 2.13127676e-02 1.47869643e-02 8.17613229e-02\n",
            " 1.56959817e-01 1.73273385e-02 3.16906199e-02 2.74801701e-01\n",
            " 3.96870852e-01 9.88209486e-01 9.81972933e-01 5.45048177e-01\n",
            " 1.00000000e+00 9.99993086e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 27 [0/54 (0%)]\tTrain Loss: 0.000941\n",
            "Train Epoch: 27 [10/54 (19%)]\tTrain Loss: 0.000193\n",
            "Train Epoch: 27 [20/54 (37%)]\tTrain Loss: 0.000364\n",
            "Train Epoch: 27 [30/54 (56%)]\tTrain Loss: 0.000420\n",
            "Train Epoch: 27 [40/54 (74%)]\tTrain Loss: 0.001038\n",
            "Train Epoch: 27 [50/54 (93%)]\tTrain Loss: 0.000222\n",
            "\n",
            "Train set: Average loss: 0.0027, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.76727612e-04 2.38899756e-02 1.24588069e-02 8.16924311e-03\n",
            " 4.91003925e-03 5.73774427e-03 1.30395845e-01 2.24607754e-02\n",
            " 2.56267725e-04 7.36884540e-03 9.32273149e-01 6.80155680e-03\n",
            " 4.22752470e-01 7.03410842e-05 2.03324505e-03 3.58926513e-06\n",
            " 2.23570623e-06 1.62083961e-04 1.07776500e-01 2.20564613e-03\n",
            " 9.64846611e-01 4.82712984e-01 3.86650473e-01 9.99985576e-01\n",
            " 5.37108719e-01 9.47222471e-01 9.90110219e-01 3.09067927e-02\n",
            " 6.54594749e-02 1.45289931e-03 1.14298470e-01 9.99991059e-01\n",
            " 9.61613238e-01 4.04731650e-03 5.97102800e-04 6.05317112e-03\n",
            " 9.37852263e-03 1.79479599e-01 5.49752563e-02 3.16726983e-01\n",
            " 2.76717812e-01 2.01205716e-01 1.88206911e-01 4.80078533e-02\n",
            " 8.40961158e-01 5.91054916e-01 9.67911720e-01 9.75583911e-01\n",
            " 7.78553963e-01 2.46125802e-01 7.16275394e-01 1.84845840e-06\n",
            " 2.01257008e-05 6.45885123e-08 1.56957671e-01 1.54879956e-06\n",
            " 7.37527132e-01 2.01212479e-05 5.88539522e-04 3.62455503e-05\n",
            " 9.99976516e-01 9.99992013e-01 9.99976635e-01 9.99999642e-01\n",
            " 1.42855093e-01 9.99782979e-01 9.08101857e-01 1.00000000e+00\n",
            " 9.94860828e-01 9.98688877e-01 2.65835673e-01 4.82605606e-01\n",
            " 9.97570097e-01 9.12120104e-01 7.17925727e-01 9.99998569e-01\n",
            " 9.96033013e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99989629e-01 1.00000000e+00 9.85289156e-01\n",
            " 6.53994799e-01 9.99999523e-01 9.99998808e-01 1.00000000e+00\n",
            " 9.09237862e-02 9.98920918e-01 9.90286410e-01 9.96702015e-01\n",
            " 1.41322941e-01 9.75326657e-01 9.51671124e-01 6.86723650e-01\n",
            " 3.40524465e-01 8.90676439e-01 9.51701820e-01 9.89054561e-01\n",
            " 3.24228376e-01 9.00922179e-01 2.43398827e-02 4.71452222e-05\n",
            " 3.54257616e-04 1.69902872e-02 5.06855128e-03 1.27980724e-01\n",
            " 8.28547478e-02 1.06357988e-02 2.78839879e-02 3.32951218e-01\n",
            " 3.78690332e-01 9.93806720e-01 9.17309880e-01 2.26847947e-01\n",
            " 1.00000000e+00 9.99999523e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 28 [0/54 (0%)]\tTrain Loss: 0.000175\n",
            "Train Epoch: 28 [10/54 (19%)]\tTrain Loss: 0.001623\n",
            "Train Epoch: 28 [20/54 (37%)]\tTrain Loss: 0.000243\n",
            "Train Epoch: 28 [30/54 (56%)]\tTrain Loss: 0.003642\n",
            "Train Epoch: 28 [40/54 (74%)]\tTrain Loss: 0.001829\n",
            "Train Epoch: 28 [50/54 (93%)]\tTrain Loss: 0.001491\n",
            "\n",
            "Train set: Average loss: 0.0027, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.84932433e-04 2.62280684e-02 1.32180927e-02 2.09852234e-02\n",
            " 8.30543134e-03 1.19049940e-02 1.34631038e-01 5.11376150e-02\n",
            " 2.79247179e-03 1.57359578e-02 9.77919042e-01 1.58570558e-02\n",
            " 5.34347773e-01 1.96660085e-06 6.31569943e-04 8.68693828e-09\n",
            " 4.96446273e-10 2.06537239e-04 1.39502138e-02 9.53198760e-05\n",
            " 8.96230698e-01 1.10774793e-01 8.35315883e-02 9.99965787e-01\n",
            " 2.08459496e-01 7.74533570e-01 9.86717343e-01 1.83208063e-02\n",
            " 3.58318686e-02 3.35011078e-04 9.80683565e-02 9.99080539e-01\n",
            " 9.47618842e-01 4.01211873e-04 7.90997074e-05 6.01886713e-04\n",
            " 2.50940514e-03 1.79398134e-01 5.77447154e-02 1.13849096e-01\n",
            " 1.39861703e-01 6.39051944e-02 1.94987088e-01 2.03631446e-02\n",
            " 5.30359685e-01 5.20500243e-01 9.54835415e-01 9.32873011e-01\n",
            " 8.56684923e-01 3.71121019e-01 8.02715123e-01 4.33130012e-08\n",
            " 8.97947939e-06 7.96050226e-10 4.63728048e-02 1.93264125e-08\n",
            " 5.08835196e-01 2.06924528e-06 1.58254849e-03 7.04956665e-06\n",
            " 9.99958158e-01 9.99991059e-01 9.99940515e-01 9.99999285e-01\n",
            " 4.52108346e-02 9.96564686e-01 9.80973542e-01 9.99795020e-01\n",
            " 9.88470793e-01 9.97369647e-01 3.60082507e-01 5.12975156e-01\n",
            " 9.90331054e-01 8.49608004e-01 7.13236153e-01 9.99327898e-01\n",
            " 9.87546861e-01 9.99999285e-01 9.99998569e-01 9.99995947e-01\n",
            " 1.00000000e+00 9.99791324e-01 1.00000000e+00 9.90307331e-01\n",
            " 3.42420787e-01 9.99859571e-01 9.99998212e-01 9.99999523e-01\n",
            " 3.34815718e-02 9.27542984e-01 9.41941381e-01 9.95394766e-01\n",
            " 4.18948084e-01 9.95555103e-01 9.80833530e-01 7.98224092e-01\n",
            " 6.28113091e-01 8.98087144e-01 6.10908389e-01 9.37392890e-01\n",
            " 5.27378559e-01 8.92115057e-01 9.75366309e-03 1.63241784e-04\n",
            " 9.32929746e-04 8.22749734e-02 1.60628911e-02 7.47918710e-02\n",
            " 7.80899525e-02 7.34319072e-03 2.35707127e-02 4.27059203e-01\n",
            " 4.14619148e-01 9.83327210e-01 9.67160463e-01 4.32150066e-01\n",
            " 1.00000000e+00 9.99999881e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 29 [0/54 (0%)]\tTrain Loss: 0.002604\n",
            "Train Epoch: 29 [10/54 (19%)]\tTrain Loss: 0.000222\n",
            "Train Epoch: 29 [20/54 (37%)]\tTrain Loss: 0.000064\n",
            "Train Epoch: 29 [30/54 (56%)]\tTrain Loss: 0.010684\n",
            "Train Epoch: 29 [40/54 (74%)]\tTrain Loss: 0.000626\n",
            "Train Epoch: 29 [50/54 (93%)]\tTrain Loss: 0.000380\n",
            "\n",
            "Train set: Average loss: 0.0048, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.92404099e-04 3.32175083e-02 1.08302170e-02 5.36211692e-02\n",
            " 4.78662085e-03 8.88013933e-03 2.91633278e-01 2.69265790e-02\n",
            " 3.44893802e-03 4.41995375e-02 9.63868916e-01 2.86728796e-02\n",
            " 5.65442741e-01 1.10460087e-05 2.92243640e-04 5.43501990e-08\n",
            " 1.35906168e-06 1.31623698e-02 4.44190614e-02 1.53703997e-02\n",
            " 9.89716351e-01 4.09321040e-01 1.14643358e-01 9.99999881e-01\n",
            " 4.44048971e-01 9.54701185e-01 9.93875206e-01 2.36136727e-02\n",
            " 1.72628518e-02 5.34882536e-04 3.79019901e-02 5.84171355e-01\n",
            " 2.23021403e-01 2.02230713e-03 4.88441146e-04 1.08658173e-03\n",
            " 3.94452969e-03 1.18497171e-01 1.32971341e-02 1.81818157e-02\n",
            " 1.30006745e-02 1.93396732e-02 2.10710570e-01 3.08895065e-03\n",
            " 1.62928939e-01 6.32702410e-02 4.92389768e-01 3.08325946e-01\n",
            " 5.56793034e-01 1.86102554e-01 4.46647525e-01 2.16822855e-07\n",
            " 1.55009966e-05 7.29235872e-09 2.71515660e-02 5.01815634e-07\n",
            " 4.75223452e-01 2.75071343e-05 1.15733799e-02 9.59445606e-06\n",
            " 9.99934912e-01 9.99997020e-01 9.99652267e-01 9.99997854e-01\n",
            " 7.16109693e-01 9.93915856e-01 8.91441286e-01 9.99838233e-01\n",
            " 9.93654132e-01 9.81572270e-01 5.60621433e-02 9.89161506e-02\n",
            " 9.39702630e-01 3.71156424e-01 7.49268055e-01 9.99842644e-01\n",
            " 9.98263538e-01 9.99999642e-01 9.99966621e-01 9.97864902e-01\n",
            " 1.00000000e+00 9.99989748e-01 1.00000000e+00 9.49418604e-01\n",
            " 3.81977797e-01 9.98431265e-01 9.99992847e-01 9.99992132e-01\n",
            " 4.66436148e-01 9.74874854e-01 9.55651462e-01 9.14523125e-01\n",
            " 3.84728432e-01 9.99102712e-01 9.90941465e-01 9.23862815e-01\n",
            " 6.96024477e-01 8.52633834e-01 4.27708805e-01 8.71981621e-01\n",
            " 5.85057795e-01 6.32551968e-01 9.78137180e-03 4.68069993e-05\n",
            " 1.78240531e-04 1.03545196e-01 2.80490960e-03 5.92120849e-02\n",
            " 6.21048212e-02 5.64116426e-03 2.52638999e-02 4.30288106e-01\n",
            " 3.19778711e-01 7.46532023e-01 6.03400707e-01 1.30435273e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 30 [0/54 (0%)]\tTrain Loss: 0.001862\n",
            "Train Epoch: 30 [10/54 (19%)]\tTrain Loss: 0.000116\n",
            "Train Epoch: 30 [20/54 (37%)]\tTrain Loss: 0.000276\n",
            "Train Epoch: 30 [30/54 (56%)]\tTrain Loss: 0.000164\n",
            "Train Epoch: 30 [40/54 (74%)]\tTrain Loss: 0.013818\n",
            "Train Epoch: 30 [50/54 (93%)]\tTrain Loss: 0.001865\n",
            "\n",
            "Train set: Average loss: 0.0036, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.65580958e-05 3.71149629e-02 6.40044501e-03 1.90591544e-01\n",
            " 2.01384965e-02 1.44362049e-02 2.35101432e-01 1.26189545e-01\n",
            " 3.52051132e-03 4.56140637e-02 9.61246848e-01 2.04143822e-02\n",
            " 6.10104561e-01 3.35494078e-05 4.03372542e-04 2.12719122e-08\n",
            " 4.90623643e-06 4.04446907e-02 9.24924314e-02 1.59885269e-02\n",
            " 9.82040286e-01 6.72677040e-01 3.58094931e-01 9.99998450e-01\n",
            " 8.61898124e-01 9.75065649e-01 9.91206706e-01 3.71559374e-02\n",
            " 2.82538403e-02 2.17964663e-03 5.78950085e-02 9.83265996e-01\n",
            " 7.38569558e-01 6.08678628e-03 1.32669543e-03 1.89017900e-03\n",
            " 7.58301979e-03 6.35468185e-01 1.08068958e-01 4.22978222e-01\n",
            " 1.55267224e-01 1.05423957e-01 4.68791932e-01 9.46086273e-03\n",
            " 4.29730564e-01 1.06794685e-01 6.34225667e-01 6.13398850e-01\n",
            " 5.25219560e-01 1.66506976e-01 3.61498296e-01 1.54557114e-07\n",
            " 7.49079300e-06 1.43159429e-09 6.06380291e-02 5.22552227e-07\n",
            " 4.61808294e-01 1.28708534e-05 1.24582918e-02 1.28174233e-05\n",
            " 9.99674678e-01 9.99957323e-01 9.99276698e-01 9.99997854e-01\n",
            " 9.88628864e-01 9.99998450e-01 9.93498802e-01 1.00000000e+00\n",
            " 9.98020887e-01 9.98045802e-01 2.77556274e-02 7.29241818e-02\n",
            " 9.95532632e-01 5.46526670e-01 9.08888996e-01 9.99991417e-01\n",
            " 9.95709181e-01 9.99996781e-01 9.99997258e-01 9.99999642e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.61027503e-01\n",
            " 3.75553548e-01 9.99606669e-01 9.99877214e-01 9.99922156e-01\n",
            " 6.82844579e-01 9.98235106e-01 9.92361009e-01 9.68548119e-01\n",
            " 6.08888209e-01 9.95087445e-01 9.67802942e-01 8.22759092e-01\n",
            " 5.72285712e-01 9.18940485e-01 8.13238323e-01 9.69180942e-01\n",
            " 7.86009073e-01 7.14247763e-01 2.37424579e-02 2.51924666e-06\n",
            " 4.34432332e-05 1.69681609e-02 5.96380793e-04 9.66216028e-02\n",
            " 1.14047505e-01 3.08711547e-03 4.51083854e-02 5.98805964e-01\n",
            " 4.40725267e-01 8.57674956e-01 6.18709505e-01 1.14419967e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 39 TN= 47 FN= 19 FP= 13\n",
            "TP+FP 52\n",
            "precision 0.75\n",
            "recall 0.6724137931034483\n",
            "F1 0.709090909090909\n",
            "acc 0.7288135593220338\n",
            "AUCp 0.7278735632183909\n",
            "AUC 0.831896551724138\n",
            "\n",
            " The epoch is 30, average recall: 0.6724, average precision: 0.7500,average F1: 0.7091, average accuracy: 0.7288, average AUC: 0.8319\n",
            "Train Epoch: 31 [0/54 (0%)]\tTrain Loss: 0.000104\n",
            "Train Epoch: 31 [10/54 (19%)]\tTrain Loss: 0.000437\n",
            "Train Epoch: 31 [20/54 (37%)]\tTrain Loss: 0.000178\n",
            "Train Epoch: 31 [30/54 (56%)]\tTrain Loss: 0.000162\n",
            "Train Epoch: 31 [40/54 (74%)]\tTrain Loss: 0.000298\n",
            "Train Epoch: 31 [50/54 (93%)]\tTrain Loss: 0.000113\n",
            "\n",
            "Train set: Average loss: 0.0023, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.76002068e-05 2.80523784e-02 4.31995746e-03 8.97338614e-03\n",
            " 6.38368074e-04 1.20892958e-03 1.33767039e-01 8.65677278e-03\n",
            " 2.19836365e-04 6.59553260e-02 9.88948107e-01 7.62997149e-03\n",
            " 4.65262949e-01 9.52324172e-06 4.70568892e-04 2.08212314e-09\n",
            " 5.64523418e-07 3.06585425e-05 5.86198876e-03 1.42079475e-03\n",
            " 7.69499063e-01 4.35732543e-01 1.06608637e-01 9.99972820e-01\n",
            " 2.80391634e-01 8.36592555e-01 9.86062706e-01 3.10506728e-02\n",
            " 1.99674331e-02 3.22344317e-03 6.71876073e-02 9.86644328e-01\n",
            " 9.22253847e-01 1.88420876e-03 7.75019813e-04 1.27071014e-03\n",
            " 7.00596441e-03 5.19296050e-01 3.50239351e-02 2.64053077e-01\n",
            " 9.65955481e-02 1.45332590e-01 6.08210713e-02 5.46368537e-03\n",
            " 8.73391479e-02 3.80901784e-01 9.39980507e-01 8.56068552e-01\n",
            " 6.87580347e-01 1.53242826e-01 3.72376859e-01 1.88801206e-08\n",
            " 4.67786421e-10 3.82962672e-12 4.53705294e-03 8.43586037e-08\n",
            " 2.96677709e-01 2.20912781e-07 1.79850170e-03 5.28908508e-08\n",
            " 9.99986649e-01 9.99999642e-01 9.99998808e-01 1.00000000e+00\n",
            " 7.22746909e-01 9.99898076e-01 8.27862322e-01 1.00000000e+00\n",
            " 9.95692194e-01 9.98799682e-01 1.91255301e-01 3.92007172e-01\n",
            " 7.17684448e-01 4.62347388e-01 8.85869861e-01 9.99999881e-01\n",
            " 9.99999285e-01 1.00000000e+00 1.00000000e+00 9.99999881e-01\n",
            " 1.00000000e+00 9.99999523e-01 1.00000000e+00 9.45598543e-01\n",
            " 3.94068182e-01 9.99990582e-01 9.99845028e-01 9.99927402e-01\n",
            " 2.06861049e-01 9.85307574e-01 9.97854769e-01 7.02790499e-01\n",
            " 8.05803165e-02 9.91009116e-01 9.68205154e-01 5.62816322e-01\n",
            " 4.26565856e-02 8.58800948e-01 7.48728037e-01 9.69504774e-01\n",
            " 6.39089704e-01 7.90978670e-01 4.11384106e-02 9.87697302e-09\n",
            " 1.33471997e-06 3.98260163e-04 5.07881225e-04 1.12528972e-01\n",
            " 7.10117742e-02 3.25108622e-03 2.17357054e-02 3.75181913e-01\n",
            " 3.98497283e-01 7.27160454e-01 7.12040782e-01 5.19545496e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 32 [0/54 (0%)]\tTrain Loss: 0.000206\n",
            "Train Epoch: 32 [10/54 (19%)]\tTrain Loss: 0.000308\n",
            "Train Epoch: 32 [20/54 (37%)]\tTrain Loss: 0.000057\n",
            "Train Epoch: 32 [30/54 (56%)]\tTrain Loss: 0.000957\n",
            "Train Epoch: 32 [40/54 (74%)]\tTrain Loss: 0.001316\n",
            "Train Epoch: 32 [50/54 (93%)]\tTrain Loss: 0.000181\n",
            "\n",
            "Train set: Average loss: 0.0017, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.90883247e-06 1.42896557e-02 3.72100482e-03 5.24989422e-03\n",
            " 7.86165881e-04 1.20619385e-04 1.25195533e-01 1.48034357e-02\n",
            " 2.49467630e-05 1.33824140e-01 9.96059418e-01 2.44763400e-02\n",
            " 7.86081731e-01 3.81536620e-06 2.01481205e-04 1.83527804e-09\n",
            " 6.23132763e-08 1.47039507e-04 6.69876695e-01 7.34180138e-02\n",
            " 9.85859334e-01 6.37143791e-01 9.67492759e-01 9.99669075e-01\n",
            " 8.29787016e-01 9.87640560e-01 9.85437512e-01 1.81332409e-01\n",
            " 6.08461559e-01 1.57962460e-02 2.85151958e-01 9.99764383e-01\n",
            " 9.91929889e-01 5.10846777e-03 9.46105691e-04 1.43946160e-03\n",
            " 6.52637193e-03 8.58078480e-01 1.02094106e-01 4.65645880e-01\n",
            " 3.39094132e-01 3.23080748e-01 1.19762838e-01 4.04597700e-01\n",
            " 9.91495490e-01 9.73195314e-01 9.95431781e-01 9.71597314e-01\n",
            " 4.76541996e-01 5.41220605e-01 3.09502482e-01 3.70535133e-07\n",
            " 1.37098596e-06 5.31805400e-09 8.18173766e-01 3.30762775e-03\n",
            " 4.84005183e-01 3.61822662e-04 1.75634841e-03 2.68598768e-07\n",
            " 9.99997973e-01 1.00000000e+00 9.99961257e-01 9.99999881e-01\n",
            " 8.80112171e-01 9.99869347e-01 9.56779480e-01 1.00000000e+00\n",
            " 9.94283855e-01 9.99020576e-01 1.67677477e-01 5.06307781e-01\n",
            " 9.99927521e-01 9.77966547e-01 9.92322743e-01 1.00000000e+00\n",
            " 9.99919653e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 9.99997258e-01 9.93774891e-01\n",
            " 7.71939516e-01 9.99998689e-01 9.99921203e-01 9.99919295e-01\n",
            " 8.03264201e-01 9.94230390e-01 9.99319792e-01 9.70156789e-01\n",
            " 3.70096803e-01 9.96839762e-01 9.97605801e-01 8.82424116e-01\n",
            " 8.49936128e-01 8.32174182e-01 9.14479673e-01 9.76707637e-01\n",
            " 5.56963325e-01 7.75157452e-01 6.08478785e-02 4.58799860e-07\n",
            " 1.04878098e-03 1.32445171e-02 3.99673067e-04 1.66540131e-01\n",
            " 6.87333345e-02 7.04895705e-03 1.33444211e-02 1.95258126e-01\n",
            " 7.64316976e-01 9.93291259e-01 7.97065675e-01 6.94495320e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 33 [0/54 (0%)]\tTrain Loss: 0.001090\n",
            "Train Epoch: 33 [10/54 (19%)]\tTrain Loss: 0.000025\n",
            "Train Epoch: 33 [20/54 (37%)]\tTrain Loss: 0.001094\n",
            "Train Epoch: 33 [30/54 (56%)]\tTrain Loss: 0.078141\n",
            "Train Epoch: 33 [40/54 (74%)]\tTrain Loss: 0.012663\n",
            "Train Epoch: 33 [50/54 (93%)]\tTrain Loss: 0.001166\n",
            "\n",
            "Train set: Average loss: 0.0094, Accuracy: 416/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [8.37938715e-05 3.59710157e-02 1.65468659e-02 8.53794962e-02\n",
            " 3.79303396e-02 7.31335059e-02 2.88450927e-01 2.04943180e-01\n",
            " 3.04794014e-02 3.32681984e-01 9.99302149e-01 1.38435394e-01\n",
            " 5.60937107e-01 1.23057724e-03 2.99196085e-03 1.24671260e-08\n",
            " 5.95606107e-05 4.35902679e-04 7.58558363e-02 5.44676244e-01\n",
            " 9.31972623e-01 7.27199465e-02 5.95829666e-01 9.99935985e-01\n",
            " 2.42356211e-01 5.98290324e-01 9.85365510e-01 6.33134842e-02\n",
            " 8.64641219e-02 9.39184893e-03 9.77086276e-02 6.52070284e-01\n",
            " 6.04923606e-01 2.96267611e-03 7.64894183e-04 8.39619152e-03\n",
            " 1.52394297e-02 4.08206940e-01 2.08678842e-01 2.00086683e-01\n",
            " 6.83588013e-02 7.03393817e-02 1.04338385e-01 1.29531100e-02\n",
            " 7.11389005e-01 5.97108066e-01 3.64050061e-01 1.98287278e-01\n",
            " 3.62888128e-01 9.58803356e-01 4.85019267e-01 3.31523297e-05\n",
            " 4.12977720e-03 1.03366228e-05 5.86768806e-01 5.88320987e-03\n",
            " 9.35293913e-01 9.88645293e-03 2.12667752e-02 2.73904414e-04\n",
            " 9.99323606e-01 9.99999881e-01 9.99999642e-01 1.00000000e+00\n",
            " 9.77354705e-01 9.99541521e-01 9.65651751e-01 9.99986410e-01\n",
            " 9.79176641e-01 9.94611502e-01 2.66975686e-02 7.92401209e-02\n",
            " 9.73032296e-01 9.56951916e-01 9.96132970e-01 9.99986291e-01\n",
            " 9.99962926e-01 1.00000000e+00 1.00000000e+00 9.95577455e-01\n",
            " 9.80710804e-01 9.99003589e-01 1.00000000e+00 9.90109205e-01\n",
            " 8.36672485e-01 9.97778237e-01 9.98049974e-01 9.99936819e-01\n",
            " 9.39436495e-01 9.62199926e-01 9.64222252e-01 8.44546974e-01\n",
            " 1.55106947e-01 9.65913475e-01 8.24430168e-01 2.51033455e-01\n",
            " 6.94297791e-01 9.70765710e-01 8.89559627e-01 7.84220278e-01\n",
            " 8.44903290e-01 7.70935893e-01 3.30651015e-01 4.35156295e-07\n",
            " 1.15385908e-03 8.81482475e-03 2.71716565e-02 8.32436234e-02\n",
            " 2.14365032e-02 1.44427475e-02 1.65045075e-02 3.57437491e-01\n",
            " 2.88177043e-01 9.73109424e-01 5.31012416e-01 4.27373290e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 34 [0/54 (0%)]\tTrain Loss: 0.002388\n",
            "Train Epoch: 34 [10/54 (19%)]\tTrain Loss: 0.000309\n",
            "Train Epoch: 34 [20/54 (37%)]\tTrain Loss: 0.010980\n",
            "Train Epoch: 34 [30/54 (56%)]\tTrain Loss: 0.000241\n",
            "Train Epoch: 34 [40/54 (74%)]\tTrain Loss: 0.004811\n",
            "Train Epoch: 34 [50/54 (93%)]\tTrain Loss: 0.001098\n",
            "\n",
            "Train set: Average loss: 0.0030, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.77812350e-05 5.68192415e-02 1.63413696e-02 3.34645398e-02\n",
            " 1.09524261e-02 2.35211089e-01 2.88360626e-01 6.87813044e-01\n",
            " 2.47619018e-01 9.54780802e-02 9.99009132e-01 2.73993742e-02\n",
            " 6.69675231e-01 3.38524692e-02 6.69514239e-02 8.54920579e-10\n",
            " 8.74277612e-06 3.27704358e-03 1.10770799e-02 9.82791409e-02\n",
            " 8.94481421e-01 7.78006688e-02 5.16313195e-01 9.99858499e-01\n",
            " 5.03357232e-01 4.53823179e-01 9.73042011e-01 1.31632369e-02\n",
            " 5.26785515e-02 1.61203707e-03 6.03052117e-02 9.95612979e-01\n",
            " 9.87277210e-01 7.25906517e-04 2.99822626e-04 7.06092920e-03\n",
            " 5.40998299e-03 3.72038603e-01 6.92271367e-02 5.79135418e-01\n",
            " 1.04242057e-01 1.49493173e-01 3.13915201e-02 7.43227638e-03\n",
            " 8.17232788e-01 3.55898827e-01 7.06140995e-01 3.99406135e-01\n",
            " 3.50995630e-01 2.34421521e-01 5.11929631e-01 1.41512089e-06\n",
            " 1.28192274e-04 2.18891770e-07 5.55281758e-01 6.91094494e-04\n",
            " 8.47788870e-01 2.16216009e-04 5.93001256e-03 1.59504256e-04\n",
            " 9.90277827e-01 9.99919176e-01 9.99434769e-01 9.99986053e-01\n",
            " 9.99746025e-01 9.99986529e-01 9.99820173e-01 9.99999166e-01\n",
            " 9.99538302e-01 9.99508858e-01 8.10168907e-02 2.65689582e-01\n",
            " 9.99943376e-01 9.93754983e-01 9.99207675e-01 9.99923110e-01\n",
            " 9.99812543e-01 1.00000000e+00 1.00000000e+00 9.99999523e-01\n",
            " 1.00000000e+00 9.99915004e-01 1.00000000e+00 9.83087599e-01\n",
            " 3.05534422e-01 9.99909759e-01 9.99798238e-01 9.99724805e-01\n",
            " 8.75620604e-01 9.99397397e-01 9.89689350e-01 9.15510952e-01\n",
            " 2.79873997e-01 9.67610717e-01 9.45613980e-01 4.76018757e-01\n",
            " 5.99913299e-01 9.58322942e-01 8.77646387e-01 9.77227628e-01\n",
            " 8.53771985e-01 9.81336892e-01 3.45894963e-01 8.64383765e-06\n",
            " 4.45976824e-04 1.98566206e-02 1.58598974e-01 1.11238010e-01\n",
            " 3.53519768e-02 8.53140652e-03 2.32631825e-02 5.17703593e-01\n",
            " 6.33930743e-01 9.90240455e-01 9.90139782e-01 3.53817552e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 35 [0/54 (0%)]\tTrain Loss: 0.000122\n",
            "Train Epoch: 35 [10/54 (19%)]\tTrain Loss: 0.000338\n",
            "Train Epoch: 35 [20/54 (37%)]\tTrain Loss: 0.000053\n",
            "Train Epoch: 35 [30/54 (56%)]\tTrain Loss: 0.002342\n",
            "Train Epoch: 35 [40/54 (74%)]\tTrain Loss: 0.040483\n",
            "Train Epoch: 35 [50/54 (93%)]\tTrain Loss: 0.000011\n",
            "\n",
            "Train set: Average loss: 0.0045, Accuracy: 418/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.11073684e-03 2.18508653e-02 5.00777410e-03 3.83795835e-02\n",
            " 4.93714176e-02 7.30148256e-01 8.69303793e-02 9.56018567e-01\n",
            " 5.04658222e-01 2.04510689e-01 9.95443702e-01 1.94745492e-02\n",
            " 4.27229315e-01 1.13045680e-03 1.90824717e-02 8.15556910e-12\n",
            " 5.39887765e-07 8.77999810e-07 2.73154899e-02 7.05164112e-03\n",
            " 7.68091977e-01 7.25830793e-02 1.86805829e-01 9.99997258e-01\n",
            " 1.35300294e-01 3.07669044e-01 9.96983230e-01 7.06586195e-03\n",
            " 3.36605459e-02 3.26404348e-04 1.47408932e-01 9.84729230e-01\n",
            " 9.93002355e-01 2.85441463e-04 3.99971905e-05 9.32849245e-04\n",
            " 2.98880064e-03 3.87163788e-01 8.73858035e-02 3.73871714e-01\n",
            " 7.38038570e-02 1.24900214e-01 3.37164998e-02 6.15385780e-03\n",
            " 3.24934483e-01 9.01551247e-01 9.14915919e-01 8.81716371e-01\n",
            " 8.87969017e-01 6.72124922e-01 9.62151051e-01 2.05232009e-08\n",
            " 1.38628995e-04 2.29387354e-08 3.42521220e-01 8.13321894e-05\n",
            " 3.14434409e-01 8.86814814e-06 1.22602517e-03 8.94629775e-05\n",
            " 9.99940038e-01 1.00000000e+00 9.99903917e-01 1.00000000e+00\n",
            " 9.93905962e-01 9.99962211e-01 9.99791443e-01 9.99994874e-01\n",
            " 9.99977589e-01 9.99038815e-01 1.89643115e-01 9.43209827e-01\n",
            " 9.99961734e-01 9.91170645e-01 9.75850821e-01 9.99589503e-01\n",
            " 9.99772489e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.58308280e-01\n",
            " 1.30201712e-01 9.99999166e-01 9.99999642e-01 9.99997735e-01\n",
            " 7.68902659e-01 9.98994529e-01 9.99980211e-01 9.78982806e-01\n",
            " 9.05524433e-01 9.94409740e-01 9.92849708e-01 7.40552545e-01\n",
            " 6.33689761e-01 8.96402001e-01 7.27449834e-01 9.16971684e-01\n",
            " 6.58489645e-01 9.94686246e-01 1.28145918e-01 2.50078877e-03\n",
            " 8.29519555e-02 9.94252980e-01 8.61266375e-01 6.49881437e-02\n",
            " 1.33902468e-02 1.68649142e-03 9.56044625e-03 2.36782104e-01\n",
            " 3.28971356e-01 9.04985785e-01 9.95309651e-01 8.47469807e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 36 [0/54 (0%)]\tTrain Loss: 0.000188\n",
            "Train Epoch: 36 [10/54 (19%)]\tTrain Loss: 0.000564\n",
            "Train Epoch: 36 [20/54 (37%)]\tTrain Loss: 0.001726\n",
            "Train Epoch: 36 [30/54 (56%)]\tTrain Loss: 0.000669\n",
            "Train Epoch: 36 [40/54 (74%)]\tTrain Loss: 0.000186\n",
            "Train Epoch: 36 [50/54 (93%)]\tTrain Loss: 0.002300\n",
            "\n",
            "Train set: Average loss: 0.0074, Accuracy: 416/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.43412694e-04 7.50632444e-03 1.27953419e-03 3.05923279e-02\n",
            " 1.61781795e-02 6.16251469e-01 2.48448923e-02 8.11492980e-01\n",
            " 1.00366168e-01 1.96735755e-01 9.99944091e-01 5.27638663e-03\n",
            " 8.18695605e-01 6.82404381e-04 6.01817667e-03 1.68043657e-09\n",
            " 8.83692792e-06 1.68977647e-06 3.33354366e-03 1.80171989e-03\n",
            " 4.53841597e-01 8.95990953e-02 2.00972065e-01 9.99995232e-01\n",
            " 2.81423628e-01 2.06086710e-01 9.90971088e-01 8.61322600e-03\n",
            " 1.13858461e-01 1.46921747e-03 3.00962920e-03 9.83708322e-01\n",
            " 9.76788044e-01 5.04430616e-03 5.53894613e-04 6.57262513e-03\n",
            " 9.41704493e-03 1.82209611e-01 4.40846719e-02 5.83930492e-01\n",
            " 1.18069477e-01 1.48852959e-01 1.46258578e-01 2.52112634e-02\n",
            " 8.98943841e-01 1.90385699e-01 4.68523532e-01 5.20947754e-01\n",
            " 2.78278857e-01 1.83972105e-01 2.87530631e-01 2.13599751e-08\n",
            " 1.94904260e-05 4.92209580e-08 2.72272304e-02 1.53008732e-04\n",
            " 2.39454314e-01 2.67539690e-05 7.70520011e-04 1.56026490e-05\n",
            " 9.99740899e-01 9.99999881e-01 9.99422193e-01 9.99997497e-01\n",
            " 9.55441177e-01 1.00000000e+00 9.99999285e-01 9.99998331e-01\n",
            " 9.99998212e-01 9.99816000e-01 3.51729128e-03 1.05922535e-01\n",
            " 9.80211616e-01 9.86805677e-01 9.67387497e-01 9.99681473e-01\n",
            " 9.77754116e-01 9.99964595e-01 9.99990344e-01 9.99999881e-01\n",
            " 1.00000000e+00 9.99144912e-01 1.00000000e+00 8.82043898e-01\n",
            " 1.94362983e-01 9.92696404e-01 9.99994516e-01 9.99997973e-01\n",
            " 2.35700235e-01 9.99419808e-01 9.93204355e-01 9.98076320e-01\n",
            " 8.62541556e-01 9.99109566e-01 9.98816252e-01 8.30121040e-01\n",
            " 4.99850363e-01 5.21768749e-01 5.71142793e-01 9.71810400e-01\n",
            " 1.84878021e-01 9.21451092e-01 7.07255900e-02 8.08644763e-05\n",
            " 1.51103614e-02 1.36634037e-01 9.42168012e-02 1.20736003e-01\n",
            " 1.42855858e-02 2.33346806e-03 4.06425027e-03 9.56898481e-02\n",
            " 2.18469933e-01 9.97874856e-01 9.74531829e-01 4.17069346e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 37 [0/54 (0%)]\tTrain Loss: 0.000798\n",
            "Train Epoch: 37 [10/54 (19%)]\tTrain Loss: 0.000349\n",
            "Train Epoch: 37 [20/54 (37%)]\tTrain Loss: 0.000599\n",
            "Train Epoch: 37 [30/54 (56%)]\tTrain Loss: 0.025473\n",
            "Train Epoch: 37 [40/54 (74%)]\tTrain Loss: 0.001722\n",
            "Train Epoch: 37 [50/54 (93%)]\tTrain Loss: 0.004609\n",
            "\n",
            "Train set: Average loss: 0.0046, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.65380265e-04 1.85551550e-02 4.06144932e-03 4.85947765e-02\n",
            " 4.58409451e-03 1.18135005e-01 7.00637102e-02 2.02092841e-01\n",
            " 6.11414015e-03 2.58762278e-02 9.78195429e-01 5.38581144e-03\n",
            " 6.94477856e-01 4.21268451e-05 2.56584748e-03 4.86273314e-14\n",
            " 3.50243879e-12 3.48303469e-11 3.96097428e-04 1.69929315e-03\n",
            " 2.02596903e-01 7.50252977e-02 1.57841012e-01 9.99374926e-01\n",
            " 2.37634033e-01 3.00059825e-01 9.87515926e-01 1.25332531e-02\n",
            " 1.40540540e-01 5.03475894e-04 1.26439957e-02 9.86351848e-01\n",
            " 6.61549330e-01 1.18974684e-04 3.72038048e-05 5.24892006e-04\n",
            " 6.62369712e-04 1.50386244e-02 4.32610661e-02 2.48247519e-01\n",
            " 2.97689773e-02 3.59614603e-02 1.17037863e-01 8.20177374e-04\n",
            " 7.34971166e-01 2.31819391e-01 1.16862923e-01 1.77748382e-01\n",
            " 6.16346002e-01 1.76364079e-01 3.49109232e-01 2.84905292e-13\n",
            " 1.32313727e-09 5.66412528e-10 6.99599320e-03 1.35089067e-07\n",
            " 1.32951140e-01 1.40706004e-06 2.58266550e-06 5.03052320e-08\n",
            " 9.70468462e-01 9.99290347e-01 9.96858120e-01 9.99656916e-01\n",
            " 9.97842312e-01 9.99999762e-01 9.99830842e-01 9.99991775e-01\n",
            " 9.50642288e-01 9.90351856e-01 2.17606835e-02 3.83305073e-01\n",
            " 9.83399689e-01 7.69707799e-01 9.18786108e-01 9.99618649e-01\n",
            " 9.99072552e-01 1.00000000e+00 1.00000000e+00 9.99999881e-01\n",
            " 1.00000000e+00 9.99674439e-01 9.99999881e-01 7.08027720e-01\n",
            " 1.09762631e-01 9.93703902e-01 9.99966741e-01 9.99999523e-01\n",
            " 1.53153837e-01 9.87823725e-01 9.77899313e-01 8.50662470e-01\n",
            " 5.15475035e-01 9.87482905e-01 8.25829208e-01 8.38061094e-01\n",
            " 3.88775244e-02 4.50362682e-01 5.98852634e-01 3.22432667e-01\n",
            " 1.48525104e-01 8.75030637e-01 7.48542100e-02 1.73805233e-07\n",
            " 2.81874934e-04 2.77443789e-02 5.55769056e-02 7.44315907e-02\n",
            " 4.23936173e-03 6.90871268e-04 5.87242702e-03 2.05828518e-01\n",
            " 2.01398835e-01 8.20970476e-01 9.48945940e-01 3.74788344e-01\n",
            " 9.99998450e-01 9.99998927e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 38 [0/54 (0%)]\tTrain Loss: 0.000354\n",
            "Train Epoch: 38 [10/54 (19%)]\tTrain Loss: 0.000012\n",
            "Train Epoch: 38 [20/54 (37%)]\tTrain Loss: 0.010910\n",
            "Train Epoch: 38 [30/54 (56%)]\tTrain Loss: 0.000075\n",
            "Train Epoch: 38 [40/54 (74%)]\tTrain Loss: 0.000741\n",
            "Train Epoch: 38 [50/54 (93%)]\tTrain Loss: 0.002455\n",
            "\n",
            "Train set: Average loss: 0.0042, Accuracy: 419/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.90539533e-05 3.91202495e-02 1.13234017e-02 8.26441050e-02\n",
            " 1.26894983e-02 1.00191779e-01 1.94281325e-01 3.47283185e-01\n",
            " 9.18885600e-03 3.33830416e-02 9.82302487e-01 2.58544483e-03\n",
            " 8.59362602e-01 4.97204671e-03 1.52538987e-02 4.82936635e-10\n",
            " 6.10335960e-08 1.51654888e-07 1.75862864e-01 2.25756213e-01\n",
            " 9.64638710e-01 2.57328510e-01 9.59528983e-01 1.00000000e+00\n",
            " 7.21127272e-01 9.91841555e-01 9.99229193e-01 4.58085351e-02\n",
            " 7.20969081e-01 4.21407312e-04 4.41011526e-02 9.99999046e-01\n",
            " 7.11991251e-01 5.02252427e-04 1.94822613e-04 7.30936276e-03\n",
            " 1.86298066e-03 2.08269820e-01 3.31591755e-01 2.39371449e-01\n",
            " 2.26531681e-02 3.26042958e-02 1.65003553e-01 3.99605092e-03\n",
            " 9.98740017e-01 4.81782287e-01 6.20999001e-02 4.96278219e-02\n",
            " 1.09496996e-01 7.02523813e-02 1.70008019e-01 5.34797207e-12\n",
            " 8.16246029e-06 1.70258883e-08 2.96593815e-01 2.42582778e-03\n",
            " 8.60444188e-01 3.67583387e-04 5.40123321e-04 3.26125621e-04\n",
            " 9.99816477e-01 1.00000000e+00 9.99999285e-01 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99634624e-01 9.99828577e-01 8.28833692e-03 1.61905199e-01\n",
            " 1.00000000e+00 9.73082006e-01 9.99675035e-01 1.00000000e+00\n",
            " 9.99999762e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.89375114e-01\n",
            " 1.81351960e-01 9.99998808e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.79121089e-01 9.99999762e-01 9.99806464e-01 9.36038852e-01\n",
            " 8.76884282e-01 9.99902844e-01 9.99984860e-01 9.97865736e-01\n",
            " 9.29933369e-01 7.67692327e-01 9.21867728e-01 6.32785499e-01\n",
            " 6.12201273e-01 7.83155739e-01 1.05629556e-01 3.51743111e-05\n",
            " 9.55514703e-03 1.08336464e-01 4.66709770e-02 4.29365337e-01\n",
            " 1.56364907e-02 3.49500007e-03 1.95371900e-02 3.41302514e-01\n",
            " 9.58058834e-01 1.00000000e+00 9.95640516e-01 5.19584179e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 39 [0/54 (0%)]\tTrain Loss: 0.000481\n",
            "Train Epoch: 39 [10/54 (19%)]\tTrain Loss: 0.000210\n",
            "Train Epoch: 39 [20/54 (37%)]\tTrain Loss: 0.000060\n",
            "Train Epoch: 39 [30/54 (56%)]\tTrain Loss: 0.000463\n",
            "Train Epoch: 39 [40/54 (74%)]\tTrain Loss: 0.003024\n",
            "Train Epoch: 39 [50/54 (93%)]\tTrain Loss: 0.000044\n",
            "\n",
            "Train set: Average loss: 0.0029, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.74315969e-05 1.85525287e-02 8.66030343e-03 9.62753743e-02\n",
            " 3.04709896e-02 8.03677216e-02 1.24897078e-01 3.45799088e-01\n",
            " 4.64510312e-03 8.90954062e-02 9.93933380e-01 4.81776148e-03\n",
            " 9.11685228e-01 7.80985644e-03 1.91868413e-02 7.12323395e-11\n",
            " 7.24996482e-07 8.23167466e-06 7.04718471e-01 1.71173200e-01\n",
            " 9.68747914e-01 6.93600237e-01 9.84199286e-01 1.00000000e+00\n",
            " 9.82919097e-01 9.99599159e-01 9.99989033e-01 6.31555170e-02\n",
            " 5.15741825e-01 4.99041576e-04 9.86723602e-02 9.99999762e-01\n",
            " 8.20566893e-01 7.30621163e-03 1.25413691e-03 1.65191945e-02\n",
            " 3.21248267e-03 6.12698078e-01 9.14624631e-01 8.86770368e-01\n",
            " 3.72271657e-01 2.80938238e-01 3.60270828e-01 2.77586770e-03\n",
            " 9.95603323e-01 5.84730566e-01 1.42471433e-01 4.16971669e-02\n",
            " 1.32166356e-01 1.01109266e-01 2.62173325e-01 1.87394283e-14\n",
            " 2.89224363e-06 8.07833911e-10 5.96278846e-01 1.08991691e-03\n",
            " 9.07232106e-01 6.53307434e-05 7.47625527e-05 2.03051259e-05\n",
            " 9.99992132e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99996781e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.95223463e-01 9.99772966e-01 1.97029598e-02 2.86918223e-01\n",
            " 1.00000000e+00 9.77605045e-01 9.99117792e-01 1.00000000e+00\n",
            " 9.99882698e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99686718e-01\n",
            " 4.39914107e-01 9.99999285e-01 9.99999166e-01 1.00000000e+00\n",
            " 9.60680544e-01 1.00000000e+00 9.99956012e-01 9.89511669e-01\n",
            " 9.55367804e-01 9.99901175e-01 9.99994159e-01 9.95885551e-01\n",
            " 9.77576256e-01 9.60053980e-01 9.14028108e-01 8.84735644e-01\n",
            " 9.23128009e-01 9.83340025e-01 7.81892359e-01 5.31129008e-05\n",
            " 5.66186383e-02 2.99775690e-01 2.52071947e-01 4.64565277e-01\n",
            " 1.59527600e-01 5.34095755e-03 1.80378929e-02 5.13415217e-01\n",
            " 9.86097693e-01 1.00000000e+00 9.99594390e-01 9.03224409e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 40 [0/54 (0%)]\tTrain Loss: 0.000162\n",
            "Train Epoch: 40 [10/54 (19%)]\tTrain Loss: 0.001477\n",
            "Train Epoch: 40 [20/54 (37%)]\tTrain Loss: 0.000031\n",
            "Train Epoch: 40 [30/54 (56%)]\tTrain Loss: 0.000052\n",
            "Train Epoch: 40 [40/54 (74%)]\tTrain Loss: 0.000766\n",
            "Train Epoch: 40 [50/54 (93%)]\tTrain Loss: 0.000891\n",
            "\n",
            "Train set: Average loss: 0.0016, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [7.45056241e-05 8.02403875e-03 5.46650030e-03 2.31318716e-02\n",
            " 4.38919058e-03 1.22604845e-03 4.60279994e-02 1.65565070e-02\n",
            " 7.11764642e-06 1.50148883e-01 9.97820258e-01 4.10388503e-03\n",
            " 9.19745326e-01 8.11222781e-05 1.54136517e-03 1.07033293e-16\n",
            " 4.39249415e-10 2.07545595e-11 3.62113357e-01 6.11276776e-02\n",
            " 8.68961453e-01 2.35928044e-01 9.06645596e-01 1.00000000e+00\n",
            " 6.56237006e-01 9.83277738e-01 9.99749482e-01 1.60825197e-02\n",
            " 2.42319629e-01 6.54146192e-04 1.23145171e-02 9.99216318e-01\n",
            " 7.69959450e-01 2.87391711e-03 5.96524915e-04 3.37079074e-03\n",
            " 1.71744323e-03 2.50289351e-01 1.30737573e-01 6.94358885e-01\n",
            " 3.25095266e-01 1.52793109e-01 5.85621931e-02 2.77250912e-03\n",
            " 9.55259860e-01 9.33593273e-01 3.70900154e-01 1.61972180e-01\n",
            " 2.90772319e-01 3.23098421e-01 4.69900399e-01 1.98434773e-17\n",
            " 1.83197812e-07 3.43254425e-10 3.35592955e-01 5.39573957e-04\n",
            " 4.49251473e-01 1.26291843e-05 4.91574426e-08 4.32590852e-08\n",
            " 9.99718606e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.49250817e-01 9.99953985e-01 9.55164850e-01 1.00000000e+00\n",
            " 9.77952540e-01 9.86937106e-01 1.85010750e-02 5.42617977e-01\n",
            " 9.99958038e-01 4.51587439e-01 9.47589040e-01 9.99855280e-01\n",
            " 9.98323739e-01 9.99999881e-01 9.99959350e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.99964476e-01 1.00000000e+00 9.93433595e-01\n",
            " 4.82681721e-01 9.99994755e-01 9.98472869e-01 9.99977231e-01\n",
            " 5.83637953e-01 9.99777734e-01 9.97927666e-01 9.89656448e-01\n",
            " 7.47112513e-01 9.99179065e-01 9.99628186e-01 9.86437023e-01\n",
            " 7.62177944e-01 8.37574780e-01 7.64307976e-01 6.50627911e-01\n",
            " 8.18421841e-01 9.48540986e-01 1.74516022e-01 4.21125442e-05\n",
            " 4.27107140e-02 3.99067283e-01 4.00982946e-01 4.00418341e-01\n",
            " 3.19210514e-02 3.63811036e-03 9.52493306e-03 2.77694881e-01\n",
            " 9.40567017e-01 9.99383688e-01 9.33523238e-01 3.15911293e-01\n",
            " 9.99999881e-01 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 43 TN= 50 FN= 15 FP= 10\n",
            "TP+FP 53\n",
            "precision 0.8113207547169812\n",
            "recall 0.7413793103448276\n",
            "F1 0.7747747747747747\n",
            "acc 0.788135593220339\n",
            "AUCp 0.7873563218390806\n",
            "AUC 0.8410919540229885\n",
            "\n",
            " The epoch is 40, average recall: 0.7414, average precision: 0.8113,average F1: 0.7748, average accuracy: 0.7881, average AUC: 0.8411\n",
            "Train Epoch: 41 [0/54 (0%)]\tTrain Loss: 0.000075\n",
            "Train Epoch: 41 [10/54 (19%)]\tTrain Loss: 0.000232\n",
            "Train Epoch: 41 [20/54 (37%)]\tTrain Loss: 0.000286\n",
            "Train Epoch: 41 [30/54 (56%)]\tTrain Loss: 0.000099\n",
            "Train Epoch: 41 [40/54 (74%)]\tTrain Loss: 0.000088\n",
            "Train Epoch: 41 [50/54 (93%)]\tTrain Loss: 0.009718\n",
            "\n",
            "Train set: Average loss: 0.0066, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [8.09397956e-04 6.20826939e-03 4.06704657e-03 1.90409124e-02\n",
            " 9.12319683e-03 4.59315255e-03 2.70740930e-02 1.14734741e-02\n",
            " 2.36067663e-05 1.08559303e-01 9.73742306e-01 2.75343214e-03\n",
            " 7.58930683e-01 1.59101607e-03 2.41359719e-03 1.78863124e-09\n",
            " 2.13052249e-06 1.77354020e-10 7.28847735e-05 2.93245493e-03\n",
            " 2.43103608e-01 3.75195220e-02 2.15255782e-01 1.00000000e+00\n",
            " 1.39338225e-01 2.82468319e-01 9.96984661e-01 6.16910635e-03\n",
            " 7.09029660e-03 4.81011026e-04 2.37696781e-03 9.56958771e-01\n",
            " 5.33731803e-02 4.80619119e-03 1.36820436e-03 1.52549949e-02\n",
            " 6.78657228e-03 4.38087471e-02 5.73015846e-02 6.09292269e-01\n",
            " 1.85566947e-01 1.37922645e-01 1.97455343e-02 4.16615774e-04\n",
            " 3.20888668e-01 1.70834195e-02 3.06633692e-02 9.80553869e-03\n",
            " 8.29988271e-02 5.27373254e-02 9.69250202e-02 1.94688536e-12\n",
            " 6.84620289e-08 2.33145485e-08 4.39887447e-03 7.68013433e-06\n",
            " 8.86921287e-02 6.53717962e-07 2.30612002e-07 3.43796160e-06\n",
            " 9.99835253e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 8.71135294e-01 1.00000000e+00 7.91972756e-01 1.00000000e+00\n",
            " 9.89953578e-01 9.06396568e-01 4.92177252e-03 6.26589209e-02\n",
            " 9.99966383e-01 3.77956256e-02 4.47458893e-01 9.99976158e-01\n",
            " 9.99992609e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 8.99689794e-01\n",
            " 1.41820341e-01 1.00000000e+00 9.99732196e-01 9.99999642e-01\n",
            " 1.56490088e-01 9.99999642e-01 9.81063724e-01 9.91912127e-01\n",
            " 6.05993308e-02 9.73401546e-01 9.87289488e-01 6.36628211e-01\n",
            " 3.23448107e-02 8.58564973e-01 4.18898612e-01 6.69156432e-01\n",
            " 8.25631618e-01 6.86225653e-01 9.99782383e-02 1.24381804e-05\n",
            " 1.31727837e-04 1.65133476e-02 1.78817790e-02 3.55528027e-01\n",
            " 4.13315557e-02 2.20191316e-03 1.18593508e-02 2.11778224e-01\n",
            " 7.42291570e-01 9.98356521e-01 8.46158981e-01 1.67870596e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 42 [0/54 (0%)]\tTrain Loss: 0.008675\n",
            "Train Epoch: 42 [10/54 (19%)]\tTrain Loss: 0.012094\n",
            "Train Epoch: 42 [20/54 (37%)]\tTrain Loss: 0.000106\n",
            "Train Epoch: 42 [30/54 (56%)]\tTrain Loss: 0.000727\n",
            "Train Epoch: 42 [40/54 (74%)]\tTrain Loss: 0.000117\n",
            "Train Epoch: 42 [50/54 (93%)]\tTrain Loss: 0.000147\n",
            "\n",
            "Train set: Average loss: 0.0017, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.18591495e-04 1.49886915e-02 8.44463613e-03 2.00942717e-02\n",
            " 1.23271337e-02 6.61178317e-04 7.24147186e-02 9.35799442e-03\n",
            " 2.51340765e-07 2.63874590e-01 9.99017596e-01 7.74477422e-03\n",
            " 9.00436342e-01 4.56825946e-04 3.33956280e-03 2.40272066e-16\n",
            " 5.20562272e-13 7.10337711e-17 1.13994000e-03 8.01089872e-03\n",
            " 2.87101477e-01 1.67773485e-01 5.44497013e-01 9.99998689e-01\n",
            " 7.50720978e-01 2.51959860e-01 9.95822787e-01 1.39230713e-02\n",
            " 1.22574633e-02 3.47801222e-04 6.37997501e-03 9.87994552e-01\n",
            " 4.20099109e-01 1.37689977e-03 2.48946133e-04 1.39259053e-02\n",
            " 4.24153684e-03 2.42148563e-01 2.39720345e-01 6.72475159e-01\n",
            " 3.71896923e-01 1.49758235e-01 5.57384267e-02 4.35249473e-04\n",
            " 5.43230474e-01 1.12299174e-02 3.48628983e-02 2.01066602e-02\n",
            " 4.03397292e-01 1.67217717e-01 3.22671533e-01 1.42046379e-21\n",
            " 4.63713345e-10 1.14833262e-12 5.41470107e-03 2.41403882e-06\n",
            " 6.29692435e-01 8.08582357e-08 1.50769244e-10 1.86392484e-07\n",
            " 9.97671187e-01 9.99998093e-01 9.99974251e-01 9.99982357e-01\n",
            " 9.56058681e-01 9.99999404e-01 9.80845451e-01 9.99982595e-01\n",
            " 9.53116298e-01 9.27056074e-01 8.65329523e-03 2.05042049e-01\n",
            " 9.98784125e-01 2.23371670e-01 7.77515471e-01 9.94629979e-01\n",
            " 9.98738110e-01 1.00000000e+00 9.99816835e-01 9.99998808e-01\n",
            " 1.00000000e+00 9.85514760e-01 9.99996066e-01 8.84519875e-01\n",
            " 9.78813320e-02 9.98639047e-01 9.99076486e-01 9.99380231e-01\n",
            " 5.96959114e-01 9.99950886e-01 9.73535836e-01 9.83891845e-01\n",
            " 5.56756377e-01 9.85456645e-01 9.96524155e-01 8.73464465e-01\n",
            " 8.55242610e-02 9.33364809e-01 7.42461383e-01 8.16847682e-01\n",
            " 9.63587582e-01 7.56626844e-01 2.20657244e-01 7.22803990e-04\n",
            " 1.48643123e-03 1.13889806e-01 1.76970959e-01 4.30431664e-01\n",
            " 1.99718922e-01 2.42284918e-03 1.57503206e-02 4.12867993e-01\n",
            " 9.27880883e-01 9.94783938e-01 9.92711902e-01 4.83586192e-01\n",
            " 1.00000000e+00 9.99999881e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 43 [0/54 (0%)]\tTrain Loss: 0.000593\n",
            "Train Epoch: 43 [10/54 (19%)]\tTrain Loss: 0.000122\n",
            "Train Epoch: 43 [20/54 (37%)]\tTrain Loss: 0.000036\n",
            "Train Epoch: 43 [30/54 (56%)]\tTrain Loss: 0.000294\n",
            "Train Epoch: 43 [40/54 (74%)]\tTrain Loss: 0.000348\n",
            "Train Epoch: 43 [50/54 (93%)]\tTrain Loss: 0.000143\n",
            "\n",
            "Train set: Average loss: 0.0018, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.36055886e-04 2.09332295e-02 1.22209042e-02 1.42038465e-02\n",
            " 9.80593171e-03 3.98637215e-03 7.70766810e-02 1.36811547e-02\n",
            " 1.39211188e-04 9.97704789e-02 9.97546732e-01 2.93034501e-03\n",
            " 7.92343736e-01 3.17295111e-04 2.32167752e-03 3.98044234e-16\n",
            " 2.46611758e-15 2.81925609e-21 2.00408642e-04 2.62468564e-03\n",
            " 1.88332245e-01 8.74126330e-03 2.55828947e-01 1.00000000e+00\n",
            " 2.44999193e-02 1.25820830e-01 9.97609019e-01 7.28720799e-03\n",
            " 9.85792838e-03 1.51665692e-04 2.01513781e-03 9.70520675e-01\n",
            " 3.18635374e-01 9.32705516e-05 6.44056636e-05 4.15696576e-03\n",
            " 1.18358806e-03 9.22750831e-02 4.84968834e-02 3.31208169e-01\n",
            " 1.62691489e-01 3.62413675e-02 3.03940997e-02 5.67979296e-04\n",
            " 5.01347721e-01 2.02664081e-02 6.88462481e-02 2.18382813e-02\n",
            " 5.98360479e-01 1.15343131e-01 4.48023945e-01 3.53590995e-21\n",
            " 2.22109636e-10 5.72233070e-13 7.13160075e-03 2.98493575e-07\n",
            " 3.96332830e-01 3.54730045e-09 6.08668047e-11 2.71784097e-06\n",
            " 9.99990225e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.75297034e-01 1.00000000e+00 9.97046292e-01 1.00000000e+00\n",
            " 9.94723082e-01 9.64347243e-01 1.54670961e-02 1.60283983e-01\n",
            " 9.99984264e-01 3.49900365e-01 5.70268214e-01 9.99640942e-01\n",
            " 9.99962330e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99978185e-01 1.00000000e+00 5.50776899e-01\n",
            " 2.19331402e-02 9.99891400e-01 9.99999642e-01 9.99999881e-01\n",
            " 5.79278290e-01 9.99999404e-01 9.88478363e-01 9.84487712e-01\n",
            " 7.24524319e-01 9.97099400e-01 9.99890685e-01 9.44228053e-01\n",
            " 6.30551800e-02 9.02148783e-01 8.28052878e-01 6.73067808e-01\n",
            " 9.70563233e-01 7.78431952e-01 4.16200124e-02 1.41392136e-03\n",
            " 5.63347759e-03 1.38635129e-01 6.11025095e-01 9.21277404e-02\n",
            " 1.09543577e-02 1.63701200e-03 5.15847001e-03 1.26036778e-01\n",
            " 8.81349623e-01 9.99181569e-01 9.91636932e-01 2.11735278e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 44 [0/54 (0%)]\tTrain Loss: 0.000087\n",
            "Train Epoch: 44 [10/54 (19%)]\tTrain Loss: 0.000062\n",
            "Train Epoch: 44 [20/54 (37%)]\tTrain Loss: 0.001804\n",
            "Train Epoch: 44 [30/54 (56%)]\tTrain Loss: 0.000003\n",
            "Train Epoch: 44 [40/54 (74%)]\tTrain Loss: 0.000302\n",
            "Train Epoch: 44 [50/54 (93%)]\tTrain Loss: 0.000123\n",
            "\n",
            "Train set: Average loss: 0.0037, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.05331640e-03 2.15858258e-02 1.31866038e-02 1.62016489e-02\n",
            " 9.96224303e-03 8.06772895e-03 6.30943477e-02 1.62401814e-02\n",
            " 2.82207824e-04 1.61391631e-01 9.99322057e-01 4.45995433e-03\n",
            " 8.15687120e-01 3.90598114e-04 1.88756571e-03 3.84509336e-15\n",
            " 1.09466760e-14 6.35176210e-22 2.91197328e-04 2.43110256e-03\n",
            " 1.60455883e-01 1.00253662e-02 1.44027844e-01 1.00000000e+00\n",
            " 2.98799630e-02 8.63311812e-02 9.97982025e-01 5.42705646e-03\n",
            " 1.04334028e-02 7.83159121e-05 1.25970971e-03 8.76143694e-01\n",
            " 1.63700789e-01 1.05594772e-04 6.89260123e-05 2.59454548e-03\n",
            " 7.60147872e-04 9.41652954e-02 2.62106098e-02 2.40573555e-01\n",
            " 1.29712254e-01 2.09415033e-02 2.57518739e-02 6.22789084e-04\n",
            " 4.15737689e-01 6.96293563e-02 1.57613322e-01 3.33946049e-02\n",
            " 6.40264332e-01 1.25284806e-01 4.82564241e-01 2.62246934e-20\n",
            " 1.06369402e-09 4.97226609e-12 8.35302100e-03 1.54421855e-06\n",
            " 3.39634329e-01 5.78731152e-09 1.64683336e-10 1.85729709e-06\n",
            " 9.99999762e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 8.13593149e-01 9.99998331e-01 9.91513193e-01 1.00000000e+00\n",
            " 9.92786944e-01 9.82417405e-01 2.20784321e-02 1.75854683e-01\n",
            " 9.99935031e-01 3.75801831e-01 4.45980281e-01 9.99317527e-01\n",
            " 9.99933243e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99868155e-01 1.00000000e+00 5.15384555e-01\n",
            " 2.55132467e-02 9.99915719e-01 9.99996185e-01 9.99995828e-01\n",
            " 4.69505370e-01 9.99999881e-01 9.84473348e-01 9.87142086e-01\n",
            " 8.17354083e-01 9.97146308e-01 9.99770582e-01 9.11512136e-01\n",
            " 8.94116163e-02 9.11806583e-01 8.88457417e-01 7.24352479e-01\n",
            " 9.75892067e-01 9.55566525e-01 2.46476624e-02 5.12128929e-03\n",
            " 1.66073535e-02 3.22944432e-01 6.60458922e-01 2.27190554e-01\n",
            " 1.67661253e-02 2.52462574e-03 7.52219511e-03 1.46683201e-01\n",
            " 9.14030612e-01 9.97206628e-01 9.90877450e-01 1.36390477e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 45 [0/54 (0%)]\tTrain Loss: 0.000561\n",
            "Train Epoch: 45 [10/54 (19%)]\tTrain Loss: 0.000106\n",
            "Train Epoch: 45 [20/54 (37%)]\tTrain Loss: 0.000056\n",
            "Train Epoch: 45 [30/54 (56%)]\tTrain Loss: 0.000077\n",
            "Train Epoch: 45 [40/54 (74%)]\tTrain Loss: 0.000179\n",
            "Train Epoch: 45 [50/54 (93%)]\tTrain Loss: 0.000093\n",
            "\n",
            "Train set: Average loss: 0.0011, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.44308758e-04 1.39220748e-02 1.02069834e-02 1.32388826e-02\n",
            " 5.58112655e-03 1.29069213e-03 3.54650058e-02 9.45123285e-03\n",
            " 4.14844544e-06 9.06523541e-02 9.99391198e-01 7.71037163e-03\n",
            " 8.97784829e-01 1.10434712e-05 1.38257339e-03 5.65128537e-17\n",
            " 4.34274940e-19 3.96341074e-24 1.94186214e-02 4.19165194e-03\n",
            " 3.46755743e-01 1.10191349e-02 1.47930875e-01 9.99999762e-01\n",
            " 1.35944141e-02 1.31091878e-01 9.90207493e-01 6.11420115e-03\n",
            " 1.77418403e-02 9.10846720e-05 9.44677566e-04 8.32653046e-01\n",
            " 8.74483511e-02 6.16563702e-05 3.10653922e-05 7.35466310e-04\n",
            " 2.79571890e-04 6.17206953e-02 3.80864628e-02 2.45597452e-01\n",
            " 1.04224525e-01 1.63668673e-02 3.26695405e-02 2.61349720e-04\n",
            " 6.71709657e-01 2.57294923e-01 1.44937649e-01 5.18328846e-02\n",
            " 5.70684671e-01 1.18726030e-01 5.32532513e-01 1.29261132e-22\n",
            " 2.59949245e-12 6.23454630e-13 1.99715998e-02 2.15802798e-09\n",
            " 3.53797346e-01 7.49014850e-10 2.09115655e-10 1.27118113e-07\n",
            " 9.99982238e-01 1.00000000e+00 9.99999881e-01 1.00000000e+00\n",
            " 2.71230370e-01 9.98718381e-01 8.26623142e-01 9.99981403e-01\n",
            " 9.75089908e-01 9.85939503e-01 3.12025510e-02 3.40594918e-01\n",
            " 9.91333544e-01 2.64243215e-01 3.00787270e-01 9.98748779e-01\n",
            " 9.99023199e-01 9.99999762e-01 9.99676585e-01 9.99999762e-01\n",
            " 1.00000000e+00 9.94700551e-01 9.99971271e-01 5.70560634e-01\n",
            " 4.90413569e-02 9.99166846e-01 9.99977112e-01 9.99980927e-01\n",
            " 1.06680572e-01 9.98683035e-01 9.47906017e-01 9.45485890e-01\n",
            " 1.36488408e-01 9.94840443e-01 9.96345460e-01 8.88917029e-01\n",
            " 2.62705553e-02 7.47963130e-01 7.72416711e-01 6.94551587e-01\n",
            " 9.15360212e-01 5.80532074e-01 1.83763411e-02 1.52178202e-02\n",
            " 1.30654089e-02 5.21505713e-01 4.79921967e-01 3.93353403e-01\n",
            " 5.91280237e-02 5.56136109e-03 7.15231802e-03 2.07703620e-01\n",
            " 9.38428760e-01 9.88645077e-01 9.94899452e-01 2.21026331e-01\n",
            " 9.99982834e-01 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 46 [0/54 (0%)]\tTrain Loss: 0.000158\n",
            "Train Epoch: 46 [10/54 (19%)]\tTrain Loss: 0.000327\n",
            "Train Epoch: 46 [20/54 (37%)]\tTrain Loss: 0.001005\n",
            "Train Epoch: 46 [30/54 (56%)]\tTrain Loss: 0.000143\n",
            "Train Epoch: 46 [40/54 (74%)]\tTrain Loss: 0.025461\n",
            "Train Epoch: 46 [50/54 (93%)]\tTrain Loss: 0.001035\n",
            "\n",
            "Train set: Average loss: 0.0050, Accuracy: 420/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.67720201e-05 2.66333912e-02 1.51058780e-02 4.44448320e-03\n",
            " 7.49357452e-04 5.32242884e-05 1.08137064e-01 4.78405459e-03\n",
            " 8.77916227e-06 2.38648355e-01 9.98888910e-01 2.12563425e-02\n",
            " 9.00405526e-01 1.36178851e-05 2.30441149e-03 1.76703519e-12\n",
            " 5.28875208e-12 3.39939739e-08 4.02616918e-01 2.15885658e-02\n",
            " 8.23114395e-01 5.24063520e-02 6.25723243e-01 1.00000000e+00\n",
            " 2.92005748e-01 6.05279267e-01 9.95873392e-01 2.76768785e-02\n",
            " 1.74361333e-01 2.14532949e-04 4.44638636e-03 6.98293865e-01\n",
            " 9.81894359e-02 5.40364766e-04 5.68534480e-04 1.94306672e-03\n",
            " 2.47096433e-03 1.01519190e-01 4.17512842e-02 3.95087183e-01\n",
            " 3.71055692e-01 1.66393787e-01 1.88600481e-01 1.34165504e-03\n",
            " 8.61506999e-01 4.31630969e-01 2.26033688e-01 4.10420746e-02\n",
            " 5.24391115e-01 3.19026560e-01 7.04480886e-01 5.78864053e-15\n",
            " 2.49579966e-06 3.83729412e-06 8.89959335e-01 1.83900600e-04\n",
            " 8.52782190e-01 2.28302379e-05 2.29348143e-06 3.08677736e-05\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 2.91469903e-03 9.98256862e-01 9.38674688e-01 9.99984980e-01\n",
            " 9.82183456e-01 8.86406600e-01 4.88157524e-03 1.15842208e-01\n",
            " 7.91064084e-01 2.32939303e-01 1.33446231e-01 9.99995828e-01\n",
            " 9.99842286e-01 1.00000000e+00 9.96068954e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.40968156e-01 9.99999881e-01 9.93073523e-01\n",
            " 4.22390044e-01 9.99999046e-01 9.99999642e-01 1.00000000e+00\n",
            " 6.20766282e-02 9.99490976e-01 8.75245571e-01 5.73394656e-01\n",
            " 6.50937930e-02 9.98210073e-01 9.95916426e-01 9.67436135e-01\n",
            " 5.93300238e-02 9.62054610e-01 9.90871847e-01 9.57365513e-01\n",
            " 8.97127032e-01 9.43483829e-01 1.94491968e-02 1.16372313e-02\n",
            " 3.09615079e-02 2.37459958e-01 5.61436772e-01 9.91140664e-01\n",
            " 9.33135450e-01 5.22036925e-02 1.72614500e-01 6.23053670e-01\n",
            " 9.97794986e-01 9.83804405e-01 9.98131692e-01 8.58213782e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 47 [0/54 (0%)]\tTrain Loss: 0.000168\n",
            "Train Epoch: 47 [10/54 (19%)]\tTrain Loss: 0.000012\n",
            "Train Epoch: 47 [20/54 (37%)]\tTrain Loss: 0.000055\n",
            "Train Epoch: 47 [30/54 (56%)]\tTrain Loss: 0.000074\n",
            "Train Epoch: 47 [40/54 (74%)]\tTrain Loss: 0.000993\n",
            "Train Epoch: 47 [50/54 (93%)]\tTrain Loss: 0.000090\n",
            "\n",
            "Train set: Average loss: 0.0022, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.15017639e-05 2.92848974e-01 4.88935485e-02 3.53887957e-03\n",
            " 1.47851231e-03 6.83413760e-04 8.47241879e-01 1.13820909e-02\n",
            " 3.63992403e-05 1.14708103e-01 9.99197900e-01 1.02585508e-02\n",
            " 9.57052231e-01 8.03618284e-04 3.74337398e-02 3.26350277e-21\n",
            " 3.54286235e-14 2.30260613e-15 9.36242998e-01 1.32979751e-01\n",
            " 9.86966133e-01 2.57165641e-01 9.94987965e-01 1.00000000e+00\n",
            " 9.84723210e-01 9.79985714e-01 9.99925852e-01 2.15292089e-02\n",
            " 8.08421910e-01 2.04234282e-04 4.78605088e-03 9.83350456e-01\n",
            " 3.93825650e-01 1.10942863e-04 2.43301300e-04 1.26369836e-04\n",
            " 1.48013025e-03 3.48610044e-01 1.03665560e-01 5.79363048e-01\n",
            " 7.30979979e-01 5.20705879e-01 1.89130858e-01 4.49959701e-03\n",
            " 9.40867722e-01 5.36862969e-01 1.21334173e-01 2.86775250e-02\n",
            " 3.54328275e-01 6.63851202e-01 5.38641036e-01 1.40114349e-21\n",
            " 1.47825894e-08 4.29009761e-09 9.84195590e-01 5.14056301e-05\n",
            " 9.68312144e-01 4.50359039e-08 2.92965741e-09 7.31226146e-06\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 4.08098567e-03 9.99997139e-01 9.99061048e-01 9.99996543e-01\n",
            " 9.94854927e-01 9.93284225e-01 1.56033440e-02 2.96905905e-01\n",
            " 9.99968410e-01 8.37945759e-01 2.51601517e-01 9.99955535e-01\n",
            " 9.99202907e-01 9.99981761e-01 9.96054769e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.95573401e-01 9.99999881e-01 9.89299059e-01\n",
            " 5.01489103e-01 9.99942541e-01 9.99998569e-01 9.99998450e-01\n",
            " 1.84993923e-01 9.99994755e-01 9.94452178e-01 9.05278087e-01\n",
            " 7.20740378e-01 9.99353349e-01 9.99774158e-01 9.92489457e-01\n",
            " 4.80535805e-01 9.69831705e-01 9.98577356e-01 9.98613596e-01\n",
            " 9.74307597e-01 9.87458467e-01 1.48095921e-01 1.17287925e-02\n",
            " 3.02563924e-02 7.77744278e-02 6.73421681e-01 9.47057128e-01\n",
            " 9.88902390e-01 1.24251969e-01 1.98334172e-01 8.34184051e-01\n",
            " 9.98593748e-01 9.99885440e-01 9.99884367e-01 9.64274764e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 48 [0/54 (0%)]\tTrain Loss: 0.000549\n",
            "Train Epoch: 48 [10/54 (19%)]\tTrain Loss: 0.001597\n",
            "Train Epoch: 48 [20/54 (37%)]\tTrain Loss: 0.000009\n",
            "Train Epoch: 48 [30/54 (56%)]\tTrain Loss: 0.000076\n",
            "Train Epoch: 48 [40/54 (74%)]\tTrain Loss: 0.067255\n",
            "Train Epoch: 48 [50/54 (93%)]\tTrain Loss: 0.000045\n",
            "\n",
            "Train set: Average loss: 0.0055, Accuracy: 421/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.66245722e-03 2.14586295e-02 1.20491581e-02 3.69282323e-04\n",
            " 3.26355849e-03 1.33577094e-03 4.63714823e-02 8.27264227e-03\n",
            " 9.20029197e-05 4.99265045e-02 9.98606622e-01 3.80984345e-03\n",
            " 8.50014091e-01 8.69986591e-14 3.01372784e-04 1.28220548e-21\n",
            " 6.08795005e-17 2.10727654e-21 8.00402939e-01 1.31578613e-02\n",
            " 9.54111755e-01 3.90545726e-02 9.83039320e-01 1.00000000e+00\n",
            " 8.25164735e-01 8.20056736e-01 9.99956608e-01 2.47396994e-02\n",
            " 6.98910236e-01 3.10847478e-04 1.14152264e-02 9.99377191e-01\n",
            " 8.20761383e-01 7.12480571e-04 1.45349791e-03 1.18076999e-03\n",
            " 2.75915419e-03 1.22671284e-01 3.66911851e-02 2.91032106e-01\n",
            " 5.63339949e-01 5.59376657e-01 6.62146062e-02 5.60851488e-03\n",
            " 8.76981795e-01 4.00597245e-01 5.26259184e-01 1.50188059e-01\n",
            " 5.81428051e-01 2.74328649e-01 8.75973880e-01 1.19785033e-19\n",
            " 8.40091936e-16 6.84824647e-14 9.90176380e-01 1.46232514e-05\n",
            " 8.46642435e-01 1.27272048e-09 1.12980562e-11 1.51459235e-05\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 6.35953480e-03 9.99999881e-01 9.99911189e-01 9.99999762e-01\n",
            " 9.79663968e-01 9.95627642e-01 1.56953543e-01 6.67324126e-01\n",
            " 9.99999881e-01 9.57767189e-01 4.16612893e-01 9.99955654e-01\n",
            " 9.97532725e-01 9.99999762e-01 9.90772367e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.98884022e-01 1.00000000e+00 8.77807081e-01\n",
            " 1.60931438e-01 1.00000000e+00 9.99998212e-01 1.00000000e+00\n",
            " 2.58170068e-01 1.00000000e+00 9.95439351e-01 9.83674943e-01\n",
            " 7.42203534e-01 9.96678829e-01 9.99775589e-01 9.93630588e-01\n",
            " 4.16869640e-01 7.83368766e-01 9.93597507e-01 9.96462405e-01\n",
            " 8.42553377e-01 9.91878867e-01 1.47008635e-02 3.67000443e-03\n",
            " 3.32668461e-02 5.07504605e-02 8.65998149e-01 9.47418988e-01\n",
            " 9.52502966e-01 3.03137656e-02 2.05011293e-01 7.07782686e-01\n",
            " 9.98864412e-01 9.99999881e-01 9.99766767e-01 8.07452023e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 49 [0/54 (0%)]\tTrain Loss: 0.000083\n",
            "Train Epoch: 49 [10/54 (19%)]\tTrain Loss: 0.000341\n",
            "Train Epoch: 49 [20/54 (37%)]\tTrain Loss: 0.000013\n",
            "Train Epoch: 49 [30/54 (56%)]\tTrain Loss: 0.000153\n",
            "Train Epoch: 49 [40/54 (74%)]\tTrain Loss: 0.001316\n",
            "Train Epoch: 49 [50/54 (93%)]\tTrain Loss: 0.000009\n",
            "\n",
            "Train set: Average loss: 0.0041, Accuracy: 420/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.78556256e-03 2.00098269e-02 1.15785403e-02 9.99356387e-04\n",
            " 1.88250225e-02 2.03863974e-03 5.04946075e-02 2.05470975e-02\n",
            " 7.59497576e-04 3.42009030e-02 9.83489752e-01 7.87346531e-03\n",
            " 6.93945348e-01 1.47617728e-01 9.20684934e-01 1.46138223e-12\n",
            " 3.55718233e-11 3.35678178e-16 3.50992709e-01 7.94903899e-04\n",
            " 5.42071104e-01 5.76236024e-02 4.43059742e-01 9.99967694e-01\n",
            " 7.58863449e-01 1.83978200e-01 9.87700343e-01 1.13272555e-02\n",
            " 3.92684668e-01 6.09257899e-04 3.17307822e-02 9.99142170e-01\n",
            " 9.86240208e-01 2.23998242e-04 4.61152202e-04 8.33242014e-03\n",
            " 1.44312903e-03 8.43630508e-02 1.62818301e-02 1.16513133e-01\n",
            " 4.01751459e-01 3.16938907e-01 2.95868535e-02 2.15503983e-02\n",
            " 6.77796066e-01 1.75119385e-01 7.16099024e-01 3.27526331e-01\n",
            " 7.22676337e-01 5.71020469e-02 8.99347961e-01 1.45021264e-18\n",
            " 4.02827511e-13 5.34244980e-18 7.81861067e-01 1.81576415e-05\n",
            " 5.87922871e-01 7.82865647e-11 3.21611871e-14 3.89508677e-05\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 8.10864329e-01 1.00000000e+00 9.99992847e-01 1.00000000e+00\n",
            " 9.86879289e-01 9.99586761e-01 5.85230649e-01 9.10778522e-01\n",
            " 9.99997735e-01 9.14172828e-01 9.70343709e-01 9.98661399e-01\n",
            " 9.99980092e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99961495e-01 1.00000000e+00 6.99642241e-01\n",
            " 1.41078345e-02 9.99994755e-01 9.94322181e-01 9.99956012e-01\n",
            " 8.27918425e-02 1.00000000e+00 9.83195007e-01 9.68874693e-01\n",
            " 7.82385468e-01 9.57298338e-01 9.95790660e-01 9.65998352e-01\n",
            " 2.63332069e-01 7.01116264e-01 9.33334112e-01 9.25599992e-01\n",
            " 7.46305645e-01 9.73280489e-01 1.66558623e-02 3.20757888e-02\n",
            " 1.62031531e-01 4.71086055e-02 6.38656139e-01 8.50960195e-01\n",
            " 1.80297822e-01 2.27270015e-02 1.21356964e-01 1.11200154e-01\n",
            " 9.88326609e-01 9.99969006e-01 9.99764621e-01 5.42931378e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 50 [0/54 (0%)]\tTrain Loss: 0.000056\n",
            "Train Epoch: 50 [10/54 (19%)]\tTrain Loss: 0.000256\n",
            "Train Epoch: 50 [20/54 (37%)]\tTrain Loss: 0.000708\n",
            "Train Epoch: 50 [30/54 (56%)]\tTrain Loss: 0.014534\n",
            "Train Epoch: 50 [40/54 (74%)]\tTrain Loss: 0.000881\n",
            "Train Epoch: 50 [50/54 (93%)]\tTrain Loss: 0.002582\n",
            "\n",
            "Train set: Average loss: 0.0109, Accuracy: 415/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.13799134e-02 2.05613803e-02 1.95924398e-02 5.46604395e-01\n",
            " 5.08536100e-01 1.87322289e-01 4.37304005e-02 2.62080461e-01\n",
            " 3.63395326e-02 6.42669261e-01 9.99809325e-01 1.55621395e-01\n",
            " 9.41054642e-01 8.83867800e-01 8.30573678e-01 7.26083200e-03\n",
            " 9.27541126e-03 7.37453043e-01 7.00287163e-01 6.13505626e-03\n",
            " 8.80073428e-01 9.11228001e-01 6.61648333e-01 1.00000000e+00\n",
            " 9.99872327e-01 1.22710958e-01 9.99859929e-01 4.64491025e-02\n",
            " 5.01056612e-01 2.51099980e-03 1.73689090e-02 7.47333109e-01\n",
            " 2.19230294e-01 2.53964830e-02 1.53891081e-02 2.44538546e-01\n",
            " 4.58447002e-02 8.41576993e-01 5.90738118e-01 3.76158088e-01\n",
            " 6.27155423e-01 8.78363073e-01 5.47926486e-01 1.08873755e-01\n",
            " 1.16836406e-01 6.11494929e-02 1.93639755e-01 1.13838464e-01\n",
            " 1.23723105e-01 7.17680454e-02 4.94460613e-01 3.86455978e-09\n",
            " 6.29665889e-03 3.64659741e-06 9.61608291e-01 1.45117743e-02\n",
            " 8.70914638e-01 1.54114241e-04 9.11498792e-04 2.46843137e-03\n",
            " 9.71705019e-01 9.99994516e-01 9.50011790e-01 4.57754105e-01\n",
            " 2.34524384e-01 1.00000000e+00 1.00000000e+00 9.99974728e-01\n",
            " 9.58854318e-01 9.97928739e-01 5.38338184e-01 4.10016984e-01\n",
            " 9.94594872e-01 6.12939715e-01 9.47775126e-01 5.98667860e-01\n",
            " 8.70186210e-01 9.96570706e-01 9.24769163e-01 9.99919295e-01\n",
            " 1.00000000e+00 9.88125682e-01 9.35626566e-01 9.98920918e-01\n",
            " 3.59375268e-01 9.90214705e-01 9.90214586e-01 9.80525136e-01\n",
            " 4.82475698e-01 1.00000000e+00 9.99606788e-01 9.95747387e-01\n",
            " 8.81417871e-01 9.91501808e-01 9.66305256e-01 9.99421477e-01\n",
            " 3.09098184e-01 9.94819820e-01 9.68533695e-01 9.87502635e-01\n",
            " 9.47230399e-01 9.95520353e-01 9.71724927e-01 9.33351219e-02\n",
            " 1.92196339e-01 9.77160215e-01 7.66132832e-01 9.96823192e-01\n",
            " 9.98210073e-01 1.04642853e-01 8.74242783e-01 9.71799016e-01\n",
            " 9.93780077e-01 9.99689817e-01 9.84772265e-01 9.91908252e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 40 TN= 52 FN= 18 FP= 8\n",
            "TP+FP 48\n",
            "precision 0.8333333333333334\n",
            "recall 0.6896551724137931\n",
            "F1 0.7547169811320755\n",
            "acc 0.7796610169491526\n",
            "AUCp 0.7781609195402299\n",
            "AUC 0.8649425287356322\n",
            "\n",
            " The epoch is 50, average recall: 0.6897, average precision: 0.8333,average F1: 0.7547, average accuracy: 0.7797, average AUC: 0.8649\n",
            "Train Epoch: 51 [0/54 (0%)]\tTrain Loss: 0.000970\n",
            "Train Epoch: 51 [10/54 (19%)]\tTrain Loss: 0.001241\n",
            "Train Epoch: 51 [20/54 (37%)]\tTrain Loss: 0.013102\n",
            "Train Epoch: 51 [30/54 (56%)]\tTrain Loss: 0.007634\n",
            "Train Epoch: 51 [40/54 (74%)]\tTrain Loss: 0.008277\n",
            "Train Epoch: 51 [50/54 (93%)]\tTrain Loss: 0.000349\n",
            "\n",
            "Train set: Average loss: 0.0157, Accuracy: 410/425 (96%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.44279795e-02 4.90775518e-02 2.63407771e-02 6.56101182e-02\n",
            " 3.62455985e-03 6.98982537e-01 1.08674265e-01 4.36617397e-02\n",
            " 2.43228674e-02 1.14177018e-01 9.15887892e-01 3.42193842e-02\n",
            " 5.97496390e-01 4.17386033e-02 2.02870439e-03 4.75972040e-10\n",
            " 4.17163847e-06 5.56274783e-03 1.06587140e-02 2.85571907e-03\n",
            " 9.58395004e-01 4.16388839e-01 3.85261178e-01 1.00000000e+00\n",
            " 7.58856595e-01 6.18659072e-02 9.87339020e-01 3.44197638e-02\n",
            " 2.56538540e-01 2.11879684e-04 2.93348301e-02 2.37315327e-01\n",
            " 1.42498031e-01 2.43585533e-03 8.62219022e-04 1.60884559e-02\n",
            " 1.82424746e-02 5.18869907e-02 8.48134398e-01 1.82978943e-01\n",
            " 1.12012215e-01 5.80496967e-01 2.31886923e-01 9.11727606e-04\n",
            " 2.73302384e-02 1.74902175e-02 9.32911783e-02 2.80726291e-02\n",
            " 3.97847682e-01 4.70197685e-02 6.61968708e-01 4.65980370e-14\n",
            " 4.88564779e-11 7.23253109e-13 8.75244558e-01 5.82534426e-07\n",
            " 5.86216152e-01 1.04853143e-06 1.78904168e-03 2.42303067e-04\n",
            " 9.99147415e-01 9.99396324e-01 9.92782593e-01 9.99323249e-01\n",
            " 8.95210445e-01 9.99987841e-01 9.96689439e-01 1.00000000e+00\n",
            " 9.79273915e-01 9.75050032e-01 7.89441764e-01 6.99935853e-01\n",
            " 9.99603689e-01 9.04604554e-01 8.86715114e-01 9.88261282e-01\n",
            " 9.99986649e-01 1.00000000e+00 1.00000000e+00 9.92703795e-01\n",
            " 9.81533229e-01 9.85966265e-01 9.99828815e-01 4.48400050e-01\n",
            " 6.58154264e-02 9.94148135e-01 9.99755800e-01 9.98794436e-01\n",
            " 9.06462014e-01 9.70804036e-01 9.99099255e-01 2.21063614e-01\n",
            " 4.72713495e-03 9.38507378e-01 9.77309465e-01 9.70597804e-01\n",
            " 8.62441882e-02 8.92970622e-01 2.21688554e-01 9.32444990e-01\n",
            " 3.48218352e-01 8.40519130e-01 6.38883635e-02 1.15659751e-03\n",
            " 2.66947062e-03 4.16050673e-01 2.53868550e-01 9.78579521e-01\n",
            " 2.65049160e-01 2.34635770e-02 3.43315601e-01 7.97988236e-01\n",
            " 8.56739819e-01 8.81076992e-01 9.83108461e-01 1.98784366e-01\n",
            " 9.99985576e-01 9.99995470e-01]\n",
            "predict [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 52 [0/54 (0%)]\tTrain Loss: 0.000117\n",
            "Train Epoch: 52 [10/54 (19%)]\tTrain Loss: 0.004541\n",
            "Train Epoch: 52 [20/54 (37%)]\tTrain Loss: 0.000422\n",
            "Train Epoch: 52 [30/54 (56%)]\tTrain Loss: 0.001495\n",
            "Train Epoch: 52 [40/54 (74%)]\tTrain Loss: 0.000439\n",
            "Train Epoch: 52 [50/54 (93%)]\tTrain Loss: 0.001370\n",
            "\n",
            "Train set: Average loss: 0.0071, Accuracy: 418/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.52121054e-03 5.47244102e-02 1.61794238e-02 5.60737133e-01\n",
            " 4.48950101e-03 6.22532010e-01 8.08683559e-02 3.75860304e-01\n",
            " 1.41900312e-02 4.58683223e-01 9.95138407e-01 9.55031142e-02\n",
            " 9.58624423e-01 1.16982395e-02 1.63647719e-02 6.71711823e-15\n",
            " 3.93659341e-11 4.47251499e-01 1.54262587e-01 1.00581646e-02\n",
            " 9.31851685e-01 9.96134400e-01 3.72708887e-01 1.00000000e+00\n",
            " 9.99987125e-01 2.78028041e-01 9.97972906e-01 1.37228057e-01\n",
            " 4.13809299e-01 6.71366652e-05 4.92299378e-01 9.82586265e-01\n",
            " 7.41389155e-01 1.05391268e-03 2.72921141e-04 3.85751240e-02\n",
            " 7.80073507e-03 9.63172436e-01 9.99982834e-01 8.07636917e-01\n",
            " 7.73316979e-01 9.47006583e-01 8.08592379e-01 1.29930051e-02\n",
            " 7.41200507e-01 3.33594173e-01 1.65436253e-01 8.28682557e-02\n",
            " 2.04085752e-01 2.27965742e-01 5.78853369e-01 2.04399404e-13\n",
            " 6.11409178e-07 2.74743561e-11 9.02756095e-01 2.40342561e-05\n",
            " 7.05683827e-01 2.20355287e-06 7.79987953e-04 3.14400211e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.64750946e-01 1.00000000e+00 9.99262154e-01 1.00000000e+00\n",
            " 9.99544442e-01 9.15247738e-01 4.78734612e-01 8.77694011e-01\n",
            " 9.99998927e-01 9.46030140e-01 9.84005451e-01 9.99999762e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99991894e-01\n",
            " 1.00000000e+00 9.99997139e-01 1.00000000e+00 9.99985933e-01\n",
            " 5.62359691e-01 9.99239564e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.00020540e-01 9.98030603e-01 9.94911969e-01 9.68556523e-01\n",
            " 1.91207498e-01 9.98371422e-01 9.99870062e-01 9.96392548e-01\n",
            " 6.92307949e-01 9.89367306e-01 3.61607909e-01 9.99319911e-01\n",
            " 9.74038899e-01 9.99996543e-01 8.39324415e-01 1.78422034e-02\n",
            " 1.94045883e-02 9.97404277e-01 8.40515614e-01 8.61752570e-01\n",
            " 6.57584786e-01 1.77079178e-02 1.25965551e-01 8.07786524e-01\n",
            " 8.30011725e-01 9.99050319e-01 9.99765456e-01 9.99587119e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 53 [0/54 (0%)]\tTrain Loss: 0.006329\n",
            "Train Epoch: 53 [10/54 (19%)]\tTrain Loss: 0.000317\n",
            "Train Epoch: 53 [20/54 (37%)]\tTrain Loss: 0.000145\n",
            "Train Epoch: 53 [30/54 (56%)]\tTrain Loss: 0.000160\n",
            "Train Epoch: 53 [40/54 (74%)]\tTrain Loss: 0.000696\n",
            "Train Epoch: 53 [50/54 (93%)]\tTrain Loss: 0.000241\n",
            "\n",
            "Train set: Average loss: 0.0015, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.40582959e-03 4.33542095e-02 1.32503388e-02 1.41595453e-01\n",
            " 3.74766300e-04 4.76537794e-01 6.84562251e-02 1.36671975e-01\n",
            " 4.84314281e-03 4.86456782e-01 9.95001018e-01 8.79427716e-02\n",
            " 9.10227954e-01 7.84337753e-04 8.08070600e-03 3.62353705e-19\n",
            " 3.46746384e-15 4.07554135e-02 4.17190701e-01 1.46710631e-02\n",
            " 9.71858680e-01 9.50807095e-01 5.29986024e-01 1.00000000e+00\n",
            " 9.97251332e-01 4.56938177e-01 9.99735534e-01 1.75745681e-01\n",
            " 6.36930585e-01 2.33525352e-05 7.19562948e-01 9.97919261e-01\n",
            " 8.84163201e-01 1.66534272e-04 4.92309591e-05 8.09449982e-03\n",
            " 2.02939613e-03 8.06132495e-01 9.94434893e-01 4.00414824e-01\n",
            " 4.90222216e-01 7.93437302e-01 5.59641957e-01 3.59458965e-03\n",
            " 7.67118454e-01 7.94211805e-01 3.76268595e-01 1.13190927e-01\n",
            " 3.32242906e-01 7.88481712e-01 8.74599457e-01 1.87218151e-14\n",
            " 5.37282449e-06 1.33266922e-11 9.84236121e-01 3.90423120e-05\n",
            " 8.79078805e-01 7.05766581e-07 2.18054018e-04 2.10338243e-04\n",
            " 9.99998212e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.35612619e-01 1.00000000e+00 9.99548256e-01 1.00000000e+00\n",
            " 9.94262993e-01 9.96919751e-01 8.35926473e-01 9.29589570e-01\n",
            " 9.99999762e-01 9.96245921e-01 9.81620193e-01 9.99981165e-01\n",
            " 9.99998808e-01 9.99999881e-01 1.00000000e+00 9.99976158e-01\n",
            " 1.00000000e+00 9.99981046e-01 1.00000000e+00 9.97498214e-01\n",
            " 3.53365570e-01 9.99813735e-01 9.99999404e-01 1.00000000e+00\n",
            " 9.54986513e-01 9.94517744e-01 9.99148369e-01 9.74716842e-01\n",
            " 4.04333174e-02 9.99003351e-01 9.99799192e-01 9.97618735e-01\n",
            " 8.22518349e-01 9.71577406e-01 6.38240218e-01 9.94888246e-01\n",
            " 9.59597111e-01 9.99813259e-01 2.77051836e-01 1.80344488e-02\n",
            " 4.01469581e-02 9.98475134e-01 9.59551215e-01 7.92329609e-01\n",
            " 3.87387842e-01 1.15887448e-02 8.28583837e-02 6.75528526e-01\n",
            " 8.78533065e-01 9.99305844e-01 9.97890770e-01 9.78689849e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 54 [0/54 (0%)]\tTrain Loss: 0.000100\n",
            "Train Epoch: 54 [10/54 (19%)]\tTrain Loss: 0.000342\n",
            "Train Epoch: 54 [20/54 (37%)]\tTrain Loss: 0.000713\n",
            "Train Epoch: 54 [30/54 (56%)]\tTrain Loss: 0.001904\n",
            "Train Epoch: 54 [40/54 (74%)]\tTrain Loss: 0.009210\n",
            "Train Epoch: 54 [50/54 (93%)]\tTrain Loss: 0.000914\n",
            "\n",
            "Train set: Average loss: 0.0159, Accuracy: 417/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.12617736e-02 4.36611801e-01 2.05216900e-01 1.26940325e-01\n",
            " 1.92478346e-03 3.90194654e-01 7.55395055e-01 1.66774973e-01\n",
            " 2.01983806e-02 1.83615655e-01 9.37790275e-01 8.89095142e-02\n",
            " 7.48320699e-01 9.54532225e-05 4.26811073e-03 1.08699267e-17\n",
            " 6.31163848e-16 1.13138661e-03 3.76943558e-01 1.59835839e-03\n",
            " 9.35013473e-01 2.41583735e-01 6.50379717e-01 9.97668207e-01\n",
            " 3.24444622e-01 2.31685713e-01 9.84967172e-01 4.88553867e-02\n",
            " 2.40378693e-01 2.24960066e-04 2.17890572e-02 8.33844364e-01\n",
            " 8.98200810e-01 1.40587581e-04 3.48958070e-04 1.92743167e-03\n",
            " 3.84572474e-03 3.17746729e-01 2.09364206e-01 1.06579371e-01\n",
            " 1.10502742e-01 7.43615776e-02 4.40678030e-01 1.32048328e-03\n",
            " 6.93394125e-01 2.43625581e-01 5.98233998e-01 2.54344463e-01\n",
            " 5.75900197e-01 5.69916189e-01 8.98642182e-01 1.73468573e-09\n",
            " 2.89276126e-04 4.76533023e-06 9.33603466e-01 2.97752558e-03\n",
            " 9.17525589e-01 2.31529179e-04 1.81854807e-03 2.57268286e-04\n",
            " 9.96277153e-01 9.95012701e-01 9.78505611e-01 9.72604394e-01\n",
            " 9.39890683e-01 9.99356329e-01 9.91783977e-01 9.99105632e-01\n",
            " 9.87154484e-01 9.93327618e-01 9.52650011e-01 9.06085670e-01\n",
            " 9.99373734e-01 8.60546947e-01 7.66712546e-01 9.88489211e-01\n",
            " 9.98894393e-01 9.99989390e-01 9.99997139e-01 9.99706805e-01\n",
            " 9.98823702e-01 9.97100174e-01 9.99996662e-01 7.66451240e-01\n",
            " 1.08487740e-01 9.88570750e-01 9.98912096e-01 9.99415874e-01\n",
            " 8.14128220e-01 6.67456508e-01 9.63379979e-01 9.91134644e-01\n",
            " 2.16557980e-01 9.89381313e-01 9.98491168e-01 9.93259490e-01\n",
            " 1.98250845e-01 8.30727577e-01 7.99463868e-01 7.07487941e-01\n",
            " 7.85572529e-01 8.59040260e-01 4.00208123e-03 4.22556065e-02\n",
            " 1.54919207e-01 9.90391731e-01 9.45779920e-01 3.15269291e-01\n",
            " 6.03457652e-02 1.81447659e-02 1.26192436e-01 1.70649230e-01\n",
            " 6.94949448e-01 9.84308898e-01 5.46748579e-01 4.33530398e-02\n",
            " 9.99872923e-01 9.99847412e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 55 [0/54 (0%)]\tTrain Loss: 0.000423\n",
            "Train Epoch: 55 [10/54 (19%)]\tTrain Loss: 0.005647\n",
            "Train Epoch: 55 [20/54 (37%)]\tTrain Loss: 0.004513\n",
            "Train Epoch: 55 [30/54 (56%)]\tTrain Loss: 0.005075\n",
            "Train Epoch: 55 [40/54 (74%)]\tTrain Loss: 0.000091\n",
            "Train Epoch: 55 [50/54 (93%)]\tTrain Loss: 0.000133\n",
            "\n",
            "Train set: Average loss: 0.0048, Accuracy: 419/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.79494300e-02 1.09878480e-01 3.51840816e-02 2.00263992e-01\n",
            " 2.07023392e-03 1.87222585e-01 3.34058553e-01 2.49027699e-01\n",
            " 2.65959464e-02 5.04149608e-02 9.19300616e-01 2.65647154e-02\n",
            " 8.10249925e-01 1.38691359e-03 1.93978213e-02 1.44831294e-15\n",
            " 4.38516688e-11 2.25572810e-01 5.59026361e-01 3.03380173e-02\n",
            " 9.22337234e-01 9.41618264e-01 9.04094934e-01 9.99895573e-01\n",
            " 9.97289896e-01 6.13139033e-01 9.99291301e-01 5.88483632e-01\n",
            " 3.78056735e-01 3.72963026e-04 1.31147832e-01 9.44873869e-01\n",
            " 2.28905618e-01 1.47407549e-03 1.56527536e-03 6.55179750e-03\n",
            " 9.52884182e-03 5.60734928e-01 9.53231275e-01 3.57858926e-01\n",
            " 5.93553662e-01 6.42962575e-01 8.00899267e-01 2.07826635e-03\n",
            " 7.22950280e-01 2.07398850e-02 1.01568341e-01 5.79621494e-02\n",
            " 1.83442935e-01 7.31580853e-02 4.37864423e-01 2.34403552e-10\n",
            " 8.11825771e-07 1.81188824e-08 9.67950463e-01 4.17575066e-04\n",
            " 8.76745045e-01 3.54980439e-04 1.17119600e-03 2.08742822e-05\n",
            " 9.99990106e-01 9.99963522e-01 9.99981403e-01 9.99990940e-01\n",
            " 9.34042931e-01 1.00000000e+00 9.99995351e-01 1.00000000e+00\n",
            " 9.90095139e-01 7.93507397e-01 9.67337430e-01 9.64192092e-01\n",
            " 9.99998808e-01 8.36834967e-01 8.42947602e-01 9.99347270e-01\n",
            " 9.99761999e-01 1.00000000e+00 1.00000000e+00 9.99979377e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99854922e-01\n",
            " 6.80028200e-01 9.99147773e-01 9.99998808e-01 9.99999642e-01\n",
            " 7.23178685e-01 9.97353315e-01 9.81685102e-01 6.31958663e-01\n",
            " 5.70064858e-02 9.89099801e-01 9.99513626e-01 9.92202401e-01\n",
            " 6.79687783e-03 9.46609020e-01 4.47182804e-01 9.79934156e-01\n",
            " 9.07225609e-01 9.95588064e-01 1.93851039e-01 1.01951207e-03\n",
            " 4.51055728e-03 6.92478597e-01 2.28798673e-01 3.95525724e-01\n",
            " 5.68887949e-01 2.58566514e-02 1.35876492e-01 5.02665102e-01\n",
            " 5.76464355e-01 9.98360574e-01 8.77049088e-01 7.03393519e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 56 [0/54 (0%)]\tTrain Loss: 0.000195\n",
            "Train Epoch: 56 [10/54 (19%)]\tTrain Loss: 0.000101\n",
            "Train Epoch: 56 [20/54 (37%)]\tTrain Loss: 0.000340\n",
            "Train Epoch: 56 [30/54 (56%)]\tTrain Loss: 0.001266\n",
            "Train Epoch: 56 [40/54 (74%)]\tTrain Loss: 0.000372\n",
            "Train Epoch: 56 [50/54 (93%)]\tTrain Loss: 0.000093\n",
            "\n",
            "Train set: Average loss: 0.0025, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.74365789e-02 1.96164861e-01 4.32309508e-02 1.78153589e-01\n",
            " 7.52497930e-04 1.54254884e-01 4.95217681e-01 2.36478448e-01\n",
            " 1.63919739e-02 5.02531715e-02 9.24706161e-01 2.24230569e-02\n",
            " 8.32243741e-01 8.73227790e-03 3.77002358e-02 1.60474395e-17\n",
            " 3.70648338e-11 2.50462711e-01 8.42296302e-01 7.65667781e-02\n",
            " 9.64360237e-01 9.29510891e-01 8.91240239e-01 9.99909639e-01\n",
            " 9.93140996e-01 6.13596618e-01 9.98249710e-01 5.34708440e-01\n",
            " 6.38241649e-01 1.44546677e-04 1.87000319e-01 9.59901810e-01\n",
            " 3.70028704e-01 8.59221502e-04 5.90024807e-04 3.94283235e-03\n",
            " 8.92119296e-03 6.65555775e-01 9.32703137e-01 3.79133224e-01\n",
            " 5.76376975e-01 7.29380131e-01 7.54107773e-01 6.70598773e-03\n",
            " 8.40164423e-01 2.51697898e-02 2.04859480e-01 7.65687898e-02\n",
            " 2.09600359e-01 7.23109096e-02 4.79632586e-01 1.67947357e-11\n",
            " 4.72377479e-08 6.16601570e-09 9.85379457e-01 2.47545395e-04\n",
            " 9.14865971e-01 1.29696738e-04 7.85509008e-04 3.31554816e-06\n",
            " 9.99897003e-01 9.99942303e-01 9.99991775e-01 9.99979734e-01\n",
            " 9.47667539e-01 1.00000000e+00 9.99999762e-01 1.00000000e+00\n",
            " 9.93618727e-01 9.71742392e-01 9.77662146e-01 9.66016173e-01\n",
            " 9.99998689e-01 9.25943971e-01 8.89642179e-01 9.99698877e-01\n",
            " 9.99964952e-01 1.00000000e+00 1.00000000e+00 9.99986649e-01\n",
            " 1.00000000e+00 9.99999404e-01 1.00000000e+00 9.98932302e-01\n",
            " 7.50901759e-01 9.98678863e-01 9.99987483e-01 9.99994755e-01\n",
            " 8.45974028e-01 9.98149753e-01 9.90721703e-01 7.18855262e-01\n",
            " 4.73964922e-02 9.85139072e-01 9.98535395e-01 9.91788208e-01\n",
            " 1.18418047e-02 9.43460345e-01 8.75056803e-01 9.86680984e-01\n",
            " 9.20244992e-01 9.88015831e-01 2.67641306e-01 5.17886598e-04\n",
            " 4.65327408e-03 7.07370341e-01 2.68027961e-01 5.29859841e-01\n",
            " 7.09408641e-01 2.81552374e-02 1.95567220e-01 6.84383631e-01\n",
            " 8.41223776e-01 9.97411191e-01 9.37112033e-01 7.25514948e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 57 [0/54 (0%)]\tTrain Loss: 0.000062\n",
            "Train Epoch: 57 [10/54 (19%)]\tTrain Loss: 0.015768\n",
            "Train Epoch: 57 [20/54 (37%)]\tTrain Loss: 0.000236\n",
            "Train Epoch: 57 [30/54 (56%)]\tTrain Loss: 0.000367\n",
            "Train Epoch: 57 [40/54 (74%)]\tTrain Loss: 0.001418\n",
            "Train Epoch: 57 [50/54 (93%)]\tTrain Loss: 0.000058\n",
            "\n",
            "Train set: Average loss: 0.0023, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.07632892e-02 8.69062990e-02 2.21324153e-02 1.04376934e-01\n",
            " 3.65148205e-03 1.51198894e-01 1.66533396e-01 1.60301089e-01\n",
            " 1.90476030e-02 3.70000675e-02 9.18055594e-01 2.01203711e-02\n",
            " 7.47345209e-01 2.68220832e-03 1.74634494e-02 4.73817261e-17\n",
            " 7.57197398e-08 1.29608557e-01 8.96287382e-01 6.22785747e-01\n",
            " 9.63141918e-01 9.06494558e-01 8.68097723e-01 9.99853373e-01\n",
            " 9.89017606e-01 6.21496856e-01 9.98680770e-01 5.08141339e-01\n",
            " 6.45279944e-01 2.07139936e-04 1.53919533e-01 9.05702531e-01\n",
            " 1.70226812e-01 9.44707775e-04 6.06609625e-04 3.18046473e-03\n",
            " 6.64297491e-03 4.57933903e-01 8.09897184e-01 2.70151526e-01\n",
            " 5.57127237e-01 6.62807465e-01 5.69949508e-01 8.28810874e-03\n",
            " 9.24937189e-01 8.92046914e-02 3.20644677e-01 5.23843132e-02\n",
            " 3.65909338e-01 6.58390075e-02 6.34454131e-01 1.31961816e-10\n",
            " 2.57574957e-05 8.53186350e-07 9.93833303e-01 1.27528072e-03\n",
            " 9.85645354e-01 3.02215834e-04 2.24497006e-03 3.13352903e-05\n",
            " 9.99996305e-01 9.99999762e-01 1.00000000e+00 9.99999881e-01\n",
            " 9.05880451e-01 9.99933004e-01 9.99992728e-01 9.99999762e-01\n",
            " 9.93953764e-01 9.84486997e-01 9.91202831e-01 9.64614391e-01\n",
            " 1.00000000e+00 9.01346385e-01 9.45129335e-01 9.99696016e-01\n",
            " 9.99176800e-01 9.99996305e-01 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.97275651e-01\n",
            " 5.21255672e-01 9.99320030e-01 9.99999285e-01 9.99998927e-01\n",
            " 7.68313050e-01 9.99047935e-01 9.95644331e-01 8.92173231e-01\n",
            " 2.10293785e-01 9.92234468e-01 9.99907732e-01 9.96921420e-01\n",
            " 4.26276140e-02 9.46051776e-01 9.52560604e-01 9.87671971e-01\n",
            " 9.49440539e-01 9.75553930e-01 2.02121943e-01 7.61910109e-03\n",
            " 1.81921199e-02 9.24491644e-01 5.51675618e-01 3.57264757e-01\n",
            " 6.82699800e-01 2.65521724e-02 1.47981584e-01 6.80716336e-01\n",
            " 8.66412878e-01 9.99325514e-01 9.08106267e-01 4.50172752e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 58 [0/54 (0%)]\tTrain Loss: 0.001175\n",
            "Train Epoch: 58 [10/54 (19%)]\tTrain Loss: 0.000347\n",
            "Train Epoch: 58 [20/54 (37%)]\tTrain Loss: 0.003742\n",
            "Train Epoch: 58 [30/54 (56%)]\tTrain Loss: 0.000361\n",
            "Train Epoch: 58 [40/54 (74%)]\tTrain Loss: 0.000076\n",
            "Train Epoch: 58 [50/54 (93%)]\tTrain Loss: 0.033024\n",
            "\n",
            "Train set: Average loss: 0.0030, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [9.43816174e-03 2.01568715e-02 9.02846735e-03 5.45167923e-02\n",
            " 1.81909534e-03 8.73123854e-02 7.84738287e-02 2.00278666e-02\n",
            " 9.18460265e-03 3.64575684e-02 8.80319357e-01 1.38159180e-02\n",
            " 6.99475646e-01 1.23694970e-03 4.96548880e-03 8.71260240e-18\n",
            " 9.79051038e-06 8.29922137e-05 8.64752054e-01 7.77271748e-01\n",
            " 9.89598036e-01 8.92224312e-01 9.37515795e-01 9.99986291e-01\n",
            " 9.80634451e-01 8.60453904e-01 9.99791563e-01 5.23291111e-01\n",
            " 8.21759820e-01 4.90046979e-04 7.85961077e-02 9.06826138e-01\n",
            " 1.15971945e-01 1.01810752e-03 4.73046937e-04 3.32740229e-03\n",
            " 6.17590966e-03 3.43974456e-02 9.07906175e-01 2.69533396e-01\n",
            " 4.33909208e-01 6.55790448e-01 2.04217687e-01 1.54233573e-03\n",
            " 7.63285756e-01 1.16561808e-01 1.54541939e-01 2.41155475e-02\n",
            " 2.04449847e-01 5.70484810e-02 3.86099607e-01 3.63531839e-12\n",
            " 3.33941807e-06 1.26031665e-08 9.94216442e-01 9.29342350e-05\n",
            " 9.23643291e-01 5.65997434e-05 5.01412957e-04 5.90414820e-05\n",
            " 9.99472916e-01 9.99930382e-01 9.99929309e-01 9.99724686e-01\n",
            " 8.89374018e-01 9.97701108e-01 9.98576999e-01 9.99991059e-01\n",
            " 9.88046885e-01 9.89351809e-01 9.87890482e-01 9.63343143e-01\n",
            " 1.00000000e+00 9.45292413e-01 9.44131017e-01 9.95139122e-01\n",
            " 9.99350488e-01 9.99991298e-01 1.00000000e+00 9.99998450e-01\n",
            " 1.00000000e+00 9.99959946e-01 1.00000000e+00 9.90784883e-01\n",
            " 2.35914290e-01 9.99919415e-01 9.99635339e-01 9.99779761e-01\n",
            " 9.05584812e-01 9.99773324e-01 9.99826729e-01 6.55223727e-01\n",
            " 3.47273424e-02 9.99409795e-01 9.99975204e-01 9.99441564e-01\n",
            " 2.44245440e-01 8.80352974e-01 9.06633079e-01 9.91473794e-01\n",
            " 8.75972629e-01 9.15553808e-01 4.55447644e-01 1.31158473e-03\n",
            " 4.09489125e-03 7.83805072e-01 1.73877552e-01 2.67444134e-01\n",
            " 5.91373146e-01 3.21470052e-02 9.61478278e-02 7.27797389e-01\n",
            " 9.17273641e-01 9.99990225e-01 9.66583967e-01 3.39550406e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 59 [0/54 (0%)]\tTrain Loss: 0.018673\n",
            "Train Epoch: 59 [10/54 (19%)]\tTrain Loss: 0.000635\n",
            "Train Epoch: 59 [20/54 (37%)]\tTrain Loss: 0.000255\n",
            "Train Epoch: 59 [30/54 (56%)]\tTrain Loss: 0.000276\n",
            "Train Epoch: 59 [40/54 (74%)]\tTrain Loss: 0.000216\n",
            "Train Epoch: 59 [50/54 (93%)]\tTrain Loss: 0.000388\n",
            "\n",
            "Train set: Average loss: 0.0021, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [7.30732456e-03 1.18174134e-02 7.12437974e-03 5.72240353e-02\n",
            " 1.50311901e-03 2.68403310e-02 7.93934837e-02 2.40618382e-02\n",
            " 5.44094667e-03 5.17767817e-02 9.80472267e-01 9.81010031e-03\n",
            " 8.69825006e-01 3.60779850e-05 1.20051503e-04 4.16849100e-17\n",
            " 3.18952334e-06 3.92912352e-06 6.83778152e-02 8.81605409e-03\n",
            " 9.46850717e-01 5.68745613e-01 7.05694497e-01 9.99612272e-01\n",
            " 8.43026519e-01 3.42837483e-01 9.97062147e-01 2.64041312e-02\n",
            " 1.23325370e-01 1.08872933e-04 2.92328442e-03 2.11361483e-01\n",
            " 3.32892984e-02 2.92769604e-04 1.93548927e-04 2.15944904e-03\n",
            " 4.06708475e-03 5.67514449e-03 8.80347908e-01 3.47095370e-01\n",
            " 5.21478355e-01 4.57594723e-01 4.71549928e-01 1.00952003e-03\n",
            " 6.27857029e-01 5.24996147e-02 5.13625257e-02 2.06925925e-02\n",
            " 1.73267797e-01 2.43164580e-02 2.95444816e-01 1.95093802e-11\n",
            " 4.19056420e-07 5.94962524e-09 9.71040010e-01 3.16059086e-05\n",
            " 6.95244193e-01 2.06513305e-05 4.14997950e-04 1.87593141e-05\n",
            " 9.98965263e-01 9.99121368e-01 9.99628544e-01 9.99555171e-01\n",
            " 4.62411076e-01 9.98106241e-01 9.90050733e-01 9.99997973e-01\n",
            " 9.91915643e-01 9.88745987e-01 7.20836639e-01 6.71662629e-01\n",
            " 9.99671698e-01 7.83439159e-01 7.46524811e-01 9.91538167e-01\n",
            " 9.98385072e-01 9.99932170e-01 9.99999881e-01 9.99997973e-01\n",
            " 9.99999881e-01 9.98856425e-01 1.00000000e+00 7.79317141e-01\n",
            " 1.51518926e-01 9.99559224e-01 9.98900771e-01 9.97584939e-01\n",
            " 6.44413412e-01 9.98781621e-01 9.98130262e-01 8.21280122e-01\n",
            " 4.65387404e-02 9.92937028e-01 9.99346912e-01 9.93120432e-01\n",
            " 2.42205616e-02 7.32735634e-01 7.84398079e-01 9.83985484e-01\n",
            " 8.83262575e-01 6.58762395e-01 1.89502612e-01 5.89589356e-04\n",
            " 2.81788432e-03 3.68604720e-01 8.47949758e-02 4.89953578e-01\n",
            " 5.05333126e-01 2.44334079e-02 1.37187928e-01 7.30417609e-01\n",
            " 8.88367593e-01 9.99773324e-01 5.67804217e-01 4.87369299e-02\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 60 [0/54 (0%)]\tTrain Loss: 0.000942\n",
            "Train Epoch: 60 [10/54 (19%)]\tTrain Loss: 0.000209\n",
            "Train Epoch: 60 [20/54 (37%)]\tTrain Loss: 0.000238\n",
            "Train Epoch: 60 [30/54 (56%)]\tTrain Loss: 0.000135\n",
            "Train Epoch: 60 [40/54 (74%)]\tTrain Loss: 0.000109\n",
            "Train Epoch: 60 [50/54 (93%)]\tTrain Loss: 0.000089\n",
            "\n",
            "Train set: Average loss: 0.0006, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [9.64668579e-03 1.31488992e-02 6.94553507e-03 4.71068807e-02\n",
            " 1.48531550e-03 4.10664044e-02 8.85042772e-02 3.05167064e-02\n",
            " 8.19106214e-03 7.47045577e-02 9.91835475e-01 1.23142228e-02\n",
            " 9.24973309e-01 3.04058085e-05 1.19000208e-04 1.88162496e-18\n",
            " 6.26658391e-07 6.25912116e-06 3.74107845e-02 8.35161936e-03\n",
            " 9.37742054e-01 4.88018066e-01 6.99947536e-01 9.99470890e-01\n",
            " 6.99487269e-01 4.26298171e-01 9.97015119e-01 2.56162342e-02\n",
            " 1.50909871e-01 6.57809069e-05 2.63910112e-03 1.96694389e-01\n",
            " 4.85592671e-02 2.09370599e-04 1.30391389e-04 1.24885142e-03\n",
            " 3.84024112e-03 6.00736449e-03 8.82583559e-01 3.64870042e-01\n",
            " 5.65885484e-01 4.75719571e-01 4.57692802e-01 7.67234305e-04\n",
            " 7.32055426e-01 6.27019778e-02 6.19523972e-02 1.94850601e-02\n",
            " 1.92924008e-01 3.55111882e-02 4.00107950e-01 1.02523077e-11\n",
            " 8.14760952e-08 1.68175440e-09 9.80352044e-01 1.19235774e-05\n",
            " 6.92269921e-01 1.64288576e-05 2.57493550e-04 1.00512552e-05\n",
            " 9.98601258e-01 9.98417258e-01 9.99697804e-01 9.99526143e-01\n",
            " 6.30703270e-01 9.98707891e-01 9.95043278e-01 9.99992609e-01\n",
            " 9.92975116e-01 9.91706491e-01 7.50622511e-01 6.10168874e-01\n",
            " 9.99767005e-01 7.79402733e-01 7.24205196e-01 9.95161951e-01\n",
            " 9.99149323e-01 9.99965072e-01 1.00000000e+00 9.99996066e-01\n",
            " 1.00000000e+00 9.99057233e-01 1.00000000e+00 7.24026680e-01\n",
            " 1.30765691e-01 9.98892605e-01 9.99118388e-01 9.97993827e-01\n",
            " 7.02338278e-01 9.99415874e-01 9.99041855e-01 9.14163888e-01\n",
            " 7.54887313e-02 9.95845616e-01 9.99382138e-01 9.95905280e-01\n",
            " 3.12183164e-02 7.77910233e-01 8.17436218e-01 9.84042048e-01\n",
            " 9.26807404e-01 7.04034626e-01 1.92230433e-01 8.53060745e-04\n",
            " 3.97956837e-03 5.23837566e-01 1.69307366e-01 6.22096658e-01\n",
            " 6.24222577e-01 2.69624721e-02 2.39983529e-01 8.55088472e-01\n",
            " 9.27238882e-01 9.99752581e-01 5.69864810e-01 4.63802107e-02\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 47 TN= 45 FN= 11 FP= 15\n",
            "TP+FP 62\n",
            "precision 0.7580645161290323\n",
            "recall 0.8103448275862069\n",
            "F1 0.7833333333333333\n",
            "acc 0.7796610169491526\n",
            "AUCp 0.7801724137931034\n",
            "AUC 0.8623563218390805\n",
            "\n",
            " The epoch is 60, average recall: 0.8103, average precision: 0.7581,average F1: 0.7833, average accuracy: 0.7797, average AUC: 0.8624\n",
            "Train Epoch: 61 [0/54 (0%)]\tTrain Loss: 0.000412\n",
            "Train Epoch: 61 [10/54 (19%)]\tTrain Loss: 0.000074\n",
            "Train Epoch: 61 [20/54 (37%)]\tTrain Loss: 0.000180\n",
            "Train Epoch: 61 [30/54 (56%)]\tTrain Loss: 0.000033\n",
            "Train Epoch: 61 [40/54 (74%)]\tTrain Loss: 0.001119\n",
            "Train Epoch: 61 [50/54 (93%)]\tTrain Loss: 0.000224\n",
            "\n",
            "Train set: Average loss: 0.0018, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.13538259e-02 3.11293080e-02 6.30914280e-03 4.84133139e-02\n",
            " 8.48073250e-05 4.37273458e-02 1.39702037e-01 2.01195255e-02\n",
            " 7.02022016e-03 8.85256156e-02 9.91967142e-01 1.78931113e-02\n",
            " 9.56887543e-01 6.72815531e-07 2.19217745e-05 4.52928990e-18\n",
            " 2.89401170e-10 2.70630579e-07 4.74769529e-03 1.25725223e-02\n",
            " 9.48830843e-01 4.18281287e-01 8.14436078e-01 9.98734176e-01\n",
            " 2.09082603e-01 7.73187220e-01 9.97141778e-01 1.22866230e-02\n",
            " 2.41638571e-02 8.94494860e-06 3.00067617e-03 3.66318710e-02\n",
            " 4.22021151e-02 9.73290007e-05 1.41301382e-04 3.16458347e-04\n",
            " 2.29030475e-03 9.75344796e-04 8.09898794e-01 2.42475420e-01\n",
            " 2.32322931e-01 1.09677084e-01 3.29497397e-01 7.02256075e-05\n",
            " 6.69429421e-01 4.70250435e-02 1.44999504e-01 3.49483602e-02\n",
            " 5.17629325e-01 1.39716297e-01 8.04318666e-01 3.33900306e-13\n",
            " 4.62724273e-07 3.22465082e-10 9.93592322e-01 8.04784759e-06\n",
            " 9.02863324e-01 1.70046605e-05 6.26779802e-05 9.34571199e-06\n",
            " 9.99301791e-01 9.99056399e-01 9.99970913e-01 9.99768674e-01\n",
            " 9.16385055e-01 9.97518897e-01 9.90708351e-01 9.99906421e-01\n",
            " 9.93173778e-01 9.96551394e-01 9.13568676e-01 8.14943194e-01\n",
            " 9.99774277e-01 7.64441669e-01 6.77247584e-01 9.98411655e-01\n",
            " 9.99975204e-01 9.99999762e-01 1.00000000e+00 9.99982715e-01\n",
            " 9.99999762e-01 9.99357641e-01 1.00000000e+00 1.72591761e-01\n",
            " 2.98622865e-02 9.97512817e-01 9.99707162e-01 9.99132812e-01\n",
            " 7.58664906e-01 9.94320154e-01 9.98388529e-01 9.80823636e-01\n",
            " 7.67026022e-02 9.97386277e-01 9.99601662e-01 9.97438431e-01\n",
            " 3.81816700e-02 9.60662425e-01 7.94748247e-01 9.73425806e-01\n",
            " 9.16443825e-01 6.34697735e-01 2.64909044e-02 1.17032486e-03\n",
            " 5.11713885e-03 9.58941042e-01 5.07094085e-01 6.82089150e-01\n",
            " 3.69531155e-01 2.42605601e-02 3.77104729e-01 8.87225330e-01\n",
            " 9.57661271e-01 9.99471724e-01 6.72086596e-01 2.82189995e-02\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 62 [0/54 (0%)]\tTrain Loss: 0.000055\n",
            "Train Epoch: 62 [10/54 (19%)]\tTrain Loss: 0.000119\n",
            "Train Epoch: 62 [20/54 (37%)]\tTrain Loss: 0.000039\n",
            "Train Epoch: 62 [30/54 (56%)]\tTrain Loss: 0.000061\n",
            "Train Epoch: 62 [40/54 (74%)]\tTrain Loss: 0.000464\n",
            "Train Epoch: 62 [50/54 (93%)]\tTrain Loss: 0.000029\n",
            "\n",
            "Train set: Average loss: 0.0004, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.15447454e-02 3.30418572e-02 6.51475135e-03 5.07568121e-02\n",
            " 1.51004788e-04 8.16642568e-02 1.18311763e-01 3.12601365e-02\n",
            " 1.06548537e-02 8.04974511e-02 9.94127274e-01 1.84679609e-02\n",
            " 9.59013820e-01 8.08052646e-05 2.75798200e-04 3.13211290e-16\n",
            " 1.26297635e-07 3.63309141e-06 1.25328936e-02 5.68795903e-03\n",
            " 9.76857662e-01 7.18630254e-01 8.81539941e-01 9.99666929e-01\n",
            " 7.30831504e-01 8.67637277e-01 9.98899460e-01 1.82370413e-02\n",
            " 1.51263863e-01 3.60779486e-05 4.92390525e-03 7.11576939e-01\n",
            " 2.12035388e-01 1.44727092e-04 1.10475507e-04 3.65022162e-04\n",
            " 1.93692592e-03 5.85515983e-03 9.54578400e-01 5.24784267e-01\n",
            " 5.81426382e-01 4.48846042e-01 4.30496663e-01 2.43026312e-04\n",
            " 8.66014719e-01 1.22792974e-01 3.84793788e-01 8.00350383e-02\n",
            " 5.71750164e-01 1.99102536e-01 8.66908073e-01 1.50296942e-10\n",
            " 1.94554741e-06 1.26028232e-09 9.97404754e-01 1.73927747e-05\n",
            " 9.47772801e-01 1.37976940e-05 1.90238934e-04 1.66976315e-05\n",
            " 9.99931097e-01 9.99992967e-01 1.00000000e+00 9.99999523e-01\n",
            " 9.22568560e-01 9.99768198e-01 9.99445856e-01 9.99997616e-01\n",
            " 9.99276817e-01 9.99617457e-01 9.52060938e-01 9.23518956e-01\n",
            " 9.99998689e-01 9.27033782e-01 8.39776814e-01 9.99598205e-01\n",
            " 9.99999762e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99949098e-01 1.00000000e+00 9.37440634e-01\n",
            " 6.90165684e-02 9.99961495e-01 9.99934793e-01 9.99947309e-01\n",
            " 8.15259039e-01 9.99767125e-01 9.99715865e-01 9.90722418e-01\n",
            " 8.30856413e-02 9.99170184e-01 9.99883175e-01 9.99001563e-01\n",
            " 9.02462602e-02 9.81161296e-01 9.52899218e-01 9.97514367e-01\n",
            " 9.64737535e-01 9.54802036e-01 1.12582251e-01 2.13638693e-03\n",
            " 5.59934787e-03 9.66230810e-01 6.07012987e-01 7.14909077e-01\n",
            " 6.13199413e-01 2.38310788e-02 4.69387472e-01 9.31808293e-01\n",
            " 9.81608689e-01 9.99936342e-01 9.82813060e-01 4.39184427e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 63 [0/54 (0%)]\tTrain Loss: 0.000024\n",
            "Train Epoch: 63 [10/54 (19%)]\tTrain Loss: 0.000158\n",
            "Train Epoch: 63 [20/54 (37%)]\tTrain Loss: 0.036601\n",
            "Train Epoch: 63 [30/54 (56%)]\tTrain Loss: 0.000154\n",
            "Train Epoch: 63 [40/54 (74%)]\tTrain Loss: 0.000899\n",
            "Train Epoch: 63 [50/54 (93%)]\tTrain Loss: 0.000681\n",
            "\n",
            "Train set: Average loss: 0.0026, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.22509478e-02 4.99340072e-02 8.64875503e-03 1.60827428e-01\n",
            " 3.93539929e-04 1.71821237e-01 6.59244180e-01 4.89611328e-02\n",
            " 7.27841109e-02 8.58881548e-02 9.93806839e-01 1.61138847e-02\n",
            " 9.43332851e-01 2.13221274e-03 1.47433183e-03 4.68396589e-12\n",
            " 2.38331959e-05 2.50256649e-04 7.55309016e-02 3.86662409e-03\n",
            " 9.74037170e-01 6.39927149e-01 7.23816752e-01 9.99998093e-01\n",
            " 4.63774174e-01 8.56912076e-01 9.99727547e-01 3.00909579e-02\n",
            " 3.84805351e-02 7.52442575e-05 3.81221855e-03 7.68351555e-02\n",
            " 3.60784829e-02 4.62804455e-04 1.70240004e-04 1.06258318e-03\n",
            " 3.47272772e-03 6.59726141e-03 9.48939383e-01 6.31023526e-01\n",
            " 5.03856599e-01 3.52181971e-01 6.67901933e-01 2.90407159e-04\n",
            " 3.76955509e-01 6.40872642e-02 2.09794268e-01 4.09195870e-02\n",
            " 6.45995498e-01 3.10794890e-01 8.82320344e-01 7.76118370e-08\n",
            " 3.49124421e-05 5.41399325e-09 9.95261908e-01 8.12400976e-05\n",
            " 9.64172602e-01 5.85928792e-05 8.63654714e-04 7.82921634e-05\n",
            " 9.99620914e-01 9.99952912e-01 9.99997973e-01 9.99910474e-01\n",
            " 8.11157167e-01 9.99993086e-01 9.99831319e-01 9.99997139e-01\n",
            " 9.99932051e-01 9.99329090e-01 3.96638483e-01 4.03347045e-01\n",
            " 9.99897361e-01 6.90301001e-01 8.12761188e-01 9.95834351e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999046e-01\n",
            " 1.00000000e+00 9.99725282e-01 1.00000000e+00 8.51267695e-01\n",
            " 1.69945732e-01 9.98825848e-01 9.99050915e-01 9.97346520e-01\n",
            " 7.68988073e-01 9.99470890e-01 9.99767840e-01 9.72850323e-01\n",
            " 5.90179227e-02 9.99666452e-01 9.99933720e-01 9.99456942e-01\n",
            " 3.28750983e-02 9.68203843e-01 7.76326954e-01 9.95389938e-01\n",
            " 8.29382956e-01 6.26697361e-01 8.16822797e-02 1.23333791e-03\n",
            " 2.65180040e-03 9.43756282e-01 5.17783463e-01 6.02918983e-01\n",
            " 7.57986724e-01 2.56823376e-02 4.49414700e-01 9.52745974e-01\n",
            " 9.73094881e-01 9.99837399e-01 9.26063061e-01 1.15477070e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 64 [0/54 (0%)]\tTrain Loss: 0.001735\n",
            "Train Epoch: 64 [10/54 (19%)]\tTrain Loss: 0.000051\n",
            "Train Epoch: 64 [20/54 (37%)]\tTrain Loss: 0.000040\n",
            "Train Epoch: 64 [30/54 (56%)]\tTrain Loss: 0.000030\n",
            "Train Epoch: 64 [40/54 (74%)]\tTrain Loss: 0.000093\n",
            "Train Epoch: 64 [50/54 (93%)]\tTrain Loss: 0.000782\n",
            "\n",
            "Train set: Average loss: 0.0019, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [7.62406643e-03 3.17063242e-01 8.73224810e-03 4.71931845e-02\n",
            " 3.82467580e-04 7.19586909e-02 6.26986682e-01 3.05428151e-02\n",
            " 4.76431437e-02 1.40614629e-01 9.95574832e-01 1.96431279e-02\n",
            " 9.62623715e-01 3.07578797e-04 5.73572870e-05 2.08490794e-10\n",
            " 3.00729084e-06 6.30578234e-06 5.59128566e-05 5.44657989e-04\n",
            " 8.29938054e-01 1.68041810e-02 7.01074675e-02 9.99982476e-01\n",
            " 2.20912565e-02 2.71776676e-01 9.99301314e-01 5.99497138e-03\n",
            " 6.80690937e-05 4.08207461e-06 2.90103699e-03 6.83172140e-04\n",
            " 1.15718525e-02 4.65781253e-04 4.40490956e-04 1.89324736e-03\n",
            " 4.25373437e-03 2.70854891e-03 9.41079855e-01 4.27162588e-01\n",
            " 1.58511609e-01 9.83933434e-02 3.59389752e-01 4.45430996e-05\n",
            " 1.45395966e-02 5.59724942e-02 9.87485498e-02 2.73820236e-02\n",
            " 7.57617414e-01 4.17756468e-01 9.17575657e-01 1.50584043e-08\n",
            " 1.41474329e-05 7.62747661e-08 9.92597103e-01 1.84263117e-05\n",
            " 9.77182686e-01 7.24280972e-06 1.47626339e-03 8.20721834e-05\n",
            " 9.99789298e-01 9.99771655e-01 9.98985231e-01 9.98907685e-01\n",
            " 4.94490683e-01 9.98603523e-01 9.98583674e-01 9.99997497e-01\n",
            " 9.99955177e-01 9.99478042e-01 1.62482023e-01 3.59933227e-01\n",
            " 9.97725785e-01 8.24996233e-01 7.94817805e-01 9.99423385e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99836922e-01\n",
            " 9.99677062e-01 9.99510884e-01 1.00000000e+00 2.29146048e-01\n",
            " 3.17375883e-02 9.98182833e-01 9.99452531e-01 9.99073625e-01\n",
            " 9.76329684e-01 9.95984912e-01 9.99718487e-01 8.22046101e-01\n",
            " 2.80148946e-02 9.98762131e-01 9.99222517e-01 9.97275174e-01\n",
            " 1.63553804e-02 9.94435430e-01 9.81893897e-01 9.87067401e-01\n",
            " 8.87490273e-01 8.45198631e-01 1.97034642e-01 2.07022205e-03\n",
            " 8.23024288e-03 9.82726932e-01 6.93439901e-01 4.37327683e-01\n",
            " 7.07773507e-01 1.81140490e-02 2.71658272e-01 9.81234610e-01\n",
            " 8.03937376e-01 9.95338202e-01 8.80617201e-01 5.44707142e-02\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 65 [0/54 (0%)]\tTrain Loss: 0.000249\n",
            "Train Epoch: 65 [10/54 (19%)]\tTrain Loss: 0.000102\n",
            "Train Epoch: 65 [20/54 (37%)]\tTrain Loss: 0.004157\n",
            "Train Epoch: 65 [30/54 (56%)]\tTrain Loss: 0.001268\n",
            "Train Epoch: 65 [40/54 (74%)]\tTrain Loss: 0.008443\n",
            "Train Epoch: 65 [50/54 (93%)]\tTrain Loss: 0.000052\n",
            "\n",
            "Train set: Average loss: 0.0034, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.90994193e-03 1.40039548e-01 7.67659768e-03 2.88096219e-01\n",
            " 3.17430869e-03 1.40129760e-01 4.69043553e-01 5.40317185e-02\n",
            " 9.58680809e-02 9.04909670e-02 9.97698724e-01 9.80240852e-03\n",
            " 9.31000650e-01 8.79556406e-03 2.36232355e-02 5.38774714e-07\n",
            " 8.40366556e-05 1.92399875e-05 1.56268820e-01 5.50996475e-02\n",
            " 9.90178704e-01 7.48189092e-01 7.41950631e-01 9.99986291e-01\n",
            " 7.89227664e-01 5.76674879e-01 9.99027610e-01 1.56814214e-02\n",
            " 2.55341351e-01 2.65873061e-03 2.88383965e-03 4.93995957e-02\n",
            " 3.79422270e-02 1.56820519e-03 1.08869746e-03 2.68882024e-03\n",
            " 5.67356497e-03 2.50167828e-02 9.82753873e-01 5.81322253e-01\n",
            " 4.19744432e-01 5.17628491e-01 3.52033526e-01 9.29548696e-04\n",
            " 5.08543491e-01 7.27262422e-02 5.57732284e-02 1.20347710e-02\n",
            " 4.25358176e-01 1.01676613e-01 7.55904496e-01 6.00332305e-06\n",
            " 2.80033943e-04 3.44066757e-05 9.91705656e-01 1.10982079e-03\n",
            " 9.84689713e-01 2.72257399e-04 2.15043663e-03 5.41010289e-04\n",
            " 9.99985814e-01 9.99969840e-01 9.99994636e-01 9.99965549e-01\n",
            " 9.98456717e-01 9.99997616e-01 9.99962330e-01 9.99984145e-01\n",
            " 9.99992847e-01 9.99721467e-01 2.28629276e-01 5.69491088e-01\n",
            " 9.99483347e-01 9.73654270e-01 9.92679238e-01 9.94253933e-01\n",
            " 9.99999404e-01 1.00000000e+00 1.00000000e+00 9.99692202e-01\n",
            " 1.00000000e+00 9.99733150e-01 1.00000000e+00 6.25868887e-02\n",
            " 1.67058241e-02 9.97803152e-01 9.99264657e-01 9.98903155e-01\n",
            " 9.24024224e-01 9.99908090e-01 9.99798000e-01 9.91076589e-01\n",
            " 2.69109160e-01 9.99088049e-01 9.99798715e-01 9.98522818e-01\n",
            " 2.89913923e-01 9.94699001e-01 9.88199234e-01 9.96867955e-01\n",
            " 8.74416411e-01 8.73652518e-01 2.96082854e-01 9.41946451e-03\n",
            " 5.01195341e-03 9.89349604e-01 8.30261767e-01 5.98205805e-01\n",
            " 9.31949914e-01 3.82543914e-02 5.92656255e-01 9.42632794e-01\n",
            " 9.50685978e-01 9.97804344e-01 9.83905613e-01 2.80701578e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 66 [0/54 (0%)]\tTrain Loss: 0.000375\n",
            "Train Epoch: 66 [10/54 (19%)]\tTrain Loss: 0.000044\n",
            "Train Epoch: 66 [20/54 (37%)]\tTrain Loss: 0.000086\n",
            "Train Epoch: 66 [30/54 (56%)]\tTrain Loss: 0.000062\n",
            "Train Epoch: 66 [40/54 (74%)]\tTrain Loss: 0.000750\n",
            "Train Epoch: 66 [50/54 (93%)]\tTrain Loss: 0.000141\n",
            "\n",
            "Train set: Average loss: 0.0006, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.36107954e-03 9.11432598e-03 4.31198115e-03 1.89865291e-01\n",
            " 1.27676816e-03 5.67602813e-02 3.16774510e-02 2.99672130e-02\n",
            " 2.70968806e-02 7.02574998e-02 9.97356176e-01 9.24736541e-03\n",
            " 9.18216169e-01 8.82035878e-04 2.21242220e-03 4.72644700e-07\n",
            " 3.36266021e-05 8.68232109e-06 1.54830539e-03 1.74225029e-03\n",
            " 8.79281998e-01 1.69152528e-01 1.77226648e-01 9.99999762e-01\n",
            " 2.25165457e-01 1.28853172e-01 9.99226809e-01 5.85600920e-03\n",
            " 6.37820456e-03 1.06408901e-03 2.04133056e-03 2.26597395e-02\n",
            " 3.38624381e-02 8.94614379e-04 9.90584842e-04 1.75666180e-03\n",
            " 3.02850455e-03 1.17119709e-02 9.73177850e-01 2.78229773e-01\n",
            " 1.88446000e-01 3.31197977e-01 3.15324157e-01 6.69154921e-04\n",
            " 1.73816130e-01 8.39171112e-02 1.67125896e-01 1.60288848e-02\n",
            " 5.93805909e-01 7.78860003e-02 8.73006165e-01 1.74381148e-05\n",
            " 1.58580704e-04 2.43940649e-05 9.83701825e-01 6.18385908e-04\n",
            " 9.76776779e-01 2.63463531e-04 1.60224072e-03 4.00203251e-04\n",
            " 9.99993563e-01 9.99979138e-01 1.00000000e+00 9.99999642e-01\n",
            " 9.95128870e-01 9.99998927e-01 9.99899507e-01 9.99999642e-01\n",
            " 9.99999046e-01 9.99809086e-01 1.89071998e-01 3.29324871e-01\n",
            " 9.99707997e-01 8.90994191e-01 9.83905852e-01 9.97070074e-01\n",
            " 9.99999881e-01 1.00000000e+00 1.00000000e+00 9.99999762e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 2.76654847e-02\n",
            " 8.88772123e-03 9.99775589e-01 9.99704063e-01 9.99924541e-01\n",
            " 5.69941938e-01 9.99475420e-01 9.99101758e-01 9.84166145e-01\n",
            " 1.01767674e-01 9.99308467e-01 9.99839902e-01 9.95352864e-01\n",
            " 9.14589129e-03 9.75280404e-01 6.51201129e-01 9.94944274e-01\n",
            " 7.00969696e-01 8.38071764e-01 6.49870187e-02 3.83686973e-03\n",
            " 2.42311764e-03 9.57903802e-01 7.18857586e-01 3.81290019e-01\n",
            " 8.11636031e-01 1.61815509e-02 5.15208185e-01 9.11787152e-01\n",
            " 8.97925317e-01 9.70431089e-01 9.20704007e-01 9.57298204e-02\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 67 [0/54 (0%)]\tTrain Loss: 0.000055\n",
            "Train Epoch: 67 [10/54 (19%)]\tTrain Loss: 0.000325\n",
            "Train Epoch: 67 [20/54 (37%)]\tTrain Loss: 0.000042\n",
            "Train Epoch: 67 [30/54 (56%)]\tTrain Loss: 0.000216\n",
            "Train Epoch: 67 [40/54 (74%)]\tTrain Loss: 0.000045\n",
            "Train Epoch: 67 [50/54 (93%)]\tTrain Loss: 0.000367\n",
            "\n",
            "Train set: Average loss: 0.0038, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.05061678e-03 1.12882648e-02 5.93240140e-03 3.05496246e-01\n",
            " 1.17437739e-03 2.26068199e-02 2.64605172e-02 7.22128525e-02\n",
            " 1.16433948e-01 3.10136508e-02 9.73335683e-01 3.34605714e-03\n",
            " 5.38718939e-01 7.23555640e-05 2.41505704e-03 2.28047071e-07\n",
            " 2.44812509e-05 1.97350664e-06 1.14898640e-03 5.84288174e-03\n",
            " 9.32813168e-01 2.08233930e-02 6.80337012e-01 9.99996424e-01\n",
            " 2.40045674e-02 2.78851599e-01 9.98567581e-01 1.02982149e-02\n",
            " 3.65815423e-02 6.64007477e-03 1.15603022e-03 8.14663246e-02\n",
            " 5.55370934e-02 7.07319647e-04 4.98414680e-04 5.37853863e-04\n",
            " 1.03922596e-03 6.59672618e-02 9.89731610e-01 2.56116122e-01\n",
            " 3.30241084e-01 2.70209640e-01 3.02307069e-01 1.61465001e-03\n",
            " 6.56831801e-01 1.52204394e-01 5.67445308e-02 8.38217698e-03\n",
            " 1.62206709e-01 7.21636638e-02 4.69264865e-01 1.08165341e-06\n",
            " 1.22925529e-04 8.17880755e-06 9.59459722e-01 1.57146205e-04\n",
            " 9.80260432e-01 6.98363292e-05 1.34128449e-03 5.22134476e-04\n",
            " 9.99997616e-01 9.99998331e-01 9.99998927e-01 9.99999642e-01\n",
            " 9.98239040e-01 9.99955654e-01 9.99746263e-01 1.00000000e+00\n",
            " 9.99997020e-01 9.73046899e-01 4.66058403e-02 1.68839723e-01\n",
            " 9.99694586e-01 9.72763658e-01 9.96515155e-01 9.98913527e-01\n",
            " 9.99996901e-01 1.00000000e+00 1.00000000e+00 9.99999404e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 2.71699000e-02\n",
            " 4.84459102e-03 9.99927878e-01 9.99928951e-01 9.99817789e-01\n",
            " 7.40315795e-01 9.81536925e-01 9.99647617e-01 9.33103383e-01\n",
            " 8.09598789e-02 9.99493837e-01 9.99763668e-01 9.99151826e-01\n",
            " 1.80184618e-02 8.99643958e-01 8.16876769e-01 8.10377657e-01\n",
            " 6.72643006e-01 9.25582290e-01 1.47964694e-02 1.80423271e-03\n",
            " 9.97723895e-04 6.92547321e-01 3.23344916e-02 6.66556805e-02\n",
            " 5.10085166e-01 1.12827951e-02 5.27934134e-02 4.46308821e-01\n",
            " 4.70885813e-01 9.96105969e-01 8.61061633e-01 1.25672460e-01\n",
            " 1.00000000e+00 9.99999762e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 68 [0/54 (0%)]\tTrain Loss: 0.000021\n",
            "Train Epoch: 68 [10/54 (19%)]\tTrain Loss: 0.001063\n",
            "Train Epoch: 68 [20/54 (37%)]\tTrain Loss: 0.000527\n",
            "Train Epoch: 68 [30/54 (56%)]\tTrain Loss: 0.000676\n",
            "Train Epoch: 68 [40/54 (74%)]\tTrain Loss: 0.000287\n",
            "Train Epoch: 68 [50/54 (93%)]\tTrain Loss: 0.000155\n",
            "\n",
            "Train set: Average loss: 0.0036, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.31476953e-03 7.55351633e-02 1.13947503e-02 7.60970712e-01\n",
            " 2.41792505e-03 8.62744078e-02 4.50326681e-01 2.59077251e-01\n",
            " 3.41553479e-01 5.92281997e-01 9.97839034e-01 2.84042880e-02\n",
            " 9.53786850e-01 9.91208362e-04 1.81624275e-02 4.27348859e-08\n",
            " 4.23639649e-05 5.24536663e-05 2.17758166e-03 1.34231254e-01\n",
            " 9.48832452e-01 6.11619473e-01 9.38090503e-01 9.99994516e-01\n",
            " 9.35301960e-01 9.28617954e-01 9.99791920e-01 3.46122012e-02\n",
            " 1.11704431e-01 7.00447522e-03 1.60217960e-03 5.65212548e-01\n",
            " 3.60868305e-01 6.45777793e-04 1.42904304e-04 7.41618569e-04\n",
            " 8.31685145e-04 7.04996824e-01 9.99251544e-01 9.02515590e-01\n",
            " 9.10489678e-01 9.03165340e-01 5.53637624e-01 1.97481527e-03\n",
            " 8.32623422e-01 3.46464753e-01 2.27888804e-02 2.97189574e-03\n",
            " 2.59623677e-01 5.47723532e-01 5.01094759e-01 1.22338903e-07\n",
            " 1.50702865e-04 2.40169555e-07 9.51788068e-01 2.30200545e-04\n",
            " 9.90282059e-01 2.18253790e-05 1.44310016e-03 5.30570978e-04\n",
            " 9.99999523e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99213219e-01 9.99996543e-01 9.99981999e-01 1.00000000e+00\n",
            " 9.99998689e-01 9.98707175e-01 6.27023131e-02 2.24286780e-01\n",
            " 9.99785721e-01 9.99480426e-01 9.99938250e-01 9.99023199e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99992132e-01\n",
            " 1.00000000e+00 9.99997139e-01 1.00000000e+00 9.96535778e-01\n",
            " 1.30541980e-01 9.99741733e-01 9.99923110e-01 9.99613941e-01\n",
            " 9.70731020e-01 9.99988556e-01 1.00000000e+00 9.99460638e-01\n",
            " 9.47670221e-01 9.99961376e-01 9.99978185e-01 9.99889135e-01\n",
            " 5.07108986e-01 9.99195993e-01 9.72414553e-01 9.98435915e-01\n",
            " 9.86521125e-01 9.99933362e-01 8.97610128e-01 5.32698222e-02\n",
            " 2.45811115e-03 9.94268656e-01 9.93221283e-01 4.91160721e-01\n",
            " 9.19937670e-01 1.34774018e-02 2.17927352e-01 9.35562789e-01\n",
            " 8.63310635e-01 9.99990940e-01 9.98677313e-01 9.99405503e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 69 [0/54 (0%)]\tTrain Loss: 0.000047\n",
            "Train Epoch: 69 [10/54 (19%)]\tTrain Loss: 0.000168\n",
            "Train Epoch: 69 [20/54 (37%)]\tTrain Loss: 0.000081\n",
            "Train Epoch: 69 [30/54 (56%)]\tTrain Loss: 0.002409\n",
            "Train Epoch: 69 [40/54 (74%)]\tTrain Loss: 0.000079\n",
            "Train Epoch: 69 [50/54 (93%)]\tTrain Loss: 0.000911\n",
            "\n",
            "Train set: Average loss: 0.0049, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.51919823e-03 3.13022062e-02 6.98791863e-03 9.16010916e-01\n",
            " 3.10698012e-03 1.40064344e-01 9.66364667e-02 7.24468887e-01\n",
            " 2.71433055e-01 7.36528710e-02 9.96672034e-01 7.20324088e-03\n",
            " 9.73666608e-01 4.57239425e-04 1.36860665e-02 1.14880159e-08\n",
            " 3.54482872e-05 1.01039404e-05 9.13354335e-04 6.31113201e-02\n",
            " 9.38230395e-01 4.15577680e-01 8.35052073e-01 9.99754846e-01\n",
            " 6.97228670e-01 7.52069712e-01 9.95906353e-01 6.68364484e-03\n",
            " 7.70040080e-02 1.91366638e-03 2.24761944e-03 9.88758087e-01\n",
            " 6.07969165e-01 4.49995423e-04 1.00274716e-04 6.05267007e-04\n",
            " 4.97388653e-04 9.81545627e-01 9.99656439e-01 6.87167704e-01\n",
            " 8.26337218e-01 9.35467660e-01 8.09923291e-01 1.82097079e-03\n",
            " 9.87720311e-01 9.13630664e-01 2.90099561e-01 1.47974696e-02\n",
            " 4.71829325e-02 3.16401874e-03 2.29880378e-01 1.77348525e-07\n",
            " 4.55812769e-05 1.11383308e-07 9.18891191e-01 1.82237447e-04\n",
            " 9.72967327e-01 1.34306647e-05 7.51195068e-04 1.24670842e-04\n",
            " 9.99951601e-01 9.99996066e-01 9.99989152e-01 9.99997735e-01\n",
            " 9.99941468e-01 1.00000000e+00 9.99999404e-01 1.00000000e+00\n",
            " 9.99998689e-01 9.99650836e-01 3.62153232e-01 9.53615010e-01\n",
            " 9.99979377e-01 9.99847531e-01 9.99989033e-01 9.99532938e-01\n",
            " 9.99999881e-01 1.00000000e+00 1.00000000e+00 9.99998927e-01\n",
            " 1.00000000e+00 9.99999762e-01 1.00000000e+00 9.87620533e-01\n",
            " 2.24053692e-02 9.99937057e-01 9.99825299e-01 9.99772251e-01\n",
            " 9.89055872e-01 9.99990702e-01 9.99998212e-01 9.99349415e-01\n",
            " 9.94380772e-01 9.99712765e-01 9.99913096e-01 9.99806345e-01\n",
            " 2.71719605e-01 9.94708776e-01 9.92379665e-01 9.98476446e-01\n",
            " 9.62586224e-01 9.99730885e-01 8.37081015e-01 3.84632200e-02\n",
            " 1.47881277e-03 9.87092257e-01 8.05173635e-01 4.55336869e-01\n",
            " 8.18528712e-01 2.28343960e-02 2.97658563e-01 7.56244123e-01\n",
            " 8.79590690e-01 9.99644041e-01 9.99654531e-01 9.99709308e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 70 [0/54 (0%)]\tTrain Loss: 0.000077\n",
            "Train Epoch: 70 [10/54 (19%)]\tTrain Loss: 0.000170\n",
            "Train Epoch: 70 [20/54 (37%)]\tTrain Loss: 0.000782\n",
            "Train Epoch: 70 [30/54 (56%)]\tTrain Loss: 0.024780\n",
            "Train Epoch: 70 [40/54 (74%)]\tTrain Loss: 0.000120\n",
            "Train Epoch: 70 [50/54 (93%)]\tTrain Loss: 0.000018\n",
            "\n",
            "Train set: Average loss: 0.0022, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.86856839e-03 6.12834655e-03 3.89963877e-03 9.95294154e-01\n",
            " 1.09353317e-02 9.31976557e-01 1.68121368e-01 9.96947110e-01\n",
            " 8.36575925e-01 1.17944248e-01 9.96223092e-01 1.64072625e-02\n",
            " 9.86630321e-01 3.69114608e-01 2.84069460e-02 2.87587568e-06\n",
            " 2.98971485e-04 3.98660486e-04 3.32633667e-02 4.59345341e-01\n",
            " 9.64664996e-01 9.61580276e-01 9.75081563e-01 1.00000000e+00\n",
            " 9.98350978e-01 9.93861318e-01 9.99963522e-01 1.17457733e-02\n",
            " 9.18522757e-03 3.00695479e-04 7.71329831e-03 9.99795139e-01\n",
            " 6.07345179e-02 9.24165361e-04 3.68817913e-04 2.25390727e-03\n",
            " 1.17921608e-03 9.99973059e-01 9.99876857e-01 9.99717891e-01\n",
            " 9.99247313e-01 9.99727428e-01 7.29982436e-01 7.16003706e-04\n",
            " 9.99866128e-01 9.96389925e-01 9.89593625e-01 5.29519618e-02\n",
            " 3.79062623e-01 2.24423409e-02 9.95377660e-01 4.74823673e-06\n",
            " 2.55859643e-03 3.38270866e-06 9.96518970e-01 2.30304170e-02\n",
            " 9.93923724e-01 5.96299469e-05 1.76007254e-03 6.52181974e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 8.89570713e-01 9.93724883e-01\n",
            " 1.00000000e+00 9.99999881e-01 1.00000000e+00 9.99999881e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99272764e-01\n",
            " 2.05722824e-02 1.00000000e+00 1.00000000e+00 9.99999881e-01\n",
            " 9.99804318e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99996305e-01 9.99999404e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.95461524e-01 9.99880910e-01 9.92427528e-01 9.99996543e-01\n",
            " 9.98964906e-01 9.99696732e-01 9.77314949e-01 1.73445016e-01\n",
            " 3.34614608e-03 9.99998212e-01 9.99461710e-01 9.07186568e-01\n",
            " 9.71798122e-01 3.94269032e-03 8.55066597e-01 9.96309102e-01\n",
            " 9.99578893e-01 1.00000000e+00 9.99997854e-01 9.99246478e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 45 TN= 47 FN= 13 FP= 13\n",
            "TP+FP 58\n",
            "precision 0.7758620689655172\n",
            "recall 0.7758620689655172\n",
            "F1 0.7758620689655173\n",
            "acc 0.7796610169491526\n",
            "AUCp 0.7795977011494254\n",
            "AUC 0.8718390804597702\n",
            "\n",
            " The epoch is 70, average recall: 0.7759, average precision: 0.7759,average F1: 0.7759, average accuracy: 0.7797, average AUC: 0.8718\n",
            "Train Epoch: 71 [0/54 (0%)]\tTrain Loss: 0.000384\n",
            "Train Epoch: 71 [10/54 (19%)]\tTrain Loss: 0.000052\n",
            "Train Epoch: 71 [20/54 (37%)]\tTrain Loss: 0.001219\n",
            "Train Epoch: 71 [30/54 (56%)]\tTrain Loss: 0.000032\n",
            "Train Epoch: 71 [40/54 (74%)]\tTrain Loss: 0.000006\n",
            "Train Epoch: 71 [50/54 (93%)]\tTrain Loss: 0.000083\n",
            "\n",
            "Train set: Average loss: 0.0016, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.33912358e-03 2.12597242e-03 2.16275407e-03 8.84655595e-01\n",
            " 8.16923392e-04 6.93184137e-01 4.08663861e-02 5.68975031e-01\n",
            " 3.72136682e-01 8.18485767e-02 9.69006538e-01 9.82417632e-03\n",
            " 9.19526935e-01 1.70384068e-03 8.08618148e-04 1.30510699e-07\n",
            " 8.75681872e-05 2.27380442e-04 4.76566423e-03 9.36772022e-03\n",
            " 8.25653374e-01 8.73573661e-01 5.78267753e-01 9.99986053e-01\n",
            " 9.85222280e-01 3.70593518e-01 9.71224964e-01 1.11170951e-03\n",
            " 2.69137672e-04 9.94127040e-05 6.95300041e-05 2.29402538e-03\n",
            " 8.79851810e-04 4.19123768e-04 1.30084649e-04 7.95052212e-04\n",
            " 6.89535635e-04 8.04404616e-01 9.30329919e-01 8.18942726e-01\n",
            " 8.40117693e-01 9.08320129e-01 2.95508415e-01 2.13604053e-05\n",
            " 5.76042831e-01 3.31696752e-03 5.17253811e-03 4.53337794e-04\n",
            " 2.00154167e-03 1.15374848e-03 7.60565652e-03 2.08710674e-07\n",
            " 1.28160493e-04 5.56510088e-07 8.67224395e-01 1.46965834e-03\n",
            " 9.40100908e-01 1.39602962e-05 1.63915497e-03 6.22315929e-05\n",
            " 9.99844551e-01 9.99996066e-01 9.99987245e-01 9.99999762e-01\n",
            " 9.99426246e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99998569e-01 9.99888062e-01 2.63973116e-03 1.90531425e-02\n",
            " 9.91198003e-01 8.67213666e-01 9.98948395e-01 9.72087443e-01\n",
            " 9.98981893e-01 9.99988675e-01 1.00000000e+00 9.99905229e-01\n",
            " 1.00000000e+00 9.99203384e-01 9.99999166e-01 8.67434442e-01\n",
            " 9.32199694e-03 9.98306870e-01 9.99886155e-01 9.99033928e-01\n",
            " 9.66939032e-01 1.00000000e+00 1.00000000e+00 9.99687552e-01\n",
            " 9.98488307e-01 9.99965310e-01 9.99998927e-01 9.99994159e-01\n",
            " 6.36614859e-01 9.97237444e-01 6.15411043e-01 9.98762846e-01\n",
            " 9.96222377e-01 9.40490365e-01 1.68119222e-01 6.56665070e-03\n",
            " 3.42402782e-04 9.97683525e-01 7.87434340e-01 5.79969227e-01\n",
            " 8.91904593e-01 2.34908797e-03 4.64622498e-01 9.84374821e-01\n",
            " 9.92564142e-01 9.99776661e-01 9.93331254e-01 9.58364666e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 72 [0/54 (0%)]\tTrain Loss: 0.000238\n",
            "Train Epoch: 72 [10/54 (19%)]\tTrain Loss: 0.000143\n",
            "Train Epoch: 72 [20/54 (37%)]\tTrain Loss: 0.000072\n",
            "Train Epoch: 72 [30/54 (56%)]\tTrain Loss: 0.000423\n",
            "Train Epoch: 72 [40/54 (74%)]\tTrain Loss: 0.000018\n",
            "Train Epoch: 72 [50/54 (93%)]\tTrain Loss: 0.000053\n",
            "\n",
            "Train set: Average loss: 0.0030, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.13597592e-03 2.22745910e-03 1.76971743e-03 9.43184018e-01\n",
            " 6.31738082e-03 2.14693904e-01 1.05930641e-02 4.21677470e-01\n",
            " 1.35275498e-01 3.04608107e-01 9.99222517e-01 7.31679471e-03\n",
            " 9.92331147e-01 9.42531158e-04 1.22844777e-03 8.08190521e-07\n",
            " 1.95683839e-04 2.27844866e-04 5.11039235e-03 8.24903604e-03\n",
            " 9.16224897e-01 9.73648846e-01 9.74439442e-01 9.99999523e-01\n",
            " 9.95854020e-01 9.11336601e-01 9.89755213e-01 7.52098113e-03\n",
            " 2.29389854e-02 1.14862749e-03 1.07968182e-04 4.33967918e-01\n",
            " 2.59819254e-02 1.18508225e-03 1.87413199e-04 2.08850787e-03\n",
            " 3.88395623e-03 9.90235627e-01 9.15768206e-01 9.80348408e-01\n",
            " 9.84787643e-01 9.84840691e-01 6.51943982e-01 1.21288397e-03\n",
            " 8.98057044e-01 1.92199964e-02 4.10202518e-02 1.99202891e-03\n",
            " 3.43524106e-03 1.30438362e-03 1.40676480e-02 9.54748998e-07\n",
            " 9.67518463e-06 1.44348030e-06 9.03066576e-01 1.31452305e-03\n",
            " 6.08258784e-01 3.57067438e-05 1.08955137e-03 2.19998674e-05\n",
            " 9.99999166e-01 9.99996662e-01 9.99999285e-01 9.99999881e-01\n",
            " 6.38599336e-01 9.99996543e-01 1.00000000e+00 9.99999166e-01\n",
            " 9.99987960e-01 9.98960614e-01 7.76295178e-03 2.92997379e-02\n",
            " 9.99325633e-01 7.69789279e-01 9.83281553e-01 9.84678507e-01\n",
            " 9.98727381e-01 9.99971271e-01 1.00000000e+00 9.99986887e-01\n",
            " 1.00000000e+00 9.99698520e-01 1.00000000e+00 9.79259133e-01\n",
            " 1.95211247e-02 9.99982834e-01 9.99964356e-01 9.99833226e-01\n",
            " 8.59695315e-01 9.99999762e-01 1.00000000e+00 9.99981403e-01\n",
            " 9.44509804e-01 9.99699116e-01 9.99999523e-01 9.99999523e-01\n",
            " 4.13102180e-01 9.47059274e-01 5.30216277e-01 9.98203039e-01\n",
            " 9.90527630e-01 9.35525239e-01 1.25297800e-01 4.62913141e-03\n",
            " 1.94184089e-04 9.73513246e-01 7.29093850e-01 7.29337275e-01\n",
            " 8.57700169e-01 2.37856945e-03 3.63850921e-01 9.78892088e-01\n",
            " 9.87219930e-01 9.99877810e-01 5.92317879e-01 7.95515120e-01\n",
            " 9.99999404e-01 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 73 [0/54 (0%)]\tTrain Loss: 0.000230\n",
            "Train Epoch: 73 [10/54 (19%)]\tTrain Loss: 0.000055\n",
            "Train Epoch: 73 [20/54 (37%)]\tTrain Loss: 0.000071\n",
            "Train Epoch: 73 [30/54 (56%)]\tTrain Loss: 0.000171\n",
            "Train Epoch: 73 [40/54 (74%)]\tTrain Loss: 0.000398\n",
            "Train Epoch: 73 [50/54 (93%)]\tTrain Loss: 0.000038\n",
            "\n",
            "Train set: Average loss: 0.0009, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [7.04638101e-03 4.33846470e-03 2.59099668e-03 9.75865245e-01\n",
            " 1.95341781e-02 7.35499203e-01 3.75005901e-02 5.03239632e-01\n",
            " 5.18230557e-01 4.75029051e-01 9.99639869e-01 8.74594133e-03\n",
            " 9.97215033e-01 1.67240773e-03 1.06989348e-03 8.56669317e-07\n",
            " 1.67877181e-04 1.81011012e-04 3.87207530e-02 3.38775963e-01\n",
            " 9.85207319e-01 9.87275541e-01 9.93013918e-01 9.99999881e-01\n",
            " 9.98357356e-01 9.95308816e-01 9.98262703e-01 2.23419908e-02\n",
            " 6.71078861e-02 5.05134522e-04 1.23264283e-04 7.02178955e-01\n",
            " 9.96076614e-02 5.83688379e-04 1.82150936e-04 1.60253816e-03\n",
            " 2.90232012e-03 9.99147296e-01 9.94522810e-01 9.98399675e-01\n",
            " 9.98962998e-01 9.97074246e-01 8.82104099e-01 7.29020592e-03\n",
            " 9.96628582e-01 6.08516335e-01 9.42768008e-02 2.87128007e-03\n",
            " 8.09779670e-03 3.82972998e-03 8.39313641e-02 1.26172324e-06\n",
            " 5.41077425e-05 2.27479995e-06 9.72352445e-01 3.39234876e-03\n",
            " 9.01528299e-01 4.31149492e-05 1.46957219e-03 1.46988368e-05\n",
            " 9.99996424e-01 9.99997258e-01 9.99999046e-01 1.00000000e+00\n",
            " 9.89591658e-01 9.99996185e-01 1.00000000e+00 9.99997497e-01\n",
            " 9.99995947e-01 9.99708354e-01 1.82629209e-02 8.73965994e-02\n",
            " 9.99956489e-01 9.77344930e-01 9.94021952e-01 9.97577846e-01\n",
            " 9.99643922e-01 9.99993563e-01 1.00000000e+00 9.99997377e-01\n",
            " 1.00000000e+00 9.99982715e-01 1.00000000e+00 9.90189016e-01\n",
            " 2.52638478e-02 9.99997139e-01 9.99996424e-01 9.99971986e-01\n",
            " 9.79203463e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.98403490e-01 9.99994397e-01 9.99999881e-01 1.00000000e+00\n",
            " 8.85032415e-01 9.97727573e-01 9.29969132e-01 9.99756634e-01\n",
            " 9.99255717e-01 9.67512488e-01 3.18720967e-01 1.69184487e-02\n",
            " 2.37610802e-04 9.99784529e-01 9.94846106e-01 7.86170542e-01\n",
            " 9.64502037e-01 2.76382640e-03 4.20705676e-01 9.96572375e-01\n",
            " 9.97557998e-01 9.99882579e-01 9.58554983e-01 9.48958695e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 74 [0/54 (0%)]\tTrain Loss: 0.000172\n",
            "Train Epoch: 74 [10/54 (19%)]\tTrain Loss: 0.005011\n",
            "Train Epoch: 74 [20/54 (37%)]\tTrain Loss: 0.000113\n",
            "Train Epoch: 74 [30/54 (56%)]\tTrain Loss: 0.000064\n",
            "Train Epoch: 74 [40/54 (74%)]\tTrain Loss: 0.000059\n",
            "Train Epoch: 74 [50/54 (93%)]\tTrain Loss: 0.000972\n",
            "\n",
            "Train set: Average loss: 0.0006, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.22342694e-03 3.26605537e-03 2.32397951e-03 8.13665092e-01\n",
            " 2.04539206e-03 3.36076707e-01 3.51636708e-02 8.71285796e-02\n",
            " 1.05777599e-01 7.89687932e-02 9.96871293e-01 3.02736019e-03\n",
            " 9.86582816e-01 3.49904149e-04 3.41463310e-04 6.70151650e-08\n",
            " 6.08997470e-05 2.50015873e-05 1.00030133e-03 9.18186456e-03\n",
            " 9.46384251e-01 8.77616167e-01 9.78477597e-01 1.00000000e+00\n",
            " 9.91913915e-01 8.73811245e-01 9.98881519e-01 3.58033297e-03\n",
            " 1.55333418e-03 2.04519980e-04 9.30052556e-05 6.48470283e-01\n",
            " 5.47480322e-02 2.94546277e-04 6.27605696e-05 5.91106655e-04\n",
            " 1.05163839e-03 9.93280590e-01 9.97469187e-01 9.88563061e-01\n",
            " 9.93901491e-01 9.93160605e-01 5.32774925e-01 3.01514170e-04\n",
            " 9.54567015e-01 5.10447519e-03 2.85897707e-03 4.13750357e-04\n",
            " 3.24593252e-03 1.11024419e-03 1.64884441e-02 2.72293050e-08\n",
            " 3.73482976e-06 1.04975356e-07 7.80294716e-01 1.65542078e-04\n",
            " 9.47614133e-01 2.02646934e-05 6.31520874e-04 6.36596997e-06\n",
            " 9.99999881e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.86105025e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99998450e-01 9.99837279e-01 1.21310167e-02 1.29323334e-01\n",
            " 9.99969840e-01 9.82982576e-01 9.96306300e-01 9.99574721e-01\n",
            " 9.99928236e-01 9.99999404e-01 1.00000000e+00 9.99998093e-01\n",
            " 1.00000000e+00 9.99999642e-01 1.00000000e+00 9.95276690e-01\n",
            " 9.42631997e-03 9.99999881e-01 9.99999642e-01 9.99991894e-01\n",
            " 9.70616579e-01 1.00000000e+00 1.00000000e+00 9.99926090e-01\n",
            " 9.75167930e-01 9.99981880e-01 9.99999285e-01 1.00000000e+00\n",
            " 9.31255147e-02 9.98254836e-01 4.35245842e-01 9.99611557e-01\n",
            " 9.99295354e-01 9.90853906e-01 1.47981048e-01 3.18364822e-03\n",
            " 1.33593130e-04 9.99716938e-01 9.96067643e-01 1.30500376e-01\n",
            " 8.40817630e-01 1.29004184e-03 7.59893060e-02 9.85391676e-01\n",
            " 9.90203619e-01 9.99225497e-01 9.96027946e-01 9.85543728e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 75 [0/54 (0%)]\tTrain Loss: 0.000086\n",
            "Train Epoch: 75 [10/54 (19%)]\tTrain Loss: 0.000783\n",
            "Train Epoch: 75 [20/54 (37%)]\tTrain Loss: 0.000100\n",
            "Train Epoch: 75 [30/54 (56%)]\tTrain Loss: 0.000100\n",
            "Train Epoch: 75 [40/54 (74%)]\tTrain Loss: 0.000046\n",
            "Train Epoch: 75 [50/54 (93%)]\tTrain Loss: 0.000065\n",
            "\n",
            "Train set: Average loss: 0.0013, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.22437945e-03 3.67459143e-03 2.31928867e-03 8.05852592e-01\n",
            " 1.60981005e-03 4.99052644e-01 4.62654866e-02 1.91828579e-01\n",
            " 2.61881828e-01 8.66023377e-02 9.96799350e-01 3.12266569e-03\n",
            " 9.94060457e-01 1.14757859e-03 1.92052350e-04 4.38959002e-09\n",
            " 3.47889400e-05 5.64671755e-05 1.37382504e-02 4.11246926e-01\n",
            " 9.90505278e-01 9.86707330e-01 9.91137028e-01 9.99999046e-01\n",
            " 9.99080300e-01 9.90542471e-01 9.99160647e-01 8.26626457e-03\n",
            " 8.32515862e-03 3.14728750e-05 1.89774961e-04 8.96095514e-01\n",
            " 2.86429487e-02 1.18283628e-04 1.67223970e-05 8.12527258e-04\n",
            " 9.59329947e-04 9.98057067e-01 9.99188840e-01 9.98005331e-01\n",
            " 9.98977423e-01 9.99375522e-01 8.78898859e-01 4.89221537e-04\n",
            " 9.83288467e-01 3.12739089e-02 3.91709292e-03 3.94257891e-04\n",
            " 3.25725158e-03 2.63215695e-03 3.39783989e-02 2.37647635e-09\n",
            " 1.38241821e-05 1.75273254e-08 9.49711859e-01 4.09176049e-04\n",
            " 9.76759732e-01 5.79107382e-06 1.05733343e-03 1.46438413e-06\n",
            " 1.00000000e+00 9.99999166e-01 9.99999523e-01 1.00000000e+00\n",
            " 9.70944166e-01 9.99999642e-01 1.00000000e+00 9.99999762e-01\n",
            " 9.99997735e-01 9.99918342e-01 1.12850089e-02 1.92972064e-01\n",
            " 9.99939442e-01 9.95451987e-01 9.98107910e-01 9.99464810e-01\n",
            " 9.99848962e-01 9.99998093e-01 9.99999881e-01 9.99980092e-01\n",
            " 1.00000000e+00 9.99991059e-01 1.00000000e+00 9.97042120e-01\n",
            " 3.93918306e-02 9.99990463e-01 9.99993920e-01 9.99963164e-01\n",
            " 9.83262420e-01 9.99999881e-01 1.00000000e+00 9.99784887e-01\n",
            " 9.94642735e-01 9.99952435e-01 9.99992371e-01 9.99998927e-01\n",
            " 2.88919598e-01 9.99639273e-01 9.51779425e-01 9.99837041e-01\n",
            " 9.99606788e-01 9.93923247e-01 7.94464409e-01 8.21779668e-03\n",
            " 1.08980545e-04 9.99784172e-01 9.97880101e-01 5.28757393e-01\n",
            " 9.85725403e-01 3.07442248e-03 2.56900311e-01 9.98313069e-01\n",
            " 9.92711425e-01 9.98900890e-01 9.98051882e-01 9.94066000e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 76 [0/54 (0%)]\tTrain Loss: 0.000093\n",
            "Train Epoch: 76 [10/54 (19%)]\tTrain Loss: 0.000059\n",
            "Train Epoch: 76 [20/54 (37%)]\tTrain Loss: 0.000075\n",
            "Train Epoch: 76 [30/54 (56%)]\tTrain Loss: 0.000094\n",
            "Train Epoch: 76 [40/54 (74%)]\tTrain Loss: 0.000093\n",
            "Train Epoch: 76 [50/54 (93%)]\tTrain Loss: 0.000022\n",
            "\n",
            "Train set: Average loss: 0.0014, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.60708381e-03 3.43721989e-03 2.02901545e-03 7.95090973e-01\n",
            " 2.27292092e-03 2.86421925e-01 2.30344851e-02 8.84686634e-02\n",
            " 1.34835780e-01 5.33260033e-02 9.93057370e-01 2.88907019e-03\n",
            " 9.92184699e-01 9.14282515e-04 3.79793288e-04 8.07788147e-09\n",
            " 3.51416202e-05 4.75834422e-05 6.33099116e-03 1.78286299e-01\n",
            " 9.86684978e-01 8.88950944e-01 9.88701224e-01 1.00000000e+00\n",
            " 9.94642258e-01 9.88924921e-01 9.99075890e-01 8.06433707e-02\n",
            " 1.07273264e-02 1.45570099e-04 2.67552674e-01 9.92085516e-01\n",
            " 4.16523546e-01 1.23163234e-04 4.50183979e-05 8.89956194e-04\n",
            " 9.65352927e-04 9.97403681e-01 9.99330163e-01 9.90967572e-01\n",
            " 9.96374547e-01 9.98438299e-01 9.17033017e-01 2.84788292e-03\n",
            " 9.94903922e-01 1.61073998e-01 1.61317978e-02 1.12491276e-03\n",
            " 2.89003197e-02 6.69452548e-03 2.10969716e-01 2.28356249e-08\n",
            " 1.10896333e-04 1.28912404e-06 7.91024566e-01 9.78930038e-04\n",
            " 9.60780382e-01 4.14868045e-05 1.31424889e-03 1.14949153e-05\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.79620397e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99997735e-01 9.99820054e-01 3.25729102e-01 4.99445677e-01\n",
            " 9.99988794e-01 9.97308612e-01 9.97694671e-01 9.99997854e-01\n",
            " 9.99987483e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.97691393e-01\n",
            " 1.35861114e-02 9.99999881e-01 1.00000000e+00 9.99999881e-01\n",
            " 9.83412623e-01 9.99995708e-01 1.00000000e+00 9.99568403e-01\n",
            " 9.93595958e-01 9.99986410e-01 9.99999881e-01 1.00000000e+00\n",
            " 1.18014112e-01 9.98407662e-01 5.16989172e-01 9.99247074e-01\n",
            " 9.97269571e-01 9.86186028e-01 5.72931230e-01 1.73878558e-02\n",
            " 3.35812743e-04 9.99743521e-01 9.97869372e-01 7.92100057e-02\n",
            " 8.50756228e-01 2.00734520e-03 3.65444347e-02 9.68322277e-01\n",
            " 9.76839602e-01 9.99402404e-01 9.97449934e-01 9.92237985e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 77 [0/54 (0%)]\tTrain Loss: 0.000076\n",
            "Train Epoch: 77 [10/54 (19%)]\tTrain Loss: 0.000154\n",
            "Train Epoch: 77 [20/54 (37%)]\tTrain Loss: 0.000104\n",
            "Train Epoch: 77 [30/54 (56%)]\tTrain Loss: 0.002101\n",
            "Train Epoch: 77 [40/54 (74%)]\tTrain Loss: 0.000141\n",
            "Train Epoch: 77 [50/54 (93%)]\tTrain Loss: 0.001482\n",
            "\n",
            "Train set: Average loss: 0.0030, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.90874089e-03 7.04243639e-03 3.30848107e-03 7.56963193e-01\n",
            " 1.50509400e-03 2.19019443e-01 3.00268494e-02 4.62847836e-02\n",
            " 5.40321581e-02 2.73337603e-01 9.94832873e-01 1.61652360e-02\n",
            " 9.82907474e-01 1.23510649e-03 9.51363763e-04 1.91332217e-09\n",
            " 1.20721677e-04 1.59723323e-03 3.33148427e-02 1.24730961e-02\n",
            " 9.64175463e-01 9.44216669e-01 9.90716517e-01 1.00000000e+00\n",
            " 9.97074127e-01 9.75318611e-01 9.99448597e-01 1.69772163e-01\n",
            " 6.39197184e-03 1.09675995e-04 5.67375958e-01 7.61720955e-01\n",
            " 1.31796584e-01 7.61164774e-05 1.84878136e-05 2.33311951e-03\n",
            " 1.52956473e-03 9.57297444e-01 9.85403597e-01 4.62798238e-01\n",
            " 6.12685502e-01 8.19845200e-01 6.41150713e-01 7.42355012e-04\n",
            " 2.43176788e-01 1.60840601e-01 1.33529929e-02 2.76250462e-03\n",
            " 1.51708558e-01 7.92211741e-02 4.31105435e-01 4.33331371e-09\n",
            " 1.91780433e-04 4.01788071e-07 6.39500916e-01 2.11307150e-03\n",
            " 8.67006063e-01 1.12943533e-04 1.40724052e-03 2.95800437e-05\n",
            " 9.99963522e-01 9.99965668e-01 9.93217766e-01 9.98283803e-01\n",
            " 8.97722483e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99962926e-01 9.98649061e-01 3.16913694e-01 1.63422644e-01\n",
            " 9.99294400e-01 7.57863164e-01 9.38600540e-01 9.99260366e-01\n",
            " 9.99986649e-01 1.00000000e+00 9.99999166e-01 9.99999881e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.94215548e-01\n",
            " 1.28724445e-02 9.99983549e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.63851094e-01 9.99967337e-01 9.99999642e-01 9.67345119e-01\n",
            " 8.14733446e-01 9.99987245e-01 9.99998093e-01 9.99999881e-01\n",
            " 4.78828639e-01 9.88813102e-01 1.17807172e-01 7.73671567e-01\n",
            " 9.07831073e-01 9.85001445e-01 2.67864764e-01 1.45214070e-02\n",
            " 1.43015257e-03 9.99369085e-01 9.97094750e-01 2.27218106e-01\n",
            " 8.21489513e-01 2.35205097e-03 5.67258671e-02 8.75702560e-01\n",
            " 9.68822777e-01 9.99393702e-01 9.82649684e-01 9.58148479e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 78 [0/54 (0%)]\tTrain Loss: 0.000096\n",
            "Train Epoch: 78 [10/54 (19%)]\tTrain Loss: 0.000045\n",
            "Train Epoch: 78 [20/54 (37%)]\tTrain Loss: 0.000133\n",
            "Train Epoch: 78 [30/54 (56%)]\tTrain Loss: 0.000106\n",
            "Train Epoch: 78 [40/54 (74%)]\tTrain Loss: 0.000070\n",
            "Train Epoch: 78 [50/54 (93%)]\tTrain Loss: 0.000093\n",
            "\n",
            "Train set: Average loss: 0.0017, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.99158047e-03 2.45113252e-03 1.20627938e-03 8.75220358e-01\n",
            " 3.05817034e-02 9.07370150e-01 5.89252077e-03 7.98288882e-01\n",
            " 9.06491518e-01 9.08443391e-01 9.99947906e-01 6.05960898e-02\n",
            " 9.83272314e-01 5.93585009e-03 1.74295390e-03 3.22251566e-11\n",
            " 4.07956017e-04 7.95287173e-03 7.58098602e-01 5.06390750e-01\n",
            " 9.90986228e-01 9.98896122e-01 9.99434054e-01 9.99999881e-01\n",
            " 9.99974489e-01 9.97924805e-01 9.99951243e-01 2.07169607e-01\n",
            " 6.07184991e-02 3.40205661e-05 3.75558406e-01 8.06173742e-01\n",
            " 1.07025079e-01 4.65654040e-04 3.22317173e-05 1.45996287e-02\n",
            " 9.04963445e-03 9.87221420e-01 9.68129516e-01 9.77013528e-01\n",
            " 9.88353193e-01 9.66922998e-01 9.16856527e-01 6.68771798e-03\n",
            " 9.44977283e-01 9.98786986e-01 7.12517619e-01 2.31750518e-01\n",
            " 3.72328967e-01 9.77115035e-01 9.58393574e-01 4.41037145e-08\n",
            " 1.32114906e-02 1.60431887e-06 9.99202192e-01 6.32406294e-01\n",
            " 9.53682780e-01 6.65720669e-04 3.93358106e-03 1.12291586e-04\n",
            " 9.99999881e-01 1.00000000e+00 9.99771535e-01 1.00000000e+00\n",
            " 6.31289124e-01 1.00000000e+00 1.00000000e+00 9.99999881e-01\n",
            " 9.99761760e-01 9.99937892e-01 3.92680228e-01 4.10493851e-01\n",
            " 9.99935031e-01 8.95194471e-01 9.85110939e-01 9.99402046e-01\n",
            " 9.99464810e-01 1.00000000e+00 9.99867201e-01 9.99997377e-01\n",
            " 1.00000000e+00 9.99884486e-01 1.00000000e+00 9.99753177e-01\n",
            " 1.33291081e-01 9.99999881e-01 9.99999881e-01 9.99997854e-01\n",
            " 9.59482074e-01 1.00000000e+00 9.99996424e-01 9.96306062e-01\n",
            " 8.89935732e-01 9.99999762e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.98544216e-01 9.98628259e-01 9.06766951e-01 9.96721566e-01\n",
            " 9.83162403e-01 9.97803986e-01 4.95743185e-01 1.34090325e-02\n",
            " 5.59508009e-03 9.99602497e-01 9.99484181e-01 9.93115604e-01\n",
            " 9.97976363e-01 1.90895088e-02 8.57414901e-01 9.99610960e-01\n",
            " 9.98759270e-01 9.99929547e-01 9.90898311e-01 9.87673938e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 79 [0/54 (0%)]\tTrain Loss: 0.000019\n",
            "Train Epoch: 79 [10/54 (19%)]\tTrain Loss: 0.000015\n",
            "Train Epoch: 79 [20/54 (37%)]\tTrain Loss: 0.000077\n",
            "Train Epoch: 79 [30/54 (56%)]\tTrain Loss: 0.000177\n",
            "Train Epoch: 79 [40/54 (74%)]\tTrain Loss: 0.000182\n",
            "Train Epoch: 79 [50/54 (93%)]\tTrain Loss: 0.000023\n",
            "\n",
            "Train set: Average loss: 0.0016, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.68257645e-04 3.58339283e-03 6.59839367e-04 8.11943859e-02\n",
            " 7.81399256e-04 7.02661633e-01 1.29523901e-02 3.55123878e-02\n",
            " 3.26976418e-01 7.52188802e-01 9.99965429e-01 3.09201740e-02\n",
            " 9.48168695e-01 1.50853978e-03 7.40666117e-04 1.48965896e-13\n",
            " 6.33804229e-05 4.71684252e-05 5.36504447e-01 8.79493952e-01\n",
            " 9.83464599e-01 9.37611103e-01 9.98859167e-01 9.99999881e-01\n",
            " 9.99429643e-01 9.95669246e-01 9.99957561e-01 3.46624367e-02\n",
            " 2.88651306e-02 1.45976428e-05 3.88646387e-02 4.29518938e-01\n",
            " 1.73365474e-02 1.81554293e-04 9.51806669e-06 2.67787627e-03\n",
            " 4.07045102e-03 2.86430687e-01 9.43637490e-01 8.18817616e-01\n",
            " 8.30401480e-01 7.56739438e-01 1.31555572e-01 1.10689456e-04\n",
            " 3.33142191e-01 7.95821071e-01 6.22316776e-03 1.42926211e-03\n",
            " 4.45906483e-02 6.94410145e-01 3.14821452e-01 9.89804766e-11\n",
            " 7.94801417e-06 1.02212676e-10 9.92928445e-01 5.17595187e-03\n",
            " 8.53948116e-01 4.02516453e-05 6.63797895e-04 4.09076250e-07\n",
            " 9.99999881e-01 1.00000000e+00 9.99976039e-01 9.99993324e-01\n",
            " 4.44730103e-01 9.99992013e-01 9.99959111e-01 1.00000000e+00\n",
            " 9.99236584e-01 9.99805510e-01 3.69680196e-01 3.05543423e-01\n",
            " 9.99870539e-01 6.61862731e-01 9.01574433e-01 9.99956727e-01\n",
            " 9.99976873e-01 1.00000000e+00 9.99984503e-01 9.99978781e-01\n",
            " 1.00000000e+00 9.99988914e-01 1.00000000e+00 9.99035239e-01\n",
            " 1.55346747e-02 1.00000000e+00 9.99999642e-01 9.99988794e-01\n",
            " 9.99455631e-01 9.99999762e-01 1.00000000e+00 8.69868219e-01\n",
            " 4.50957678e-02 9.99999523e-01 1.00000000e+00 9.99999881e-01\n",
            " 8.52984011e-01 9.98020649e-01 9.86904323e-01 9.94034231e-01\n",
            " 9.92523849e-01 9.88391280e-01 8.28039229e-01 2.44064536e-03\n",
            " 8.32326419e-04 9.99867201e-01 9.99767482e-01 8.83859396e-01\n",
            " 9.97523248e-01 4.58767898e-02 1.75586596e-01 9.99549210e-01\n",
            " 9.98820245e-01 9.99960661e-01 9.98484313e-01 9.91210818e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 80 [0/54 (0%)]\tTrain Loss: 0.000083\n",
            "Train Epoch: 80 [10/54 (19%)]\tTrain Loss: 0.000024\n",
            "Train Epoch: 80 [20/54 (37%)]\tTrain Loss: 0.000029\n",
            "Train Epoch: 80 [30/54 (56%)]\tTrain Loss: 0.000068\n",
            "Train Epoch: 80 [40/54 (74%)]\tTrain Loss: 0.000034\n",
            "Train Epoch: 80 [50/54 (93%)]\tTrain Loss: 0.000047\n",
            "\n",
            "Train set: Average loss: 0.0058, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.69089258e-04 1.76978577e-03 4.96997673e-04 1.05995223e-01\n",
            " 5.69260272e-04 3.60353321e-01 1.13179153e-02 2.69245617e-02\n",
            " 5.97767048e-02 7.46091068e-01 9.99953151e-01 1.15923882e-02\n",
            " 8.90426576e-01 4.15561110e-04 7.65729288e-04 6.70626965e-13\n",
            " 6.85335044e-06 6.68544453e-06 1.00285914e-04 1.16749266e-02\n",
            " 3.15023988e-01 8.18734914e-02 9.84211385e-01 9.99963880e-01\n",
            " 9.05351937e-01 8.42898726e-01 9.99181688e-01 5.66822523e-03\n",
            " 8.84225417e-04 3.27910857e-05 5.54517135e-02 6.64392412e-01\n",
            " 4.50288169e-02 5.28238197e-05 1.36622987e-06 7.57895410e-04\n",
            " 1.34171639e-03 1.39768034e-01 9.32640791e-01 2.39714682e-01\n",
            " 1.80945531e-01 2.23357052e-01 1.03878573e-01 1.20028148e-04\n",
            " 1.12043191e-02 9.81059521e-02 1.05161231e-03 4.58192109e-04\n",
            " 1.98533032e-02 1.07095003e-01 8.06869566e-02 2.50097686e-11\n",
            " 4.42436630e-07 3.33215622e-10 7.29363263e-01 1.02109996e-04\n",
            " 6.43182516e-01 1.70609928e-05 2.03532501e-04 1.92187869e-07\n",
            " 9.99988794e-01 9.99995828e-01 9.99812424e-01 9.99608576e-01\n",
            " 8.00621152e-01 9.99997735e-01 9.99922872e-01 1.00000000e+00\n",
            " 9.99659061e-01 9.96531665e-01 2.55231351e-01 1.52648464e-01\n",
            " 9.99355495e-01 2.06861913e-01 9.55746412e-01 9.99890447e-01\n",
            " 9.99982834e-01 1.00000000e+00 9.99999881e-01 9.99989390e-01\n",
            " 1.00000000e+00 9.99999642e-01 1.00000000e+00 9.97009218e-01\n",
            " 4.69195051e-03 9.99997616e-01 9.99996781e-01 9.99958992e-01\n",
            " 9.69797075e-01 9.99941707e-01 9.99999881e-01 3.84354621e-01\n",
            " 2.25033890e-02 9.99946952e-01 9.99952197e-01 9.99977708e-01\n",
            " 4.32955474e-03 9.94149089e-01 8.11867654e-01 8.80803585e-01\n",
            " 9.70580041e-01 9.85280037e-01 3.49170953e-01 4.46727150e-04\n",
            " 1.07277010e-04 7.19899952e-01 9.92480874e-01 2.39198446e-01\n",
            " 7.87527323e-01 8.56482610e-03 4.05801982e-02 9.97876763e-01\n",
            " 9.64944959e-01 9.98875797e-01 9.87577438e-01 9.90543306e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 49 TN= 40 FN= 9 FP= 20\n",
            "TP+FP 69\n",
            "precision 0.7101449275362319\n",
            "recall 0.8448275862068966\n",
            "F1 0.7716535433070866\n",
            "acc 0.7542372881355932\n",
            "AUCp 0.7557471264367817\n",
            "AUC 0.8669540229885058\n",
            "\n",
            " The epoch is 80, average recall: 0.8448, average precision: 0.7101,average F1: 0.7717, average accuracy: 0.7542, average AUC: 0.8670\n",
            "Train Epoch: 81 [0/54 (0%)]\tTrain Loss: 0.000025\n",
            "Train Epoch: 81 [10/54 (19%)]\tTrain Loss: 0.000365\n",
            "Train Epoch: 81 [20/54 (37%)]\tTrain Loss: 0.003155\n",
            "Train Epoch: 81 [30/54 (56%)]\tTrain Loss: 0.007585\n",
            "Train Epoch: 81 [40/54 (74%)]\tTrain Loss: 0.000262\n",
            "Train Epoch: 81 [50/54 (93%)]\tTrain Loss: 0.001317\n",
            "\n",
            "Train set: Average loss: 0.0169, Accuracy: 408/425 (96%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.54114177e-02 4.58301902e-02 1.62898842e-03 9.74450171e-01\n",
            " 7.36971647e-02 9.18686450e-01 1.06806748e-01 6.05722845e-01\n",
            " 3.65788847e-01 9.05462921e-01 7.88442254e-01 4.52104248e-02\n",
            " 9.24998403e-01 2.54214305e-04 6.90507377e-03 2.38571865e-05\n",
            " 3.03425768e-04 7.32819587e-02 9.94177580e-01 9.65248823e-01\n",
            " 9.89017844e-01 9.91438508e-01 9.97953653e-01 9.99925733e-01\n",
            " 9.99762118e-01 9.99737680e-01 9.99070227e-01 9.77335572e-01\n",
            " 3.29451829e-01 3.66083439e-03 9.05096412e-01 9.50883627e-01\n",
            " 9.53526378e-01 8.09874386e-03 1.76015042e-03 2.20346428e-03\n",
            " 2.07439270e-02 9.56560731e-01 8.08244050e-01 8.13846409e-01\n",
            " 8.56400728e-01 3.86414200e-01 7.08857000e-01 4.78834510e-02\n",
            " 9.39957559e-01 9.59124565e-01 7.08140135e-02 7.60394111e-02\n",
            " 9.74621713e-01 9.60717738e-01 9.69242752e-01 4.28236380e-09\n",
            " 2.87120207e-03 5.34267792e-06 9.91395295e-01 4.14624286e-04\n",
            " 9.40009892e-01 2.20766902e-04 1.18669518e-03 6.90724191e-05\n",
            " 9.95889962e-01 9.94938493e-01 9.75140035e-01 9.68886197e-01\n",
            " 9.82039273e-01 9.99485254e-01 9.99170899e-01 9.99995232e-01\n",
            " 9.90300298e-01 9.37876344e-01 9.10388172e-01 7.54308403e-01\n",
            " 9.99003351e-01 8.99415016e-01 8.70418608e-01 9.99895930e-01\n",
            " 9.99998689e-01 9.99991894e-01 9.99999523e-01 9.98279691e-01\n",
            " 9.99989271e-01 9.99399543e-01 9.99999762e-01 9.76061940e-01\n",
            " 2.53246054e-02 9.98928487e-01 9.99983430e-01 9.99902487e-01\n",
            " 9.96617496e-01 9.99275506e-01 9.99921203e-01 9.69494879e-01\n",
            " 9.24220622e-01 9.99940276e-01 9.99887109e-01 9.99977589e-01\n",
            " 9.95321453e-01 9.97385561e-01 9.88632977e-01 9.50858951e-01\n",
            " 8.85898292e-01 9.45658863e-01 7.96990812e-01 2.66153038e-01\n",
            " 3.71780246e-01 9.99419928e-01 9.98558104e-01 4.85010117e-01\n",
            " 9.79625404e-01 1.43063888e-02 1.87456265e-01 9.95284617e-01\n",
            " 9.67468321e-01 9.99675751e-01 7.76608512e-02 9.54404891e-01\n",
            " 9.99999881e-01 9.98616219e-01]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 82 [0/54 (0%)]\tTrain Loss: 0.004495\n",
            "Train Epoch: 82 [10/54 (19%)]\tTrain Loss: 0.000215\n",
            "Train Epoch: 82 [20/54 (37%)]\tTrain Loss: 0.000146\n",
            "Train Epoch: 82 [30/54 (56%)]\tTrain Loss: 0.000061\n",
            "Train Epoch: 82 [40/54 (74%)]\tTrain Loss: 0.000234\n",
            "Train Epoch: 82 [50/54 (93%)]\tTrain Loss: 0.000116\n",
            "\n",
            "Train set: Average loss: 0.0015, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.66116983e-03 1.80510394e-02 9.60561913e-04 7.34880686e-01\n",
            " 2.03069346e-03 5.22810936e-01 2.67451480e-02 3.02873701e-01\n",
            " 5.34849763e-02 6.36287749e-01 9.02161896e-01 1.27350753e-02\n",
            " 9.58651721e-01 8.89905323e-07 3.01990891e-04 5.70588243e-10\n",
            " 6.41478209e-07 5.01698887e-05 9.22253728e-01 3.23760718e-01\n",
            " 9.18082297e-01 8.76181960e-01 9.85128224e-01 9.99840021e-01\n",
            " 9.97174263e-01 9.86515760e-01 9.98817742e-01 4.30221289e-01\n",
            " 8.00574347e-02 2.15326829e-04 6.00012839e-01 9.20320272e-01\n",
            " 9.56610680e-01 5.35310362e-04 2.54392926e-05 1.12760914e-04\n",
            " 1.00666762e-03 8.46552551e-01 5.81623077e-01 4.61775988e-01\n",
            " 6.72120154e-01 9.85473841e-02 1.28823072e-01 4.90940642e-03\n",
            " 7.19096184e-01 9.31291699e-01 1.51634142e-01 1.20328404e-01\n",
            " 9.88022447e-01 8.96953344e-01 9.79991794e-01 3.80488653e-12\n",
            " 8.61617038e-04 1.13525353e-07 9.63121057e-01 3.57691606e-04\n",
            " 7.71194637e-01 2.50846533e-05 1.39170821e-04 2.38938242e-06\n",
            " 9.98619914e-01 9.96599138e-01 9.81866837e-01 9.84672368e-01\n",
            " 8.47643733e-01 9.98939574e-01 9.98484790e-01 9.99991894e-01\n",
            " 9.83244658e-01 9.87509727e-01 8.81883025e-01 5.24944961e-01\n",
            " 9.98391211e-01 8.63360465e-01 7.80007780e-01 9.99887586e-01\n",
            " 9.99999523e-01 9.99994636e-01 1.00000000e+00 9.99874115e-01\n",
            " 9.99999881e-01 9.99439776e-01 1.00000000e+00 9.32206273e-01\n",
            " 9.30200913e-04 9.99112308e-01 9.99969721e-01 9.99872327e-01\n",
            " 9.84765530e-01 9.96628106e-01 9.99883294e-01 9.81488883e-01\n",
            " 6.68771446e-01 9.99848008e-01 9.99547064e-01 9.99819458e-01\n",
            " 9.29866850e-01 9.90806639e-01 9.34744358e-01 5.81621349e-01\n",
            " 8.35924208e-01 7.78837144e-01 4.69135046e-01 4.86877143e-01\n",
            " 5.72310507e-01 9.99474347e-01 9.98645484e-01 2.92732745e-01\n",
            " 9.11394894e-01 6.54474972e-03 9.25489068e-02 9.82105374e-01\n",
            " 9.35675442e-01 9.99445140e-01 2.24750768e-02 1.26614645e-01\n",
            " 1.00000000e+00 9.99377429e-01]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 83 [0/54 (0%)]\tTrain Loss: 0.000235\n",
            "Train Epoch: 83 [10/54 (19%)]\tTrain Loss: 0.011510\n",
            "Train Epoch: 83 [20/54 (37%)]\tTrain Loss: 0.000073\n",
            "Train Epoch: 83 [30/54 (56%)]\tTrain Loss: 0.000316\n",
            "Train Epoch: 83 [40/54 (74%)]\tTrain Loss: 0.000413\n",
            "Train Epoch: 83 [50/54 (93%)]\tTrain Loss: 0.000053\n",
            "\n",
            "Train set: Average loss: 0.0021, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.22271634e-03 5.43773826e-03 6.55513955e-04 6.94163322e-01\n",
            " 1.62503740e-03 4.27684933e-01 1.09284073e-02 2.53947079e-01\n",
            " 2.71132682e-02 4.28143352e-01 9.25785303e-01 6.84877345e-03\n",
            " 9.69911456e-01 3.50753737e-07 1.84312972e-04 8.95966963e-08\n",
            " 1.86180350e-05 1.58993149e-04 2.21605897e-01 4.68162335e-02\n",
            " 7.21769691e-01 8.00553799e-01 9.82727587e-01 9.99987960e-01\n",
            " 9.95031476e-01 9.91249681e-01 9.99607146e-01 9.17201862e-02\n",
            " 3.27983983e-02 4.92349442e-04 5.18302381e-01 9.42084014e-01\n",
            " 9.07815933e-01 1.00898207e-03 1.59164236e-04 3.68604233e-04\n",
            " 1.82562997e-03 7.43928015e-01 3.23489010e-01 3.37210387e-01\n",
            " 5.61134100e-01 3.13912332e-02 7.42433965e-02 3.96612240e-03\n",
            " 6.05233848e-01 9.31299329e-01 3.22758496e-01 1.77667111e-01\n",
            " 9.97066081e-01 7.23991692e-01 9.87243056e-01 2.08720063e-09\n",
            " 1.34208560e-04 6.31124180e-07 8.52816820e-01 8.66055343e-05\n",
            " 6.26791120e-01 2.75112016e-05 1.01801226e-04 1.83356326e-06\n",
            " 9.99996424e-01 9.99785125e-01 9.98744726e-01 9.99164939e-01\n",
            " 7.34676182e-01 9.99975204e-01 9.99476969e-01 1.00000000e+00\n",
            " 9.95302558e-01 9.97072101e-01 9.28987265e-01 7.17593253e-01\n",
            " 9.99749362e-01 8.75885606e-01 8.64104390e-01 9.99999285e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99991775e-01 1.00000000e+00 9.83820319e-01\n",
            " 1.81215233e-03 9.99958277e-01 1.00000000e+00 9.99999762e-01\n",
            " 9.57557321e-01 9.98742282e-01 9.99997973e-01 9.79053855e-01\n",
            " 4.52127784e-01 9.99982715e-01 9.99719203e-01 9.99898553e-01\n",
            " 7.71748900e-01 9.82583344e-01 8.40798497e-01 4.68581527e-01\n",
            " 7.89111495e-01 9.66073215e-01 4.02562976e-01 2.53544927e-01\n",
            " 8.10186416e-02 9.99472320e-01 9.99870658e-01 2.20594108e-01\n",
            " 8.74769568e-01 4.36410774e-03 7.00536519e-02 9.80813324e-01\n",
            " 9.19869184e-01 9.99986529e-01 2.91436985e-02 1.13298424e-01\n",
            " 1.00000000e+00 9.99965906e-01]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 84 [0/54 (0%)]\tTrain Loss: 0.000030\n",
            "Train Epoch: 84 [10/54 (19%)]\tTrain Loss: 0.000148\n",
            "Train Epoch: 84 [20/54 (37%)]\tTrain Loss: 0.000138\n",
            "Train Epoch: 84 [30/54 (56%)]\tTrain Loss: 0.000050\n",
            "Train Epoch: 84 [40/54 (74%)]\tTrain Loss: 0.000031\n",
            "Train Epoch: 84 [50/54 (93%)]\tTrain Loss: 0.000061\n",
            "\n",
            "Train set: Average loss: 0.0007, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.16752538e-04 8.48712772e-03 9.79724922e-04 6.73506379e-01\n",
            " 5.69378783e-04 2.43226856e-01 1.65479314e-02 2.11272016e-01\n",
            " 1.27413608e-02 3.46287042e-01 9.46856380e-01 7.41300359e-03\n",
            " 9.69562709e-01 9.88312081e-08 9.54402931e-05 4.90556085e-09\n",
            " 8.97257905e-06 1.62868513e-04 1.32268772e-01 7.73963556e-02\n",
            " 8.55635345e-01 8.92446399e-01 9.82602596e-01 9.99940634e-01\n",
            " 9.97161865e-01 9.84584868e-01 9.99185264e-01 5.18926606e-02\n",
            " 2.53926981e-02 1.64541372e-04 1.77396089e-01 8.03392649e-01\n",
            " 6.72197282e-01 4.37459705e-04 5.26911026e-05 3.81130580e-04\n",
            " 1.29366550e-03 7.23086119e-01 6.07001066e-01 3.84190381e-01\n",
            " 5.62611103e-01 3.88270058e-02 4.26625647e-02 2.55706604e-03\n",
            " 6.83092237e-01 8.10320199e-01 6.39430657e-02 4.74798791e-02\n",
            " 9.44875956e-01 5.13302505e-01 9.27711129e-01 3.20035154e-09\n",
            " 9.48257803e-05 1.23517452e-07 8.28554153e-01 1.06062005e-04\n",
            " 6.26726985e-01 2.92699169e-05 8.20729692e-05 1.23536461e-06\n",
            " 9.99958038e-01 9.99282181e-01 9.94665027e-01 9.94382977e-01\n",
            " 8.71597588e-01 9.99972701e-01 9.99815881e-01 1.00000000e+00\n",
            " 9.94056463e-01 9.94735420e-01 7.57856011e-01 6.20106399e-01\n",
            " 9.99050677e-01 7.79738963e-01 8.77746344e-01 9.99986649e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999285e-01\n",
            " 1.00000000e+00 9.99966502e-01 1.00000000e+00 9.89074051e-01\n",
            " 1.99621124e-03 9.99489665e-01 9.99999285e-01 9.99975562e-01\n",
            " 9.89522278e-01 9.99387741e-01 9.99994278e-01 9.29046392e-01\n",
            " 3.94535005e-01 9.99987125e-01 9.99686956e-01 9.99939442e-01\n",
            " 9.01179552e-01 9.80885565e-01 8.53064954e-01 5.70465446e-01\n",
            " 8.62055779e-01 9.71992612e-01 4.92377460e-01 1.02117687e-01\n",
            " 2.01856550e-02 9.99035120e-01 9.99285400e-01 2.55325913e-01\n",
            " 9.24693763e-01 4.05124482e-03 6.70656264e-02 9.83974278e-01\n",
            " 8.81163538e-01 9.99985337e-01 6.90089241e-02 3.77342403e-01\n",
            " 1.00000000e+00 9.99984384e-01]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 85 [0/54 (0%)]\tTrain Loss: 0.000360\n",
            "Train Epoch: 85 [10/54 (19%)]\tTrain Loss: 0.000753\n",
            "Train Epoch: 85 [20/54 (37%)]\tTrain Loss: 0.000301\n",
            "Train Epoch: 85 [30/54 (56%)]\tTrain Loss: 0.000058\n",
            "Train Epoch: 85 [40/54 (74%)]\tTrain Loss: 0.000065\n",
            "Train Epoch: 85 [50/54 (93%)]\tTrain Loss: 0.000073\n",
            "\n",
            "Train set: Average loss: 0.0027, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.42761243e-03 9.61147714e-03 1.66638789e-03 8.49161923e-01\n",
            " 2.50808150e-03 6.30466878e-01 3.07649113e-02 5.37530541e-01\n",
            " 7.34393373e-02 2.84982204e-01 9.80771840e-01 1.34841427e-02\n",
            " 9.48990345e-01 1.85886183e-05 3.02909873e-04 3.47487543e-07\n",
            " 2.63649243e-04 5.42220601e-04 1.35997817e-01 1.45624187e-02\n",
            " 6.15055442e-01 8.15177619e-01 9.82659340e-01 9.99965787e-01\n",
            " 9.98207092e-01 9.96362150e-01 9.99023676e-01 1.17737003e-01\n",
            " 3.57763097e-02 1.28767962e-04 5.35932243e-01 9.60027277e-01\n",
            " 9.45975482e-01 8.83119821e-04 1.81591677e-04 8.77989165e-04\n",
            " 3.18274857e-03 6.89529896e-01 6.48462892e-01 3.06489050e-01\n",
            " 3.77257735e-01 6.41884580e-02 9.17539969e-02 1.07100802e-02\n",
            " 4.79026824e-01 9.12871897e-01 5.50687790e-01 1.83664054e-01\n",
            " 9.83929813e-01 8.36906016e-01 9.58095491e-01 2.25589147e-07\n",
            " 2.24243529e-04 7.84834901e-06 6.57688498e-01 1.81238967e-04\n",
            " 6.62367940e-01 6.94510745e-05 3.04972724e-04 8.81163123e-06\n",
            " 9.99997973e-01 9.99970317e-01 9.99881148e-01 9.99953628e-01\n",
            " 5.34974813e-01 9.99999881e-01 9.99935985e-01 1.00000000e+00\n",
            " 9.97918069e-01 9.98443067e-01 8.55257094e-01 7.68119335e-01\n",
            " 9.91333902e-01 6.06723666e-01 8.65204751e-01 9.99997616e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99978423e-01 1.00000000e+00 9.95409191e-01\n",
            " 1.08426725e-02 9.99957919e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.23522532e-01 9.99478877e-01 9.99997973e-01 9.84487712e-01\n",
            " 7.33940542e-01 9.99976754e-01 9.99143720e-01 9.99960423e-01\n",
            " 4.65059698e-01 9.60444093e-01 8.30837667e-01 8.26472402e-01\n",
            " 8.15116763e-01 9.95883405e-01 2.69234866e-01 1.07439324e-01\n",
            " 8.50952566e-02 9.97653782e-01 9.96805668e-01 2.47105137e-01\n",
            " 9.72403288e-01 5.44906501e-03 6.57152086e-02 9.90287185e-01\n",
            " 9.43189204e-01 9.99997973e-01 3.41135710e-01 8.86021793e-01\n",
            " 1.00000000e+00 9.99999881e-01]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 86 [0/54 (0%)]\tTrain Loss: 0.000079\n",
            "Train Epoch: 86 [10/54 (19%)]\tTrain Loss: 0.000155\n",
            "Train Epoch: 86 [20/54 (37%)]\tTrain Loss: 0.000017\n",
            "Train Epoch: 86 [30/54 (56%)]\tTrain Loss: 0.000184\n",
            "Train Epoch: 86 [40/54 (74%)]\tTrain Loss: 0.000034\n",
            "Train Epoch: 86 [50/54 (93%)]\tTrain Loss: 0.000043\n",
            "\n",
            "Train set: Average loss: 0.0010, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.36315776e-03 1.54055478e-02 1.74337369e-03 8.94067705e-01\n",
            " 2.11937516e-03 5.93075633e-01 4.67029139e-02 6.28790855e-01\n",
            " 2.30180081e-02 5.39716244e-01 9.99892831e-01 2.61577684e-02\n",
            " 9.97981429e-01 3.36874336e-06 2.77456147e-05 2.63673030e-07\n",
            " 3.40128958e-04 1.57910457e-03 8.54339659e-01 7.34212622e-02\n",
            " 9.72086847e-01 9.95084941e-01 9.97262359e-01 9.99810040e-01\n",
            " 9.99720514e-01 9.98734772e-01 9.99339879e-01 3.15078974e-01\n",
            " 1.42198533e-01 6.22758816e-05 6.42020345e-01 9.74260390e-01\n",
            " 9.74726319e-01 2.80757551e-03 2.03399235e-04 1.30981149e-03\n",
            " 5.72971953e-03 9.72571492e-01 7.34477460e-01 3.72108400e-01\n",
            " 7.31155992e-01 2.10688740e-01 1.69474393e-01 1.10103458e-01\n",
            " 8.54091823e-01 9.78376985e-01 5.79838157e-01 2.32676610e-01\n",
            " 9.61617768e-01 9.56273496e-01 9.52130437e-01 1.14876864e-08\n",
            " 1.04975326e-04 3.51419715e-07 8.03435266e-01 6.15636643e-04\n",
            " 7.80976415e-01 2.67703253e-05 1.26809318e-04 1.39117401e-06\n",
            " 9.99945760e-01 9.99795020e-01 9.98875082e-01 9.98178005e-01\n",
            " 2.73800790e-01 9.99701440e-01 9.99032497e-01 9.99989986e-01\n",
            " 9.95389700e-01 9.99310493e-01 8.89221311e-01 8.17156851e-01\n",
            " 9.63909209e-01 9.03280795e-01 9.30353224e-01 9.99733865e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99957561e-01\n",
            " 9.99999642e-01 9.98650372e-01 9.99998331e-01 9.99313593e-01\n",
            " 1.76923558e-01 9.99748290e-01 9.99900222e-01 9.99851108e-01\n",
            " 8.29167843e-01 9.99541521e-01 9.99926329e-01 9.98260915e-01\n",
            " 9.86669779e-01 9.99908447e-01 9.99472201e-01 9.99951720e-01\n",
            " 9.55450535e-01 9.91955101e-01 9.76320028e-01 9.87074137e-01\n",
            " 9.78291035e-01 9.99539256e-01 7.53488779e-01 6.32452369e-01\n",
            " 4.35548156e-01 9.99794424e-01 9.94612217e-01 6.05483413e-01\n",
            " 9.93709803e-01 6.23769639e-03 1.16554245e-01 9.95560408e-01\n",
            " 9.77258146e-01 9.99669552e-01 4.50997233e-01 9.91734266e-01\n",
            " 1.00000000e+00 9.99969840e-01]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 87 [0/54 (0%)]\tTrain Loss: 0.000476\n",
            "Train Epoch: 87 [10/54 (19%)]\tTrain Loss: 0.000152\n",
            "Train Epoch: 87 [20/54 (37%)]\tTrain Loss: 0.000011\n",
            "Train Epoch: 87 [30/54 (56%)]\tTrain Loss: 0.000021\n",
            "Train Epoch: 87 [40/54 (74%)]\tTrain Loss: 0.000048\n",
            "Train Epoch: 87 [50/54 (93%)]\tTrain Loss: 0.000067\n",
            "\n",
            "Train set: Average loss: 0.0007, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.18304761e-04 3.81121226e-03 9.10265429e-04 8.18143547e-01\n",
            " 9.40735452e-04 2.97856659e-01 1.73000135e-02 3.32771212e-01\n",
            " 1.31018711e-02 1.57339007e-01 9.99860168e-01 8.40631779e-03\n",
            " 9.95447338e-01 8.52789555e-08 1.30113085e-05 1.84429439e-07\n",
            " 8.35645405e-05 2.23820607e-04 3.37338215e-03 5.38358185e-03\n",
            " 6.20931685e-01 8.94982398e-01 9.95864987e-01 9.99992847e-01\n",
            " 9.99332249e-01 9.98521149e-01 9.99942422e-01 8.39221627e-02\n",
            " 4.85920627e-03 6.80500088e-05 4.79138672e-01 9.38321233e-01\n",
            " 9.24149334e-01 4.25258389e-04 9.84322396e-05 4.73731663e-04\n",
            " 1.74647267e-03 7.61543572e-01 4.13924217e-01 1.58622906e-01\n",
            " 3.24308842e-01 3.18055078e-02 8.24803934e-02 1.14902528e-03\n",
            " 4.93503124e-01 7.28798509e-01 6.91177472e-02 1.66119244e-02\n",
            " 9.51156855e-01 6.84067130e-01 9.08921003e-01 2.12488391e-08\n",
            " 2.72030702e-05 3.33584836e-07 5.01667202e-01 1.13107917e-05\n",
            " 6.96948171e-01 2.53572034e-05 7.77681780e-05 9.73226065e-07\n",
            " 1.00000000e+00 1.00000000e+00 9.99992371e-01 9.99989629e-01\n",
            " 6.77633584e-01 1.00000000e+00 9.99790132e-01 1.00000000e+00\n",
            " 9.95335162e-01 9.99711812e-01 7.30117261e-01 4.44291949e-01\n",
            " 9.87060308e-01 9.23936784e-01 9.86871839e-01 9.99999642e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99974132e-01 1.00000000e+00 9.99498844e-01\n",
            " 5.59672527e-03 9.99988914e-01 1.00000000e+00 1.00000000e+00\n",
            " 8.16274941e-01 9.99838948e-01 9.99999166e-01 9.84896779e-01\n",
            " 8.65002751e-01 9.99999523e-01 9.99973655e-01 9.99997020e-01\n",
            " 3.16799641e-01 9.73389030e-01 8.17915320e-01 9.60730195e-01\n",
            " 9.55367923e-01 9.99767601e-01 1.06800735e-01 9.03499126e-02\n",
            " 2.04486586e-02 9.99464810e-01 9.99609292e-01 3.65270972e-01\n",
            " 9.62240636e-01 2.95443134e-03 5.42207733e-02 9.91150856e-01\n",
            " 9.06536460e-01 9.99999881e-01 7.36209601e-02 9.03020501e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 88 [0/54 (0%)]\tTrain Loss: 0.000261\n",
            "Train Epoch: 88 [10/54 (19%)]\tTrain Loss: 0.000102\n",
            "Train Epoch: 88 [20/54 (37%)]\tTrain Loss: 0.000140\n",
            "Train Epoch: 88 [30/54 (56%)]\tTrain Loss: 0.000015\n",
            "Train Epoch: 88 [40/54 (74%)]\tTrain Loss: 0.000203\n",
            "Train Epoch: 88 [50/54 (93%)]\tTrain Loss: 0.000066\n",
            "\n",
            "Train set: Average loss: 0.0006, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.35064076e-04 3.26374918e-03 7.30402651e-04 7.31803715e-01\n",
            " 6.46993925e-04 1.86367720e-01 1.52813140e-02 2.48662800e-01\n",
            " 8.64662509e-03 1.37144834e-01 9.99606311e-01 6.54463843e-03\n",
            " 9.89085495e-01 3.78123701e-08 1.15881303e-05 7.47743698e-08\n",
            " 5.12859624e-05 1.55848684e-04 1.16700144e-03 2.84293131e-03\n",
            " 5.12040257e-01 7.39242017e-01 9.92025256e-01 9.99994636e-01\n",
            " 9.98194039e-01 9.96538043e-01 9.99925971e-01 4.52883951e-02\n",
            " 2.85841990e-03 7.12884430e-05 1.75411925e-01 8.79402220e-01\n",
            " 8.69566977e-01 3.03636160e-04 8.86276102e-05 3.73868534e-04\n",
            " 1.46262022e-03 6.27269864e-01 2.47094959e-01 1.02727719e-01\n",
            " 2.02601120e-01 1.72439013e-02 6.09723143e-02 6.48811809e-04\n",
            " 3.64266723e-01 2.73991436e-01 2.48582643e-02 7.29386834e-03\n",
            " 8.57324064e-01 3.97533685e-01 7.39092767e-01 1.70772765e-08\n",
            " 2.55937357e-05 3.23553763e-07 4.30046886e-01 7.85756038e-06\n",
            " 6.00012302e-01 2.74937065e-05 5.54861253e-05 8.51698644e-07\n",
            " 1.00000000e+00 9.99999881e-01 9.99989033e-01 9.99985814e-01\n",
            " 6.56776786e-01 1.00000000e+00 9.99772012e-01 1.00000000e+00\n",
            " 9.95899022e-01 9.98784959e-01 5.34915209e-01 2.67439723e-01\n",
            " 9.84108269e-01 8.51252854e-01 9.80652332e-01 9.99999642e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99973416e-01 1.00000000e+00 9.99137521e-01\n",
            " 3.95680545e-03 9.99987602e-01 1.00000000e+00 1.00000000e+00\n",
            " 8.21391106e-01 9.99805152e-01 9.99999166e-01 9.01393235e-01\n",
            " 6.57187283e-01 9.99999404e-01 9.99970078e-01 9.99997258e-01\n",
            " 1.94187701e-01 9.53786016e-01 6.14130199e-01 8.99473727e-01\n",
            " 9.07486081e-01 9.99483585e-01 4.99562882e-02 2.99086217e-02\n",
            " 7.68761104e-03 9.97221231e-01 9.99129236e-01 2.69197345e-01\n",
            " 9.25051749e-01 2.29136203e-03 3.60283889e-02 9.83903408e-01\n",
            " 8.29095483e-01 9.99999881e-01 3.28329913e-02 8.16276848e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 89 [0/54 (0%)]\tTrain Loss: 0.000055\n",
            "Train Epoch: 89 [10/54 (19%)]\tTrain Loss: 0.000011\n",
            "Train Epoch: 89 [20/54 (37%)]\tTrain Loss: 0.000010\n",
            "Train Epoch: 89 [30/54 (56%)]\tTrain Loss: 0.000012\n",
            "Train Epoch: 89 [40/54 (74%)]\tTrain Loss: 0.000014\n",
            "Train Epoch: 89 [50/54 (93%)]\tTrain Loss: 0.000344\n",
            "\n",
            "Train set: Average loss: 0.0022, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.76049233e-03 2.36348119e-02 9.65662999e-04 9.81693685e-01\n",
            " 4.45311628e-02 7.76536405e-01 4.32737201e-01 8.45061719e-01\n",
            " 6.22886002e-01 3.44030946e-01 9.99572456e-01 2.70823911e-02\n",
            " 9.84487832e-01 2.55113955e-05 3.77894699e-04 5.03392039e-06\n",
            " 1.06884516e-03 2.73337821e-03 6.54596537e-02 1.91646460e-02\n",
            " 9.97772157e-01 9.67951357e-01 9.89008963e-01 9.99837637e-01\n",
            " 9.98744011e-01 9.99842763e-01 9.99867797e-01 6.44446492e-01\n",
            " 3.09601631e-02 1.14665505e-04 4.59825218e-01 6.85696304e-01\n",
            " 4.11827505e-01 1.84325222e-03 5.28774457e-04 5.55350818e-03\n",
            " 1.74036529e-02 8.81904542e-01 8.94065499e-01 7.53190577e-01\n",
            " 8.99643600e-01 3.58074278e-01 2.67181039e-01 1.95937837e-03\n",
            " 5.08103311e-01 2.45397449e-01 1.17994957e-02 3.24212131e-03\n",
            " 5.98431647e-01 8.51051450e-01 5.35178304e-01 1.97339276e-07\n",
            " 1.28906904e-04 9.18816056e-07 9.46521878e-01 7.64452489e-06\n",
            " 8.27951312e-01 4.12795380e-05 6.88302272e-04 1.07352707e-05\n",
            " 9.99996781e-01 9.99989986e-01 9.98796105e-01 9.98035371e-01\n",
            " 9.71616328e-01 1.00000000e+00 9.99996662e-01 9.99999285e-01\n",
            " 9.97828424e-01 9.91828442e-01 6.05817735e-01 2.06866473e-01\n",
            " 9.97619092e-01 9.75303054e-01 9.88438070e-01 9.99505043e-01\n",
            " 9.99999046e-01 1.00000000e+00 1.00000000e+00 9.99940872e-01\n",
            " 9.99999881e-01 9.99330163e-01 1.00000000e+00 9.98011708e-01\n",
            " 4.74923104e-02 9.98927176e-01 9.99991775e-01 9.99955177e-01\n",
            " 9.90283787e-01 9.99999404e-01 9.99999762e-01 9.86531913e-01\n",
            " 9.90808427e-01 9.99999642e-01 9.99993443e-01 9.99999404e-01\n",
            " 7.61142075e-01 9.87899899e-01 9.27170336e-01 9.79747653e-01\n",
            " 9.67701554e-01 9.98640954e-01 7.51560330e-01 7.40994215e-02\n",
            " 5.05445600e-02 9.99539375e-01 9.93210614e-01 5.13468504e-01\n",
            " 9.96120393e-01 5.83714852e-03 6.77893087e-02 9.97054458e-01\n",
            " 9.57069099e-01 9.99997497e-01 4.41211075e-01 9.56687510e-01\n",
            " 1.00000000e+00 9.99999642e-01]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 90 [0/54 (0%)]\tTrain Loss: 0.000081\n",
            "Train Epoch: 90 [10/54 (19%)]\tTrain Loss: 0.000082\n",
            "Train Epoch: 90 [20/54 (37%)]\tTrain Loss: 0.000021\n",
            "Train Epoch: 90 [30/54 (56%)]\tTrain Loss: 0.000083\n",
            "Train Epoch: 90 [40/54 (74%)]\tTrain Loss: 0.000021\n",
            "Train Epoch: 90 [50/54 (93%)]\tTrain Loss: 0.000358\n",
            "\n",
            "Train set: Average loss: 0.0013, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.85320841e-03 1.84155814e-02 7.09136308e-04 9.83027279e-01\n",
            " 4.67270881e-01 7.36723006e-01 8.31387877e-01 5.05885065e-01\n",
            " 8.10658038e-02 2.70726889e-01 9.99796331e-01 1.24628572e-02\n",
            " 9.72867906e-01 2.63757794e-03 1.30817993e-02 2.17417517e-04\n",
            " 6.20841421e-03 6.03078632e-03 2.46659499e-02 1.60546903e-03\n",
            " 9.96326268e-01 9.96885836e-01 9.71583784e-01 9.99201238e-01\n",
            " 9.99987483e-01 9.99860883e-01 9.99981523e-01 1.18331805e-01\n",
            " 5.11424104e-03 2.20482500e-04 6.09977424e-01 9.66208994e-01\n",
            " 5.28045535e-01 4.66339588e-02 5.01317810e-03 8.75680670e-02\n",
            " 2.44927660e-01 9.95709419e-01 9.92611587e-01 9.84126747e-01\n",
            " 9.92736101e-01 9.59780455e-01 4.10855055e-01 2.69914940e-02\n",
            " 2.97698081e-02 6.94180548e-01 1.92387719e-02 3.70163703e-03\n",
            " 4.61328447e-01 5.77374816e-01 5.27681947e-01 1.70434942e-05\n",
            " 2.47765682e-04 5.64994298e-06 9.89962637e-01 7.32809531e-06\n",
            " 9.21930432e-01 5.47596101e-05 3.70421883e-04 1.47813507e-05\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 6.83807731e-02 1.00000000e+00 9.99999881e-01 1.00000000e+00\n",
            " 9.99738395e-01 9.87389207e-01 8.57275486e-01 6.73638523e-01\n",
            " 9.99377310e-01 9.81156111e-01 9.93010581e-01 9.99967933e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99989867e-01 1.00000000e+00 9.99982595e-01\n",
            " 9.60675299e-01 9.99997973e-01 9.99975681e-01 9.99905944e-01\n",
            " 9.51338708e-01 1.00000000e+00 1.00000000e+00 9.85258818e-01\n",
            " 9.21112061e-01 9.99984145e-01 9.99985814e-01 9.99964714e-01\n",
            " 1.19541302e-01 9.98595536e-01 9.99236345e-01 9.94912505e-01\n",
            " 9.87019777e-01 9.99998569e-01 9.96983111e-01 7.56844655e-02\n",
            " 2.37287674e-02 9.98350978e-01 9.85203147e-01 2.54547298e-01\n",
            " 9.99503255e-01 1.09827463e-02 8.41042027e-02 9.98408616e-01\n",
            " 8.69398594e-01 9.99757349e-01 9.91536617e-01 9.99607742e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 50 TN= 36 FN= 8 FP= 24\n",
            "TP+FP 74\n",
            "precision 0.6756756756756757\n",
            "recall 0.8620689655172413\n",
            "F1 0.7575757575757576\n",
            "acc 0.7288135593220338\n",
            "AUCp 0.7310344827586207\n",
            "AUC 0.8568965517241379\n",
            "\n",
            " The epoch is 90, average recall: 0.8621, average precision: 0.6757,average F1: 0.7576, average accuracy: 0.7288, average AUC: 0.8569\n",
            "Train Epoch: 91 [0/54 (0%)]\tTrain Loss: 0.000045\n",
            "Train Epoch: 91 [10/54 (19%)]\tTrain Loss: 0.000073\n",
            "Train Epoch: 91 [20/54 (37%)]\tTrain Loss: 0.000036\n",
            "Train Epoch: 91 [30/54 (56%)]\tTrain Loss: 0.000035\n",
            "Train Epoch: 91 [40/54 (74%)]\tTrain Loss: 0.000118\n",
            "Train Epoch: 91 [50/54 (93%)]\tTrain Loss: 0.000132\n",
            "\n",
            "Train set: Average loss: 0.0004, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [9.25151282e-04 9.01397504e-03 6.94880087e-04 8.89635980e-01\n",
            " 4.02950272e-02 2.12329537e-01 7.45566130e-01 9.80861932e-02\n",
            " 2.60842722e-02 6.25379086e-02 9.95807052e-01 4.91934270e-03\n",
            " 9.03809726e-01 3.12184537e-04 1.62280549e-03 1.56566384e-05\n",
            " 1.36064424e-03 6.22063992e-04 6.55943470e-04 8.39792308e-04\n",
            " 9.92405832e-01 9.52015042e-01 9.80380952e-01 9.99948144e-01\n",
            " 9.99801099e-01 9.98478591e-01 9.99997973e-01 2.72853579e-02\n",
            " 1.32626481e-03 1.37772018e-04 1.28267482e-01 7.42936671e-01\n",
            " 2.94170007e-02 4.70002880e-03 1.09317165e-03 1.02841454e-02\n",
            " 3.58258374e-02 8.06660354e-01 9.88152802e-01 9.49125767e-01\n",
            " 9.48509991e-01 7.88558245e-01 1.42193228e-01 2.25261226e-03\n",
            " 5.36789978e-03 6.11892678e-02 1.48145563e-03 4.82387608e-04\n",
            " 2.06932679e-01 2.39285469e-01 2.46879622e-01 1.21938149e-06\n",
            " 1.08665896e-04 2.08758547e-06 9.72347617e-01 3.03633055e-06\n",
            " 8.07817519e-01 2.99147341e-05 1.02693150e-04 7.92827086e-06\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 4.18844931e-02 1.00000000e+00 9.99999762e-01 1.00000000e+00\n",
            " 9.99691606e-01 9.70741808e-01 5.34849942e-01 2.47359291e-01\n",
            " 9.98837531e-01 9.50816929e-01 9.97131705e-01 9.99976039e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99994159e-01\n",
            " 1.00000000e+00 9.99966979e-01 1.00000000e+00 9.99922276e-01\n",
            " 1.72289684e-01 9.99994874e-01 9.99997735e-01 9.99918103e-01\n",
            " 9.59040701e-01 1.00000000e+00 1.00000000e+00 7.54070342e-01\n",
            " 3.05035084e-01 9.99983668e-01 9.99998569e-01 9.99989629e-01\n",
            " 3.78096290e-02 9.99195516e-01 9.92080867e-01 9.90694404e-01\n",
            " 9.80403721e-01 9.99898314e-01 9.87630665e-01 3.98835121e-03\n",
            " 3.28474026e-03 9.92970169e-01 9.89744723e-01 1.06911868e-01\n",
            " 9.99220371e-01 5.32320701e-03 4.19270173e-02 9.94902492e-01\n",
            " 7.73921251e-01 9.99973059e-01 9.73378718e-01 9.97256219e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 92 [0/54 (0%)]\tTrain Loss: 0.000078\n",
            "Train Epoch: 92 [10/54 (19%)]\tTrain Loss: 0.000050\n",
            "Train Epoch: 92 [20/54 (37%)]\tTrain Loss: 0.000006\n",
            "Train Epoch: 92 [30/54 (56%)]\tTrain Loss: 0.000154\n",
            "Train Epoch: 92 [40/54 (74%)]\tTrain Loss: 0.000085\n",
            "Train Epoch: 92 [50/54 (93%)]\tTrain Loss: 0.000044\n",
            "\n",
            "Train set: Average loss: 0.0003, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.89277076e-04 9.20781679e-03 4.02517093e-04 9.00749385e-01\n",
            " 3.92975360e-02 2.69629210e-01 6.24574363e-01 2.99308807e-01\n",
            " 3.31165008e-02 8.97034481e-02 9.98091996e-01 4.36219992e-03\n",
            " 9.39409792e-01 1.49163272e-04 1.32020924e-03 5.16482351e-06\n",
            " 6.64368446e-04 3.09939875e-04 4.88142250e-04 1.39846571e-03\n",
            " 9.95912731e-01 9.65631068e-01 9.91993308e-01 9.99775946e-01\n",
            " 9.99503255e-01 9.99103487e-01 9.99931216e-01 5.53417727e-02\n",
            " 1.82386360e-03 8.61032240e-05 9.24860299e-01 9.97576058e-01\n",
            " 4.71808553e-01 3.42975371e-03 7.33451569e-04 8.38976353e-03\n",
            " 2.92289872e-02 9.90340889e-01 9.92890120e-01 9.47888434e-01\n",
            " 9.40623701e-01 8.84678006e-01 2.51111656e-01 3.80511931e-03\n",
            " 4.55679446e-02 7.45746851e-01 2.14986671e-02 1.62584207e-03\n",
            " 5.43950856e-01 4.17896420e-01 6.08473480e-01 9.41468770e-08\n",
            " 2.78450698e-05 3.67092440e-07 9.69763458e-01 4.05941933e-07\n",
            " 7.66972780e-01 1.18549369e-05 3.11892363e-05 4.40340500e-06\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999881e-01\n",
            " 2.94288993e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99702036e-01 9.97327685e-01 9.55632508e-01 7.62559116e-01\n",
            " 9.99972343e-01 9.95130658e-01 9.99486685e-01 9.99982238e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99995828e-01\n",
            " 1.00000000e+00 9.99997020e-01 1.00000000e+00 9.99860287e-01\n",
            " 1.75930977e-01 9.99992251e-01 9.99992132e-01 9.99917626e-01\n",
            " 9.06704783e-01 9.99999642e-01 9.99999404e-01 9.68285620e-01\n",
            " 8.44770133e-01 9.99951959e-01 9.99993682e-01 9.99994993e-01\n",
            " 4.86504994e-02 9.99042094e-01 9.89049673e-01 9.94280696e-01\n",
            " 9.87289727e-01 9.99920964e-01 9.90015566e-01 1.13312518e-02\n",
            " 7.54169282e-03 9.99384999e-01 9.96346176e-01 2.15911880e-01\n",
            " 9.99289870e-01 5.84030990e-03 8.94629881e-02 9.95123327e-01\n",
            " 9.51132238e-01 9.99780595e-01 9.94831264e-01 9.98133361e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 93 [0/54 (0%)]\tTrain Loss: 0.000015\n",
            "Train Epoch: 93 [10/54 (19%)]\tTrain Loss: 0.000011\n",
            "Train Epoch: 93 [20/54 (37%)]\tTrain Loss: 0.000137\n",
            "Train Epoch: 93 [30/54 (56%)]\tTrain Loss: 0.000038\n",
            "Train Epoch: 93 [40/54 (74%)]\tTrain Loss: 0.000018\n",
            "Train Epoch: 93 [50/54 (93%)]\tTrain Loss: 0.000074\n",
            "\n",
            "Train set: Average loss: 0.0003, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.15182682e-04 2.83957133e-03 3.03441135e-04 7.94680595e-01\n",
            " 1.27721075e-02 8.03766400e-02 2.86693722e-01 6.09770380e-02\n",
            " 1.16159711e-02 9.89051089e-02 9.98659134e-01 5.33935474e-03\n",
            " 9.18934226e-01 9.90671154e-08 4.44557736e-05 4.30078337e-08\n",
            " 2.57789536e-04 9.90238477e-05 1.76948251e-05 4.79170412e-04\n",
            " 9.11331236e-01 3.70830119e-01 9.63592231e-01 9.99951839e-01\n",
            " 9.91218150e-01 9.74765420e-01 9.99970794e-01 8.32163263e-03\n",
            " 3.21930624e-04 6.31567600e-05 1.19415699e-02 3.19900692e-01\n",
            " 1.35258734e-02 9.29376984e-04 4.14186594e-04 2.25666049e-03\n",
            " 1.14892116e-02 5.96164405e-01 8.42131615e-01 4.45346951e-01\n",
            " 2.87769526e-01 8.75587389e-02 1.17370196e-01 6.30679948e-04\n",
            " 4.72187251e-03 2.55698174e-01 5.23956725e-03 6.69670815e-04\n",
            " 5.29632509e-01 2.20924690e-01 4.26015913e-01 6.64416646e-08\n",
            " 3.55478114e-05 4.01430327e-07 8.29235554e-01 6.73702232e-07\n",
            " 4.86622691e-01 1.50498872e-05 4.64021250e-05 3.87551563e-06\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.31419962e-02 9.99999642e-01 9.99992251e-01 1.00000000e+00\n",
            " 9.98665333e-01 9.19364810e-01 7.86681354e-01 1.22851610e-01\n",
            " 9.93281066e-01 8.89658928e-01 9.97168362e-01 9.99995589e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99732673e-01\n",
            " 1.00000000e+00 9.99997616e-01 1.00000000e+00 9.99919295e-01\n",
            " 1.20498501e-02 9.99997020e-01 9.99999404e-01 9.99972701e-01\n",
            " 6.44337118e-01 9.99893665e-01 9.99999881e-01 7.11598158e-01\n",
            " 2.63603508e-01 9.99980330e-01 9.99997497e-01 9.99996662e-01\n",
            " 1.37383519e-02 9.98307347e-01 7.81681418e-01 9.05642927e-01\n",
            " 9.38597500e-01 9.99182522e-01 6.75110281e-01 1.28451118e-03\n",
            " 2.85635493e-03 9.95893240e-01 9.94739711e-01 7.33516142e-02\n",
            " 9.94951248e-01 2.47570407e-03 3.63610387e-02 9.83820915e-01\n",
            " 7.16891289e-01 9.98491287e-01 1.74600948e-02 6.65266275e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 94 [0/54 (0%)]\tTrain Loss: 0.000005\n",
            "Train Epoch: 94 [10/54 (19%)]\tTrain Loss: 0.000043\n",
            "Train Epoch: 94 [20/54 (37%)]\tTrain Loss: 0.000024\n",
            "Train Epoch: 94 [30/54 (56%)]\tTrain Loss: 0.000634\n",
            "Train Epoch: 94 [40/54 (74%)]\tTrain Loss: 0.000005\n",
            "Train Epoch: 94 [50/54 (93%)]\tTrain Loss: 0.000063\n",
            "\n",
            "Train set: Average loss: 0.0002, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [9.22798994e-04 4.40642377e-03 3.00273765e-04 8.74778152e-01\n",
            " 1.20023284e-02 1.42284334e-01 4.50415671e-01 1.60453886e-01\n",
            " 1.69443563e-02 1.27055556e-01 9.99303579e-01 5.16600022e-03\n",
            " 9.52221215e-01 9.70665255e-08 3.98144111e-05 3.12015267e-08\n",
            " 1.67967795e-04 7.57186062e-05 2.24189898e-05 2.81457062e-04\n",
            " 9.19542193e-01 5.91138363e-01 9.67458606e-01 9.99907374e-01\n",
            " 9.95382369e-01 9.83395815e-01 9.99916434e-01 9.48460214e-03\n",
            " 2.16978879e-04 4.10722896e-05 3.81668322e-02 8.82480025e-01\n",
            " 1.29863560e-01 7.99900328e-04 3.10723641e-04 2.60736188e-03\n",
            " 1.09545020e-02 8.38157773e-01 8.94575417e-01 6.04953945e-01\n",
            " 3.87064755e-01 1.60926193e-01 2.76695907e-01 5.91424177e-04\n",
            " 7.07194395e-03 6.94255650e-01 5.31491116e-02 2.12693494e-03\n",
            " 8.98271859e-01 3.38907927e-01 8.63148034e-01 5.06553199e-08\n",
            " 2.86938412e-05 1.62407403e-07 8.72866631e-01 5.83639917e-07\n",
            " 7.16613412e-01 1.25425013e-05 3.96828764e-05 3.18421985e-06\n",
            " 1.00000000e+00 1.00000000e+00 9.99999881e-01 9.99999881e-01\n",
            " 1.65248364e-02 1.00000000e+00 9.99999762e-01 1.00000000e+00\n",
            " 9.99166727e-01 9.93912995e-01 9.74970043e-01 4.44220006e-01\n",
            " 9.98447537e-01 9.53602135e-01 9.98361766e-01 9.99991775e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99923944e-01\n",
            " 1.00000000e+00 9.99986410e-01 1.00000000e+00 9.99774754e-01\n",
            " 9.91859380e-03 9.99998331e-01 9.99997139e-01 9.99978185e-01\n",
            " 5.94963849e-01 9.99887347e-01 9.99999642e-01 9.04180825e-01\n",
            " 6.11817360e-01 9.99966264e-01 9.99989748e-01 9.99995470e-01\n",
            " 1.46863256e-02 9.98451471e-01 8.67437065e-01 9.43772435e-01\n",
            " 9.46277261e-01 9.99629259e-01 6.38805032e-01 3.73945548e-03\n",
            " 5.52665908e-03 9.97861326e-01 9.97504890e-01 1.11175247e-01\n",
            " 9.95912373e-01 2.04880815e-03 4.71640937e-02 9.82534707e-01\n",
            " 7.92570114e-01 9.98111606e-01 7.05146343e-02 8.54542971e-01\n",
            " 1.00000000e+00 9.99999762e-01]\n",
            "predict [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 95 [0/54 (0%)]\tTrain Loss: 0.000115\n",
            "Train Epoch: 95 [10/54 (19%)]\tTrain Loss: 0.000166\n",
            "Train Epoch: 95 [20/54 (37%)]\tTrain Loss: 0.000023\n",
            "Train Epoch: 95 [30/54 (56%)]\tTrain Loss: 0.000003\n",
            "Train Epoch: 95 [40/54 (74%)]\tTrain Loss: 0.000041\n",
            "Train Epoch: 95 [50/54 (93%)]\tTrain Loss: 0.000013\n",
            "\n",
            "Train set: Average loss: 0.0026, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [8.02363735e-04 1.07095586e-02 3.41076462e-04 9.57572043e-01\n",
            " 4.11533192e-02 4.25444454e-01 6.72088504e-01 4.32508647e-01\n",
            " 3.73042934e-02 3.31498921e-01 9.99993801e-01 1.01123340e-02\n",
            " 9.92150545e-01 1.59288334e-06 7.16621507e-05 1.26919574e-07\n",
            " 2.42517068e-04 1.71782871e-04 5.79634681e-04 7.47605052e-04\n",
            " 9.88408864e-01 9.31326091e-01 9.79699135e-01 9.99947429e-01\n",
            " 9.99563992e-01 9.97992873e-01 9.99958992e-01 1.87028330e-02\n",
            " 4.35336231e-04 3.69128102e-05 4.45983894e-02 9.40926909e-01\n",
            " 3.39303285e-01 2.14515906e-03 5.09389036e-04 4.60116519e-03\n",
            " 2.41451468e-02 9.95666504e-01 9.69347894e-01 9.19697106e-01\n",
            " 9.17457640e-01 6.05774283e-01 5.81920922e-01 2.18836800e-03\n",
            " 2.46670693e-02 9.41472352e-01 1.19740747e-01 4.71936818e-03\n",
            " 9.14815187e-01 6.07145250e-01 8.97386730e-01 4.14542107e-08\n",
            " 3.17814483e-05 1.18066943e-07 9.61232305e-01 8.96154688e-07\n",
            " 8.49467337e-01 1.08160530e-05 5.47476084e-05 2.32338584e-06\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 2.34977342e-02 9.99999881e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.99679446e-01 9.96887505e-01 9.80655193e-01 4.09780711e-01\n",
            " 9.99264657e-01 9.86687064e-01 9.98720407e-01 9.99999046e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99998331e-01\n",
            " 1.00000000e+00 9.99998450e-01 1.00000000e+00 9.99975324e-01\n",
            " 9.52026397e-02 9.99999523e-01 9.99999881e-01 9.99997616e-01\n",
            " 7.82505035e-01 9.99996424e-01 1.00000000e+00 9.78733659e-01\n",
            " 8.75688493e-01 9.99996901e-01 9.99998808e-01 9.99999285e-01\n",
            " 5.70653267e-02 9.99175131e-01 9.88742173e-01 9.94613707e-01\n",
            " 9.88404393e-01 9.99913573e-01 9.84930515e-01 2.05305312e-02\n",
            " 1.85963046e-02 9.99858379e-01 9.98579025e-01 2.95462221e-01\n",
            " 9.99521255e-01 2.74293683e-03 1.08166866e-01 9.97091889e-01\n",
            " 9.24459755e-01 9.99745786e-01 5.40317595e-01 9.93377268e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 96 [0/54 (0%)]\tTrain Loss: 0.000004\n",
            "Train Epoch: 96 [10/54 (19%)]\tTrain Loss: 0.000403\n",
            "Train Epoch: 96 [20/54 (37%)]\tTrain Loss: 0.000006\n",
            "Train Epoch: 96 [30/54 (56%)]\tTrain Loss: 0.000031\n",
            "Train Epoch: 96 [40/54 (74%)]\tTrain Loss: 0.000015\n",
            "Train Epoch: 96 [50/54 (93%)]\tTrain Loss: 0.000046\n",
            "\n",
            "Train set: Average loss: 0.0068, Accuracy: 419/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.41647661e-02 3.51364873e-02 8.42233712e-04 7.31643558e-01\n",
            " 5.93674406e-02 8.35713148e-01 4.18868661e-01 9.71274018e-01\n",
            " 2.07105964e-01 4.18638676e-01 9.98533845e-01 1.17361138e-03\n",
            " 9.36025262e-01 1.45972427e-02 1.19526479e-02 5.18457000e-07\n",
            " 4.41987577e-05 8.18542030e-05 2.00682180e-03 6.88596815e-02\n",
            " 9.99773085e-01 9.96855617e-01 9.97707009e-01 1.00000000e+00\n",
            " 9.96295989e-01 9.99995112e-01 9.99998689e-01 3.86433870e-01\n",
            " 5.05727716e-03 1.62350494e-04 3.88797894e-02 9.98494148e-01\n",
            " 9.19338584e-01 2.39757690e-04 1.65920399e-04 6.72631711e-03\n",
            " 7.80558661e-02 9.74220574e-01 1.92997873e-01 4.67406690e-01\n",
            " 8.14636111e-01 4.47480708e-01 3.07549983e-01 1.13414429e-01\n",
            " 9.52427506e-01 8.12728703e-01 9.69657719e-01 4.45729584e-01\n",
            " 8.86446476e-01 9.50484335e-01 9.72054780e-01 3.55492724e-09\n",
            " 1.65576569e-03 2.26392416e-09 4.67758924e-01 8.86701382e-05\n",
            " 6.92520797e-01 5.78516810e-05 1.47876881e-06 5.15045540e-05\n",
            " 9.99999881e-01 9.99839783e-01 9.98944938e-01 9.99945045e-01\n",
            " 9.99999285e-01 1.00000000e+00 1.00000000e+00 9.99999881e-01\n",
            " 9.73134041e-01 9.96284366e-01 9.95204091e-01 9.86755192e-01\n",
            " 1.00000000e+00 9.99993205e-01 9.78764653e-01 9.99760926e-01\n",
            " 9.86329138e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99999046e-01 1.00000000e+00 9.28341389e-01\n",
            " 6.80293962e-02 9.99970794e-01 9.99985695e-01 9.99998808e-01\n",
            " 9.88078058e-01 1.00000000e+00 1.00000000e+00 9.93098319e-01\n",
            " 9.99506235e-01 9.99999404e-01 9.99992251e-01 1.00000000e+00\n",
            " 9.98376250e-01 7.32616782e-01 9.94725406e-01 9.75841284e-01\n",
            " 9.94295657e-01 9.44163144e-01 6.06237948e-01 5.43454289e-02\n",
            " 3.28493744e-01 9.98790801e-01 9.65229392e-01 2.95507729e-01\n",
            " 1.77997455e-01 1.68613659e-03 2.61778742e-01 5.75943962e-02\n",
            " 2.21855626e-01 9.98628139e-01 2.40865797e-01 8.46135020e-01\n",
            " 1.00000000e+00 9.99999881e-01]\n",
            "predict [0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 97 [0/54 (0%)]\tTrain Loss: 0.000656\n",
            "Train Epoch: 97 [10/54 (19%)]\tTrain Loss: 0.000019\n",
            "Train Epoch: 97 [20/54 (37%)]\tTrain Loss: 0.000014\n",
            "Train Epoch: 97 [30/54 (56%)]\tTrain Loss: 0.000028\n",
            "Train Epoch: 97 [40/54 (74%)]\tTrain Loss: 0.000009\n",
            "Train Epoch: 97 [50/54 (93%)]\tTrain Loss: 0.001654\n",
            "\n",
            "Train set: Average loss: 0.0042, Accuracy: 421/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.33549416e-01 2.09737737e-02 3.62536288e-03 7.34651744e-01\n",
            " 7.51375267e-03 9.52001810e-01 9.04619992e-01 9.95485723e-01\n",
            " 2.30201140e-01 7.01550305e-01 9.75902259e-01 4.77031618e-03\n",
            " 8.04858029e-01 1.64729189e-02 1.76848508e-02 2.92295006e-08\n",
            " 7.23565017e-06 5.02876246e-05 1.30759913e-03 6.24591659e-04\n",
            " 9.98078704e-01 9.96522546e-01 7.21075058e-01 1.00000000e+00\n",
            " 9.99320984e-01 9.80587602e-01 9.99995589e-01 4.49171327e-02\n",
            " 2.62503545e-05 9.73801507e-05 1.07477130e-02 8.65072250e-01\n",
            " 4.69461262e-01 2.29443482e-04 3.13477576e-05 2.13132240e-03\n",
            " 2.14725044e-02 9.81908917e-01 1.57525197e-01 5.29985011e-01\n",
            " 6.84258580e-01 2.08617598e-01 2.08911657e-01 4.75911004e-03\n",
            " 5.22409558e-01 4.64822650e-01 2.96168923e-01 4.48030196e-02\n",
            " 8.39037359e-01 9.19535100e-01 7.98829138e-01 1.25968270e-07\n",
            " 2.37677107e-03 2.95141007e-07 5.91822207e-01 3.44109634e-04\n",
            " 9.81179535e-01 1.60340191e-04 5.23273426e-04 1.41887922e-05\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99948621e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99999881e-01 9.94551837e-01 9.91569579e-01 1.95943624e-01\n",
            " 9.99067128e-01 9.97466326e-01 9.97290552e-01 9.99997616e-01\n",
            " 9.99999881e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99999523e-01 1.00000000e+00 8.57874632e-01\n",
            " 4.80428431e-03 9.99993443e-01 9.99996424e-01 9.99998927e-01\n",
            " 9.82867479e-01 1.00000000e+00 1.00000000e+00 9.94147897e-01\n",
            " 9.95522857e-01 9.99999046e-01 9.99995589e-01 9.99999881e-01\n",
            " 8.17122102e-01 9.79926646e-01 9.98277783e-01 9.76800799e-01\n",
            " 9.99878883e-01 9.70446646e-01 5.08984625e-01 3.93701009e-02\n",
            " 7.75115937e-02 9.54560518e-01 9.97681856e-01 8.73341203e-01\n",
            " 9.58154500e-01 2.42963005e-02 7.88169682e-01 8.74428153e-01\n",
            " 8.41527760e-01 9.97652829e-01 4.04480755e-01 7.38959074e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 98 [0/54 (0%)]\tTrain Loss: 0.000591\n",
            "Train Epoch: 98 [10/54 (19%)]\tTrain Loss: 0.009013\n",
            "Train Epoch: 98 [20/54 (37%)]\tTrain Loss: 0.000033\n",
            "Train Epoch: 98 [30/54 (56%)]\tTrain Loss: 0.000148\n",
            "Train Epoch: 98 [40/54 (74%)]\tTrain Loss: 0.000021\n",
            "Train Epoch: 98 [50/54 (93%)]\tTrain Loss: 0.001429\n",
            "\n",
            "Train set: Average loss: 0.0015, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.68458780e-03 2.95726000e-03 2.75713322e-03 4.73355770e-01\n",
            " 4.65623190e-04 2.74567455e-01 4.25956994e-01 7.88193285e-01\n",
            " 8.84989742e-03 4.74261403e-01 9.70507622e-01 3.49150458e-03\n",
            " 6.89090014e-01 4.77597700e-04 1.53492962e-03 7.14326098e-11\n",
            " 1.71833339e-08 6.85012083e-07 1.30829285e-04 2.21084119e-04\n",
            " 9.47527528e-01 9.44387078e-01 8.18297207e-01 9.99997020e-01\n",
            " 9.96803045e-01 9.47926998e-01 9.99351203e-01 2.19939034e-02\n",
            " 5.39589018e-06 3.11814147e-05 3.39603536e-02 7.96127856e-01\n",
            " 6.64847493e-01 1.55603411e-04 1.14630811e-05 6.42435276e-04\n",
            " 8.70171748e-03 8.33309472e-01 6.60519227e-02 4.80027229e-01\n",
            " 7.07313657e-01 5.67409396e-02 5.58893159e-02 2.94334139e-03\n",
            " 9.44984555e-01 9.80295599e-01 9.60568905e-01 6.82228267e-01\n",
            " 9.72051799e-01 8.84922504e-01 9.76554036e-01 4.45388379e-11\n",
            " 7.26405066e-04 9.10352398e-08 8.59195352e-01 1.96848196e-04\n",
            " 9.77202237e-01 1.06820313e-04 8.39269414e-05 2.11150859e-06\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.97574747e-01 1.00000000e+00 9.99998331e-01 1.00000000e+00\n",
            " 9.99995112e-01 9.94091213e-01 9.87651587e-01 9.51749235e-02\n",
            " 9.99642134e-01 9.92332697e-01 9.92807627e-01 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 8.91915023e-01\n",
            " 5.56049077e-03 9.99999881e-01 9.99985337e-01 9.99999881e-01\n",
            " 8.17371368e-01 9.99999881e-01 1.00000000e+00 9.53369856e-01\n",
            " 9.43214238e-01 9.99951720e-01 9.99890089e-01 9.99988556e-01\n",
            " 5.78006327e-01 9.44841206e-01 9.88944709e-01 9.85553801e-01\n",
            " 9.98054266e-01 9.64369893e-01 1.74640231e-02 1.34217739e-02\n",
            " 2.72515900e-02 9.61324632e-01 9.98016238e-01 8.86029661e-01\n",
            " 8.27447653e-01 5.96422032e-02 8.07516277e-01 9.18725610e-01\n",
            " 9.37209189e-01 9.96289730e-01 8.48563015e-02 6.80447340e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 99 [0/54 (0%)]\tTrain Loss: 0.000011\n",
            "Train Epoch: 99 [10/54 (19%)]\tTrain Loss: 0.000016\n",
            "Train Epoch: 99 [20/54 (37%)]\tTrain Loss: 0.000048\n",
            "Train Epoch: 99 [30/54 (56%)]\tTrain Loss: 0.000005\n",
            "Train Epoch: 99 [40/54 (74%)]\tTrain Loss: 0.000035\n",
            "Train Epoch: 99 [50/54 (93%)]\tTrain Loss: 0.001035\n",
            "\n",
            "Train set: Average loss: 0.0008, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.16651179e-03 7.14759517e-04 2.28211237e-03 2.59591818e-01\n",
            " 1.84316668e-04 5.97719476e-02 1.40488327e-01 1.62673131e-01\n",
            " 2.91984086e-03 1.90024197e-01 9.75083828e-01 2.78339232e-03\n",
            " 4.60652232e-01 3.31876035e-05 2.79672095e-04 8.18088652e-12\n",
            " 3.15037396e-09 4.50133172e-07 1.42109047e-06 6.60175792e-05\n",
            " 3.01047623e-01 7.58294523e-01 5.51336348e-01 9.99999285e-01\n",
            " 9.77320790e-01 5.28240740e-01 9.95631933e-01 1.21417707e-02\n",
            " 1.91679510e-06 4.84124939e-05 4.25847154e-03 1.55092180e-02\n",
            " 2.56258041e-01 6.84695406e-05 9.74305385e-06 7.82871619e-04\n",
            " 9.30760894e-03 9.55827907e-02 2.96275429e-02 2.46038750e-01\n",
            " 3.87891531e-01 1.60110686e-02 1.78952087e-02 3.16489401e-04\n",
            " 3.14894468e-01 4.35387827e-02 3.02621555e-02 2.36180630e-02\n",
            " 6.92012131e-01 1.47679657e-01 4.81104016e-01 1.80452474e-12\n",
            " 1.99370406e-05 4.97116437e-09 1.61869600e-01 1.71608608e-05\n",
            " 4.55737472e-01 3.54694857e-05 1.98458929e-05 3.83050434e-07\n",
            " 1.00000000e+00 9.99997258e-01 9.99999762e-01 9.99999762e-01\n",
            " 9.92121816e-01 1.00000000e+00 9.99993682e-01 1.00000000e+00\n",
            " 9.99933720e-01 2.41730168e-01 4.94032741e-01 1.03998706e-02\n",
            " 9.85118628e-01 9.07807291e-01 9.65516567e-01 9.99986768e-01\n",
            " 9.99999881e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99982476e-01 1.00000000e+00 2.42388994e-01\n",
            " 2.37308792e-03 9.99708712e-01 9.99930382e-01 9.99998689e-01\n",
            " 7.58557737e-01 9.99999881e-01 9.99995708e-01 2.41882801e-01\n",
            " 6.76467061e-01 9.99946356e-01 9.99562323e-01 9.99985695e-01\n",
            " 4.66931909e-02 5.56038558e-01 4.57820714e-01 8.72657716e-01\n",
            " 9.92036164e-01 6.22378886e-01 3.57613410e-03 7.39617273e-04\n",
            " 1.21617655e-03 1.49659827e-01 9.90960777e-01 7.54695952e-01\n",
            " 3.25044990e-01 1.03837270e-02 4.43371445e-01 5.78366101e-01\n",
            " 5.75025201e-01 9.95400965e-01 4.44875099e-03 9.48438868e-02\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 100 [0/54 (0%)]\tTrain Loss: 0.000083\n",
            "Train Epoch: 100 [10/54 (19%)]\tTrain Loss: 0.000137\n",
            "Train Epoch: 100 [20/54 (37%)]\tTrain Loss: 0.000390\n",
            "Train Epoch: 100 [30/54 (56%)]\tTrain Loss: 0.000451\n",
            "Train Epoch: 100 [40/54 (74%)]\tTrain Loss: 0.000053\n",
            "Train Epoch: 100 [50/54 (93%)]\tTrain Loss: 0.000112\n",
            "\n",
            "Train set: Average loss: 0.0011, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.64848005e-04 6.26336038e-03 4.62883059e-03 2.23854691e-01\n",
            " 7.94149601e-05 3.51964645e-02 5.83338380e-01 1.20875090e-01\n",
            " 8.83361441e-04 6.07037306e-01 9.95011330e-01 5.41853625e-03\n",
            " 8.01502347e-01 4.27490522e-06 2.50468380e-04 7.57642985e-14\n",
            " 8.88553727e-12 6.58343936e-07 2.82553283e-05 1.24262195e-04\n",
            " 7.91086435e-01 9.86517489e-01 8.38993669e-01 9.99997735e-01\n",
            " 9.99517918e-01 8.79485190e-01 9.98654366e-01 4.21385616e-02\n",
            " 1.58735529e-05 1.17896947e-04 1.75364073e-02 7.03459263e-01\n",
            " 5.79159260e-01 8.78630453e-05 3.75887703e-06 4.42009419e-03\n",
            " 4.99100909e-02 6.43411815e-01 5.43915927e-01 9.40124393e-01\n",
            " 9.26409960e-01 6.38965905e-01 3.18021812e-02 2.32346333e-03\n",
            " 5.39940119e-01 3.82022336e-02 7.88723454e-02 5.58829084e-02\n",
            " 8.80368412e-01 3.29563916e-01 5.84815025e-01 8.36153927e-14\n",
            " 7.68083009e-06 7.64109401e-11 6.82183087e-01 1.30019689e-05\n",
            " 8.87905121e-01 1.91663876e-05 2.16099420e-06 8.14168732e-09\n",
            " 9.99992609e-01 9.99937296e-01 9.99975681e-01 9.99929070e-01\n",
            " 9.52226639e-01 1.00000000e+00 9.99938011e-01 1.00000000e+00\n",
            " 9.99779046e-01 8.33707929e-01 7.08610117e-01 8.02571997e-02\n",
            " 9.89350259e-01 9.89334762e-01 9.89601552e-01 9.99261796e-01\n",
            " 9.99984503e-01 1.00000000e+00 1.00000000e+00 9.99999762e-01\n",
            " 1.00000000e+00 9.99640465e-01 1.00000000e+00 9.59548175e-01\n",
            " 2.32931767e-02 9.99783456e-01 9.99906540e-01 9.99994397e-01\n",
            " 7.28448629e-01 1.00000000e+00 9.99998450e-01 4.14322257e-01\n",
            " 6.49232388e-01 9.99943137e-01 9.99785602e-01 9.99985695e-01\n",
            " 2.33286805e-02 9.83335495e-01 9.84356463e-01 9.97990489e-01\n",
            " 9.99227047e-01 9.91199553e-01 4.67788279e-01 8.49564839e-03\n",
            " 5.31091029e-03 5.52327573e-01 9.97707844e-01 9.76632953e-01\n",
            " 9.75203156e-01 5.52870333e-02 9.11160767e-01 9.83718455e-01\n",
            " 9.33661759e-01 9.99196112e-01 1.15355857e-01 9.57123101e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 48 TN= 41 FN= 10 FP= 19\n",
            "TP+FP 67\n",
            "precision 0.7164179104477612\n",
            "recall 0.8275862068965517\n",
            "F1 0.768\n",
            "acc 0.7542372881355932\n",
            "AUCp 0.7554597701149426\n",
            "AUC 0.8727011494252874\n",
            "\n",
            " The epoch is 100, average recall: 0.8276, average precision: 0.7164,average F1: 0.7680, average accuracy: 0.7542, average AUC: 0.8727\n",
            "Train Epoch: 101 [0/54 (0%)]\tTrain Loss: 0.000543\n",
            "Train Epoch: 101 [10/54 (19%)]\tTrain Loss: 0.000031\n",
            "Train Epoch: 101 [20/54 (37%)]\tTrain Loss: 0.000016\n",
            "Train Epoch: 101 [30/54 (56%)]\tTrain Loss: 0.000024\n",
            "Train Epoch: 101 [40/54 (74%)]\tTrain Loss: 0.000083\n",
            "Train Epoch: 101 [50/54 (93%)]\tTrain Loss: 0.000008\n",
            "\n",
            "Train set: Average loss: 0.0100, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.09424019e-04 3.91489342e-02 1.48072615e-02 3.26009765e-02\n",
            " 2.65881608e-05 1.84211938e-04 8.08611691e-01 2.35957536e-03\n",
            " 4.96264988e-07 8.05883467e-01 9.95393395e-01 1.91468038e-02\n",
            " 8.83167922e-01 1.79469916e-05 2.15316148e-04 1.60419886e-10\n",
            " 9.42163388e-06 3.83853912e-05 1.66241184e-03 4.44279984e-03\n",
            " 9.20231521e-01 9.87509191e-01 8.55720401e-01 9.99997616e-01\n",
            " 9.99685884e-01 8.46413493e-01 9.99796212e-01 1.15452848e-01\n",
            " 1.59964839e-04 4.61110903e-04 1.30955562e-01 9.38381910e-01\n",
            " 6.60216808e-01 1.88813452e-03 5.14558225e-04 3.16351429e-02\n",
            " 2.56921172e-01 4.37726468e-01 5.30447066e-01 9.89925146e-01\n",
            " 9.70994651e-01 9.36301112e-01 3.31049301e-02 3.05881090e-02\n",
            " 5.75785518e-01 7.83104837e-01 9.14955974e-01 8.59660089e-01\n",
            " 9.96534467e-01 9.47976589e-01 9.80772853e-01 2.87078223e-07\n",
            " 3.16297257e-04 1.57691477e-06 9.60544586e-01 1.68944022e-03\n",
            " 9.79874790e-01 1.71246286e-03 3.97005942e-06 2.41010298e-06\n",
            " 1.00000000e+00 9.99999762e-01 1.00000000e+00 1.00000000e+00\n",
            " 4.63703051e-02 9.97779310e-01 6.95821226e-01 1.00000000e+00\n",
            " 9.95133340e-01 9.54916358e-01 9.89391685e-01 7.51229525e-01\n",
            " 6.15270317e-01 8.35300565e-01 9.76227283e-01 9.99099851e-01\n",
            " 9.99970317e-01 1.00000000e+00 1.00000000e+00 9.99999404e-01\n",
            " 1.00000000e+00 9.99866724e-01 1.00000000e+00 9.90944505e-01\n",
            " 6.57640517e-01 9.99999881e-01 9.99936223e-01 9.99996424e-01\n",
            " 1.89940333e-01 9.99999762e-01 9.99998450e-01 4.89561766e-01\n",
            " 4.37727608e-02 9.99642968e-01 9.99576986e-01 9.99939203e-01\n",
            " 1.97501499e-02 9.94988322e-01 9.94484603e-01 9.98786986e-01\n",
            " 9.98590887e-01 9.91901100e-01 7.66568124e-01 3.95963490e-02\n",
            " 2.09133364e-02 9.53856707e-01 9.98580575e-01 9.64189529e-01\n",
            " 7.38828719e-01 2.40196157e-02 7.99453676e-01 9.38468933e-01\n",
            " 5.23189068e-01 8.73579204e-01 1.53601110e-01 9.86807764e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 102 [0/54 (0%)]\tTrain Loss: 0.000128\n",
            "Train Epoch: 102 [10/54 (19%)]\tTrain Loss: 0.000163\n",
            "Train Epoch: 102 [20/54 (37%)]\tTrain Loss: 0.000087\n",
            "Train Epoch: 102 [30/54 (56%)]\tTrain Loss: 0.000147\n",
            "Train Epoch: 102 [40/54 (74%)]\tTrain Loss: 0.000028\n",
            "Train Epoch: 102 [50/54 (93%)]\tTrain Loss: 0.000033\n",
            "\n",
            "Train set: Average loss: 0.0032, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.42945535e-03 4.60847527e-01 1.89087819e-02 9.01618153e-02\n",
            " 6.48380839e-04 1.49663538e-03 9.16905999e-01 7.87261352e-02\n",
            " 1.06880521e-04 4.57397878e-01 9.93149102e-01 2.11301409e-02\n",
            " 9.82597768e-01 1.33580843e-03 4.71938588e-03 5.17668632e-05\n",
            " 1.12506712e-03 5.34484221e-04 5.65364540e-01 4.78020638e-01\n",
            " 9.99985933e-01 9.99951482e-01 9.99818265e-01 1.00000000e+00\n",
            " 9.99999523e-01 9.99967575e-01 1.00000000e+00 3.82709920e-01\n",
            " 7.94694386e-03 9.96847404e-04 7.19060540e-01 9.99999762e-01\n",
            " 9.85864520e-01 2.35310290e-03 1.73997972e-03 4.38165963e-02\n",
            " 2.27505401e-01 9.90054011e-01 9.90227461e-01 9.97047484e-01\n",
            " 9.93262589e-01 9.90226388e-01 5.52202284e-01 4.88907367e-01\n",
            " 9.99821484e-01 9.75355327e-01 9.98349547e-01 9.74177361e-01\n",
            " 9.93286073e-01 5.93794107e-01 9.90595460e-01 2.37736245e-07\n",
            " 3.11565527e-04 5.75133708e-06 9.98864293e-01 4.21952084e-03\n",
            " 9.99379754e-01 1.19830994e-03 1.28463025e-05 2.50525714e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99494195e-01 9.99998569e-01 9.99216199e-01 1.00000000e+00\n",
            " 9.98856783e-01 9.99858499e-01 9.99869227e-01 9.97164190e-01\n",
            " 9.99959588e-01 9.98902678e-01 9.99584734e-01 9.99991059e-01\n",
            " 9.99999762e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99919295e-01\n",
            " 9.15910900e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 4.93001133e-01 1.00000000e+00 1.00000000e+00 8.50282133e-01\n",
            " 9.34437096e-01 9.99999881e-01 1.00000000e+00 1.00000000e+00\n",
            " 7.97479272e-01 9.99798834e-01 9.99986768e-01 9.99979615e-01\n",
            " 9.99956965e-01 9.99949098e-01 9.99506116e-01 2.32691109e-01\n",
            " 4.18256707e-02 9.99941111e-01 9.99787509e-01 8.94079626e-01\n",
            " 6.24831200e-01 8.67202412e-03 6.13281786e-01 8.44525099e-01\n",
            " 6.64508283e-01 9.99951720e-01 9.97321069e-01 9.99999523e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 103 [0/54 (0%)]\tTrain Loss: 0.000720\n",
            "Train Epoch: 103 [10/54 (19%)]\tTrain Loss: 0.000189\n",
            "Train Epoch: 103 [20/54 (37%)]\tTrain Loss: 0.000109\n",
            "Train Epoch: 103 [30/54 (56%)]\tTrain Loss: 0.001913\n",
            "Train Epoch: 103 [40/54 (74%)]\tTrain Loss: 0.000027\n",
            "Train Epoch: 103 [50/54 (93%)]\tTrain Loss: 0.000193\n",
            "\n",
            "Train set: Average loss: 0.0009, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.62727995e-04 1.74332708e-01 5.98743744e-03 1.31671101e-01\n",
            " 4.54568479e-04 6.46699860e-04 4.34499711e-01 1.26264796e-01\n",
            " 6.88502696e-05 2.32000753e-01 8.24270129e-01 8.80996045e-03\n",
            " 8.89412403e-01 4.68455517e-04 2.83437688e-03 3.32168238e-05\n",
            " 4.70094586e-04 1.68819888e-03 9.47232842e-01 8.31739962e-01\n",
            " 9.99996901e-01 9.99993563e-01 9.99954462e-01 1.00000000e+00\n",
            " 9.99999404e-01 9.99999881e-01 1.00000000e+00 3.15214634e-01\n",
            " 1.84538379e-01 7.05516490e-04 7.39871323e-01 1.00000000e+00\n",
            " 9.71835017e-01 1.26226805e-03 1.20948418e-03 4.25818712e-02\n",
            " 1.85086146e-01 9.95696187e-01 9.84178960e-01 9.67051387e-01\n",
            " 9.68456805e-01 9.59050417e-01 5.98375738e-01 6.43703401e-01\n",
            " 9.99877095e-01 6.30313039e-01 9.88343060e-01 7.33103931e-01\n",
            " 2.35129297e-01 2.28976458e-02 4.94124353e-01 1.91257055e-09\n",
            " 3.63341460e-05 1.22170675e-07 9.98777092e-01 1.21756620e-03\n",
            " 9.96164918e-01 1.82180796e-04 3.33187859e-06 1.13729118e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.99673486e-01 9.99999166e-01 9.99734104e-01 1.00000000e+00\n",
            " 9.95487511e-01 9.99239802e-01 9.95695591e-01 9.20777500e-01\n",
            " 9.99999642e-01 9.98613596e-01 9.97575819e-01 9.99955416e-01\n",
            " 9.99999046e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.98816371e-01\n",
            " 9.51718748e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 3.84699434e-01 1.00000000e+00 1.00000000e+00 2.82872498e-01\n",
            " 9.26016271e-01 9.99999881e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.82453108e-01 9.99115765e-01 9.99995589e-01 9.99581516e-01\n",
            " 9.99537349e-01 9.98804927e-01 9.89624679e-01 9.64002982e-02\n",
            " 6.92642666e-03 9.99724925e-01 9.93052125e-01 7.46345520e-01\n",
            " 7.88606524e-01 8.70913733e-03 6.66915298e-01 7.90901065e-01\n",
            " 8.32063019e-01 9.99991775e-01 9.87006307e-01 9.99998808e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 104 [0/54 (0%)]\tTrain Loss: 0.000034\n",
            "Train Epoch: 104 [10/54 (19%)]\tTrain Loss: 0.000251\n",
            "Train Epoch: 104 [20/54 (37%)]\tTrain Loss: 0.000011\n",
            "Train Epoch: 104 [30/54 (56%)]\tTrain Loss: 0.000021\n",
            "Train Epoch: 104 [40/54 (74%)]\tTrain Loss: 0.001415\n",
            "Train Epoch: 104 [50/54 (93%)]\tTrain Loss: 0.000014\n",
            "\n",
            "Train set: Average loss: 0.0022, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.74121786e-04 3.03189486e-01 5.24289161e-03 4.29006249e-01\n",
            " 1.09298574e-03 3.25149461e-03 6.29873753e-01 9.64836597e-01\n",
            " 2.82310066e-05 2.24342898e-01 9.38006938e-01 4.39688656e-03\n",
            " 9.34971154e-01 8.55429797e-04 3.56551968e-02 2.04017870e-05\n",
            " 1.22886151e-04 1.07436201e-04 9.24957097e-01 2.28162810e-01\n",
            " 9.99978900e-01 9.99949217e-01 9.98476088e-01 9.99956846e-01\n",
            " 9.99983668e-01 9.99939919e-01 9.99827087e-01 1.41918361e-01\n",
            " 1.18752874e-01 1.23309408e-04 3.72089565e-01 9.99993443e-01\n",
            " 9.32346940e-01 5.79836313e-04 2.37387969e-04 6.74888343e-02\n",
            " 1.52561203e-01 9.99711931e-01 9.31231439e-01 9.59276259e-01\n",
            " 9.74116147e-01 9.51308727e-01 8.28000307e-01 5.89108109e-01\n",
            " 9.99802649e-01 9.39694405e-01 9.92035687e-01 7.84155071e-01\n",
            " 1.74547136e-01 2.59781964e-02 4.83195215e-01 8.25905371e-13\n",
            " 1.65521524e-05 1.05602826e-09 9.91134524e-01 1.39813288e-03\n",
            " 9.81888711e-01 1.09817092e-05 3.51613465e-07 1.14426184e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.98634875e-01 9.99999762e-01 9.99951601e-01 9.99987245e-01\n",
            " 9.97845531e-01 9.96125996e-01 8.95783186e-01 5.12125611e-01\n",
            " 9.99958873e-01 9.99096513e-01 9.93218243e-01 9.99479115e-01\n",
            " 9.99954581e-01 1.00000000e+00 9.99999166e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.99999046e-01 9.99869704e-01 9.93736863e-01\n",
            " 3.36240262e-01 1.00000000e+00 9.99703825e-01 9.99819577e-01\n",
            " 3.91661584e-01 1.00000000e+00 1.00000000e+00 6.70138001e-01\n",
            " 9.30135429e-01 9.99996662e-01 9.99999762e-01 1.00000000e+00\n",
            " 9.93627191e-01 9.99397159e-01 9.99964118e-01 9.99573410e-01\n",
            " 9.99778926e-01 9.97958660e-01 9.90170777e-01 2.68044114e-01\n",
            " 5.98337911e-02 9.99552071e-01 9.74851310e-01 6.11021936e-01\n",
            " 8.24235022e-01 1.20539488e-02 4.02683854e-01 7.99623191e-01\n",
            " 4.23714906e-01 9.99925494e-01 9.87456620e-01 9.99983907e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 105 [0/54 (0%)]\tTrain Loss: 0.000019\n",
            "Train Epoch: 105 [10/54 (19%)]\tTrain Loss: 0.000021\n",
            "Train Epoch: 105 [20/54 (37%)]\tTrain Loss: 0.000011\n",
            "Train Epoch: 105 [30/54 (56%)]\tTrain Loss: 0.000028\n",
            "Train Epoch: 105 [40/54 (74%)]\tTrain Loss: 0.048720\n",
            "Train Epoch: 105 [50/54 (93%)]\tTrain Loss: 0.000069\n",
            "\n",
            "Train set: Average loss: 0.0040, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.51021078e-05 3.83282304e-02 2.57835700e-03 3.64763811e-02\n",
            " 2.84816833e-05 1.81582061e-06 2.13845417e-01 5.84531762e-02\n",
            " 2.19567742e-09 6.13953888e-01 9.92338538e-01 6.26583630e-03\n",
            " 8.91455412e-01 1.06254060e-06 1.24150177e-03 8.41776213e-08\n",
            " 4.16494049e-06 1.00438702e-11 3.89848769e-01 6.05821330e-03\n",
            " 9.99449790e-01 9.81597900e-01 9.83515680e-01 9.99988914e-01\n",
            " 9.99322295e-01 9.99294281e-01 9.99990463e-01 3.04284617e-02\n",
            " 7.46954978e-02 5.36616681e-05 4.10817489e-02 7.24859953e-01\n",
            " 1.26792967e-01 6.16117439e-04 1.03056394e-04 1.12099934e-03\n",
            " 1.71012059e-02 4.00048405e-01 3.71787958e-02 4.26565170e-01\n",
            " 4.91153032e-01 2.12755635e-01 3.77128311e-02 1.86571162e-02\n",
            " 9.87909555e-01 3.66941035e-01 1.03897832e-01 2.35274807e-03\n",
            " 2.02996302e-02 1.29157603e-02 2.67414413e-02 9.31218535e-14\n",
            " 1.98194414e-07 1.95417260e-09 7.90322304e-01 4.57910704e-04\n",
            " 8.86814952e-01 1.69541795e-06 8.08332845e-11 5.17172111e-07\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 7.80466735e-01 9.99974132e-01 9.98216569e-01 9.99407291e-01\n",
            " 9.90486443e-01 9.08918202e-01 6.69335306e-01 2.29533061e-01\n",
            " 9.96224403e-01 9.38501418e-01 9.43121374e-01 9.89878297e-01\n",
            " 9.99965906e-01 1.00000000e+00 1.00000000e+00 9.99876618e-01\n",
            " 1.00000000e+00 9.99950290e-01 9.99593198e-01 9.71824706e-01\n",
            " 1.04684196e-01 9.99979973e-01 9.98408854e-01 9.98627663e-01\n",
            " 1.57737151e-01 1.00000000e+00 1.00000000e+00 2.29372725e-01\n",
            " 1.80819720e-01 9.99999166e-01 1.00000000e+00 9.99997497e-01\n",
            " 9.18010652e-01 9.97356534e-01 9.82861161e-01 9.88553286e-01\n",
            " 9.99005139e-01 9.82544363e-01 5.00997305e-02 2.09414195e-02\n",
            " 1.99630880e-03 9.97291982e-01 9.25019443e-01 1.91845775e-01\n",
            " 1.36374414e-01 9.61938687e-03 6.16423637e-02 5.94238758e-01\n",
            " 3.29189211e-01 9.99171853e-01 9.28539515e-01 9.89638090e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 106 [0/54 (0%)]\tTrain Loss: 0.000091\n",
            "Train Epoch: 106 [10/54 (19%)]\tTrain Loss: 0.000170\n",
            "Train Epoch: 106 [20/54 (37%)]\tTrain Loss: 0.000522\n",
            "Train Epoch: 106 [30/54 (56%)]\tTrain Loss: 0.000481\n",
            "Train Epoch: 106 [40/54 (74%)]\tTrain Loss: 0.000124\n",
            "Train Epoch: 106 [50/54 (93%)]\tTrain Loss: 0.000028\n",
            "\n",
            "Train set: Average loss: 0.0008, Accuracy: 424/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.49136285e-05 9.45001766e-02 2.00976525e-03 1.22137582e-02\n",
            " 6.75818819e-06 4.24217376e-07 4.47545230e-01 9.85158142e-03\n",
            " 2.08136464e-09 3.56352568e-01 9.44257617e-01 2.19918671e-03\n",
            " 7.55116045e-01 1.11870875e-07 1.60192198e-03 1.21116517e-09\n",
            " 2.42088390e-06 1.22577270e-11 7.96519399e-01 3.46197262e-02\n",
            " 9.99480665e-01 9.81150568e-01 9.91935134e-01 9.99996901e-01\n",
            " 9.99211907e-01 9.98912692e-01 9.99984860e-01 2.48879623e-02\n",
            " 2.23156437e-01 7.96673921e-05 5.79158105e-02 6.57490373e-01\n",
            " 4.90683392e-02 2.66664923e-04 5.49828874e-05 5.10161626e-04\n",
            " 9.37382039e-03 6.92911670e-02 3.54325101e-02 1.29645064e-01\n",
            " 1.52250499e-01 1.06520332e-01 2.82053780e-02 5.71534829e-03\n",
            " 9.63367045e-01 1.20061509e-01 5.32892607e-02 1.39661052e-03\n",
            " 1.51634635e-02 6.45613531e-03 1.27369110e-02 4.12645600e-16\n",
            " 2.84279071e-08 1.07316947e-10 7.21021533e-01 1.39839409e-04\n",
            " 9.56405878e-01 8.85638258e-07 2.25608109e-11 2.24367469e-07\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.06767845e-01 9.99956012e-01 9.98505950e-01 9.99870181e-01\n",
            " 9.94730115e-01 9.12813306e-01 5.25243163e-01 2.43686393e-01\n",
            " 9.98736322e-01 9.26854014e-01 9.76438344e-01 9.95532513e-01\n",
            " 9.99974370e-01 9.99999762e-01 1.00000000e+00 9.99223590e-01\n",
            " 1.00000000e+00 9.99978304e-01 9.99937654e-01 9.94659960e-01\n",
            " 9.81752649e-02 9.99897957e-01 9.98855472e-01 9.98759151e-01\n",
            " 4.69387531e-01 1.00000000e+00 1.00000000e+00 6.82736337e-02\n",
            " 4.99922410e-02 9.99997139e-01 1.00000000e+00 9.99997377e-01\n",
            " 8.97837818e-01 9.95563269e-01 9.79205549e-01 9.83659625e-01\n",
            " 9.96778905e-01 9.92978990e-01 4.40885536e-02 2.48967181e-03\n",
            " 8.34636448e-04 9.95666027e-01 9.26065922e-01 9.15860012e-02\n",
            " 1.08686902e-01 8.75691697e-03 6.06432669e-02 6.08052373e-01\n",
            " 3.51360112e-01 9.99318123e-01 9.86700833e-01 9.95266438e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 107 [0/54 (0%)]\tTrain Loss: 0.000067\n",
            "Train Epoch: 107 [10/54 (19%)]\tTrain Loss: 0.000401\n",
            "Train Epoch: 107 [20/54 (37%)]\tTrain Loss: 0.000064\n",
            "Train Epoch: 107 [30/54 (56%)]\tTrain Loss: 0.000016\n",
            "Train Epoch: 107 [40/54 (74%)]\tTrain Loss: 0.000068\n",
            "Train Epoch: 107 [50/54 (93%)]\tTrain Loss: 0.000182\n",
            "\n",
            "Train set: Average loss: 0.0003, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.78805367e-05 1.45898640e-01 2.39140540e-03 2.38431785e-02\n",
            " 3.68499332e-06 3.35082802e-07 4.93576854e-01 5.98541275e-02\n",
            " 8.80873374e-10 5.79979897e-01 9.79877651e-01 2.31736177e-03\n",
            " 8.83698583e-01 2.25019257e-08 9.84455808e-04 5.60306870e-11\n",
            " 3.56061634e-07 6.58979273e-13 5.90413392e-01 2.04366669e-02\n",
            " 9.99134600e-01 9.87760901e-01 9.94364142e-01 9.99992609e-01\n",
            " 9.99554098e-01 9.98666167e-01 9.99869704e-01 1.80244762e-02\n",
            " 1.52098894e-01 4.04288403e-05 6.12219907e-02 8.86817038e-01\n",
            " 1.11192897e-01 1.26274201e-04 2.28786466e-05 4.60194045e-04\n",
            " 7.95768108e-03 3.70744228e-01 1.22495793e-01 2.59013981e-01\n",
            " 2.84427702e-01 2.19710410e-01 1.20192662e-01 7.74955051e-03\n",
            " 9.84132409e-01 2.22193196e-01 2.39791125e-01 2.77271611e-03\n",
            " 3.37012000e-02 9.01320670e-03 4.53133732e-02 2.61430419e-16\n",
            " 5.41369394e-09 1.49099934e-11 7.65816748e-01 8.09587582e-05\n",
            " 9.67526078e-01 4.75235055e-07 1.08653590e-11 7.12373307e-08\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.58724678e-01 9.99991775e-01 9.99572456e-01 9.99960780e-01\n",
            " 9.96931314e-01 9.60953593e-01 6.45253778e-01 3.75497043e-01\n",
            " 9.99669671e-01 9.69649673e-01 9.92796600e-01 9.98839676e-01\n",
            " 9.99988437e-01 9.99999762e-01 9.99999881e-01 9.99848723e-01\n",
            " 1.00000000e+00 9.99994516e-01 9.99987960e-01 9.98886049e-01\n",
            " 1.06809154e-01 9.99970913e-01 9.99392271e-01 9.99395013e-01\n",
            " 5.70825100e-01 1.00000000e+00 9.99999046e-01 1.27264440e-01\n",
            " 1.20741166e-01 9.99992847e-01 9.99999762e-01 9.99997854e-01\n",
            " 9.29182768e-01 9.98178959e-01 9.85924304e-01 9.93788779e-01\n",
            " 9.97857153e-01 9.98398006e-01 5.93390763e-02 2.29556928e-03\n",
            " 1.12829031e-03 9.95927393e-01 9.62519765e-01 1.41010553e-01\n",
            " 1.30970016e-01 7.92481750e-03 1.05565809e-01 7.56923914e-01\n",
            " 3.24088782e-01 9.99535680e-01 9.93565738e-01 9.98569846e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 108 [0/54 (0%)]\tTrain Loss: 0.000010\n",
            "Train Epoch: 108 [10/54 (19%)]\tTrain Loss: 0.000003\n",
            "Train Epoch: 108 [20/54 (37%)]\tTrain Loss: 0.000039\n",
            "Train Epoch: 108 [30/54 (56%)]\tTrain Loss: 0.000008\n",
            "Train Epoch: 108 [40/54 (74%)]\tTrain Loss: 0.000002\n",
            "Train Epoch: 108 [50/54 (93%)]\tTrain Loss: 0.001931\n",
            "\n",
            "Train set: Average loss: 0.0006, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.98205105e-05 1.67926654e-01 3.02218902e-03 2.00957377e-02\n",
            " 4.84592420e-06 6.81176402e-07 5.49378574e-01 5.58402538e-02\n",
            " 1.65494562e-09 6.01258814e-01 9.88605917e-01 2.89848913e-03\n",
            " 9.21030641e-01 3.39675310e-08 6.66863110e-04 8.13925663e-11\n",
            " 5.26184749e-07 4.36802477e-13 4.63444293e-01 1.34390993e-02\n",
            " 9.99464691e-01 9.90470886e-01 9.95851517e-01 9.99998569e-01\n",
            " 9.99781430e-01 9.99002039e-01 9.99926209e-01 1.33354086e-02\n",
            " 9.68734846e-02 3.75146810e-05 3.37794349e-02 8.87807608e-01\n",
            " 1.06095955e-01 1.33475100e-04 2.70980763e-05 5.40933397e-04\n",
            " 7.65358005e-03 3.97246420e-01 9.08358917e-02 2.59154439e-01\n",
            " 2.49167681e-01 2.25547373e-01 1.06628388e-01 5.61784627e-03\n",
            " 9.88740861e-01 2.31181532e-01 3.24299693e-01 3.68249719e-03\n",
            " 4.54316549e-02 9.80555639e-03 6.21801056e-02 5.67165426e-16\n",
            " 1.00158983e-08 2.31675634e-11 8.10309231e-01 9.70311667e-05\n",
            " 9.53652740e-01 9.21365995e-07 5.77626662e-11 1.73093113e-07\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.45823312e-01 9.99996424e-01 9.99765813e-01 9.99977589e-01\n",
            " 9.97326612e-01 9.74249959e-01 6.07033491e-01 3.46551895e-01\n",
            " 9.99780715e-01 9.72213268e-01 9.93316650e-01 9.99135554e-01\n",
            " 9.99995112e-01 1.00000000e+00 1.00000000e+00 9.99923229e-01\n",
            " 1.00000000e+00 9.99994993e-01 9.99994397e-01 9.98079419e-01\n",
            " 5.34489155e-02 9.99982715e-01 9.99615431e-01 9.99611199e-01\n",
            " 5.72252810e-01 1.00000000e+00 9.99999762e-01 1.34540290e-01\n",
            " 8.71361122e-02 9.99997616e-01 1.00000000e+00 9.99999166e-01\n",
            " 9.43287849e-01 9.98797059e-01 9.90460694e-01 9.95089471e-01\n",
            " 9.98791039e-01 9.98267174e-01 3.52189243e-02 2.45846785e-03\n",
            " 1.51439768e-03 9.96522427e-01 9.69916284e-01 1.56674013e-01\n",
            " 1.24059081e-01 7.75360316e-03 1.23778038e-01 7.97180712e-01\n",
            " 3.46365899e-01 9.99751985e-01 9.90563393e-01 9.98182535e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 109 [0/54 (0%)]\tTrain Loss: 0.000025\n",
            "Train Epoch: 109 [10/54 (19%)]\tTrain Loss: 0.000018\n",
            "Train Epoch: 109 [20/54 (37%)]\tTrain Loss: 0.000161\n",
            "Train Epoch: 109 [30/54 (56%)]\tTrain Loss: 0.001219\n",
            "Train Epoch: 109 [40/54 (74%)]\tTrain Loss: 0.001729\n",
            "Train Epoch: 109 [50/54 (93%)]\tTrain Loss: 0.001642\n",
            "\n",
            "Train set: Average loss: 0.0098, Accuracy: 421/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.96106812e-05 9.59856272e-01 8.22401699e-03 2.62652733e-03\n",
            " 2.01378998e-06 1.77542390e-08 9.80013490e-01 9.84163303e-03\n",
            " 1.09029293e-06 9.65184331e-01 9.98563826e-01 8.75402018e-02\n",
            " 9.92453277e-01 3.22573294e-04 6.75825030e-02 1.46800376e-05\n",
            " 4.61306481e-05 5.94108883e-07 9.86063778e-01 4.63330559e-02\n",
            " 9.91489947e-01 9.96830761e-01 9.51396883e-01 9.99162793e-01\n",
            " 9.99244452e-01 9.96196032e-01 9.97464895e-01 1.87662989e-02\n",
            " 2.38073349e-01 3.45370863e-05 9.17892903e-02 2.15369701e-01\n",
            " 3.63886446e-01 6.81756181e-04 1.98612674e-04 5.86509006e-04\n",
            " 1.03744744e-04 9.80807245e-01 2.85216421e-02 3.04316287e-03\n",
            " 4.36997088e-03 1.45468116e-03 6.27406120e-01 2.01205686e-02\n",
            " 9.86686707e-01 8.52459252e-01 3.94964308e-01 3.30594322e-03\n",
            " 2.81259157e-02 1.91502031e-02 7.71499053e-02 3.52713016e-07\n",
            " 7.42266944e-04 2.37545264e-05 9.28010106e-01 1.80851430e-01\n",
            " 9.90989387e-01 2.69075134e-03 1.39754547e-05 8.06763652e-04\n",
            " 9.99861240e-01 9.99886870e-01 1.00000000e+00 9.99999523e-01\n",
            " 8.08342218e-01 9.99944329e-01 9.92957711e-01 9.99097586e-01\n",
            " 5.65545201e-01 8.83079052e-01 5.92191398e-01 3.80629808e-01\n",
            " 9.99846816e-01 9.69217777e-01 9.97982264e-01 9.99266207e-01\n",
            " 9.99883294e-01 9.99532580e-01 9.95327711e-01 9.99473393e-01\n",
            " 9.99989629e-01 9.99376237e-01 9.99482095e-01 9.98822987e-01\n",
            " 1.91844907e-02 9.98590887e-01 9.95058298e-01 9.88976896e-01\n",
            " 9.46467519e-01 9.99732554e-01 9.99655128e-01 6.60012603e-01\n",
            " 5.62671907e-02 9.99937177e-01 9.99916792e-01 9.99955773e-01\n",
            " 9.99499559e-01 9.99553144e-01 9.84256446e-01 7.01591015e-01\n",
            " 9.96999264e-01 9.96008515e-01 2.52027321e-03 5.38398921e-01\n",
            " 2.69466609e-01 9.98625636e-01 9.98762727e-01 5.77886701e-01\n",
            " 5.02941847e-01 1.95666194e-01 2.92812109e-01 8.52258742e-01\n",
            " 8.23117852e-01 9.98293459e-01 6.26934111e-01 4.30893123e-01\n",
            " 9.99999762e-01 9.99746382e-01]\n",
            "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 110 [0/54 (0%)]\tTrain Loss: 0.001150\n",
            "Train Epoch: 110 [10/54 (19%)]\tTrain Loss: 0.000007\n",
            "Train Epoch: 110 [20/54 (37%)]\tTrain Loss: 0.000147\n",
            "Train Epoch: 110 [30/54 (56%)]\tTrain Loss: 0.000074\n",
            "Train Epoch: 110 [40/54 (74%)]\tTrain Loss: 0.023966\n",
            "Train Epoch: 110 [50/54 (93%)]\tTrain Loss: 0.012294\n",
            "\n",
            "Train set: Average loss: 0.0037, Accuracy: 417/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.26671104e-03 9.99910712e-01 3.19753915e-01 7.64459046e-03\n",
            " 2.43034400e-03 1.79947849e-04 9.99998927e-01 2.20716242e-02\n",
            " 5.62939967e-05 9.40222204e-01 9.99853611e-01 5.27429841e-02\n",
            " 9.98717666e-01 5.12335962e-03 8.07072371e-02 2.80239590e-04\n",
            " 6.72429334e-04 2.57252632e-05 1.91561878e-02 1.07617453e-02\n",
            " 6.22531533e-01 9.52473640e-01 1.32993683e-01 9.99998331e-01\n",
            " 9.90867138e-01 9.95573163e-01 9.99978662e-01 1.89230695e-01\n",
            " 4.56967317e-02 9.00996791e-04 9.32777971e-02 7.04063714e-01\n",
            " 9.56344366e-01 1.14882167e-03 1.10935455e-03 3.57569871e-03\n",
            " 3.61209665e-03 9.99706924e-01 3.38071108e-01 4.82194453e-01\n",
            " 1.75535858e-01 7.73482248e-02 7.05200136e-01 6.32978007e-02\n",
            " 9.88157690e-01 5.50127029e-01 9.51078117e-01 1.34219388e-02\n",
            " 5.21086395e-01 2.82243580e-01 9.28827167e-01 2.05954347e-08\n",
            " 1.18944104e-06 3.10539849e-06 4.01494443e-01 8.98909057e-05\n",
            " 5.04150689e-01 2.46515236e-04 3.90603464e-06 2.33171450e-04\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 3.16118568e-01 9.99996662e-01 9.60366368e-01 9.99999285e-01\n",
            " 4.92098808e-01 9.83017683e-01 9.96352077e-01 9.98209596e-01\n",
            " 9.99999762e-01 9.97405946e-01 9.99989033e-01 9.99995589e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 4.66360480e-01 1.00000000e+00 9.99987960e-01 1.00000000e+00\n",
            " 9.58193004e-01 1.00000000e+00 9.99998569e-01 2.98483789e-01\n",
            " 2.13155448e-02 9.99998093e-01 9.99849916e-01 9.99961138e-01\n",
            " 1.11120880e-01 9.99927282e-01 9.99891758e-01 9.93403256e-01\n",
            " 9.99854684e-01 9.99999881e-01 4.65026731e-03 2.40795016e-02\n",
            " 4.88965679e-03 8.07971418e-01 9.98567462e-01 2.21222028e-01\n",
            " 2.24428624e-02 1.50614614e-02 1.14510044e-01 3.78339767e-01\n",
            " 4.28719074e-02 9.98213291e-01 9.97561932e-01 9.15837467e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 45 TN= 43 FN= 13 FP= 17\n",
            "TP+FP 62\n",
            "precision 0.7258064516129032\n",
            "recall 0.7758620689655172\n",
            "F1 0.7500000000000001\n",
            "acc 0.7457627118644068\n",
            "AUCp 0.746264367816092\n",
            "AUC 0.8528735632183908\n",
            "\n",
            " The epoch is 110, average recall: 0.7759, average precision: 0.7258,average F1: 0.7500, average accuracy: 0.7458, average AUC: 0.8529\n",
            "Train Epoch: 111 [0/54 (0%)]\tTrain Loss: 0.000224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-0920ca45ad77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtotal_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtargetlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorelist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-922e5999fec2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(optimizer, epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzoBL5WIYX5X",
        "outputId": "038e666d-1edb-440c-dcaa-0133ff7d517b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgvuREUVYS_Y"
      },
      "source": [
        "!cp '/content/train_10epochs_model_backup/efficientNet-b4.pth' '/content/drive/MyDrive/REPORT/Research/EfficientNet/model_efficientNet-b4.pth'"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHbngMFEZ_BA"
      },
      "source": [
        "%cp '/content/model_result/efficientNet-b4.txt' '/content/drive/MyDrive/REPORT/Research/EfficientNet/result.txt'"
      ],
      "execution_count": 46,
      "outputs": []
    }
  ]
}